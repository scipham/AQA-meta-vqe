{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sy\n",
    "import scipy as sc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cirq\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **!! NOTE: This python + cirq notebook is legacy code for reference. It should work fine but the Julia + Yao Edition is much more up-to-date and tested. All relevant experiments have been performed using the Julia Edition of this notebook, *not* this one. !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loss_gradient_approx(loss_func, stochastic_approx=False, stoch_shift_size_c=0.05):\n",
    "    \n",
    "    def loss_grad_func(eval_point, *loss_func_args):\n",
    "        \"\"\"\n",
    "        Returns the gradient of the loss function at the given point.\n",
    "        If stochastic_approx is True, then the gradient is estimated using the SPSA rule.\n",
    "        \"\"\"\n",
    "        \n",
    "        loss_grad = []\n",
    "        #eval_point = np.array(eval_point)\n",
    "        \n",
    "        if not stochastic_approx: #Non-stochastic parameter shift rule\n",
    "            for input_dim in range(len(eval_point)):\n",
    "                dim_basis_vec = np.zeros(len(eval_point))\n",
    "                dim_basis_vec[input_dim] = 1.0\n",
    "                loss_grad.append((loss_func(eval_point + np.pi*dim_basis_vec/2, *loss_func_args) - loss_func(eval_point - np.pi*dim_basis_vec/2, *loss_func_args))/2)\n",
    "            loss_grad = np.array(loss_grad)\n",
    "        else: #SPSA\n",
    "            random_shift_vec = np.random.choice([-1,1],len(eval_point))\n",
    "            loss_grad = (loss_func(eval_point + stoch_shift_size_c*random_shift_vec, *loss_func_args) - loss_func(eval_point - stoch_shift_size_c*random_shift_vec, *loss_func_args))/(2*stoch_shift_size_c*random_shift_vec)\n",
    "        \n",
    "        return loss_grad\n",
    "    return loss_grad_func"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Meta-VQE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_step_circuit(qubits, thetas):\n",
    "    \"\"\"\n",
    "    Returns a circuit that applies the processing step of the VQE algorithm.\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "        qubits [list]: Qubits to apply the circuit to.\n",
    "        thetas [list or ndarray]: A list of the variational parameters.\n",
    "    \n",
    "    Yields:\n",
    "    ----------\n",
    "        [list]: Gates in the circuit.\n",
    "    \"\"\"\n",
    "    assert len(thetas) == 2 * 2 * len(qubits) # 2 paramterized layers per qubit with 2 rotation gates each\n",
    "    \n",
    "    rz_thetas = thetas[0:2*len(qubits)]\n",
    "    ry_thetas = thetas[2*len(qubits):4*len(qubits)]\n",
    "    \n",
    "    #Variational Layer 1:\n",
    "    for q_i, _ in enumerate(qubits):\n",
    "        yield [cirq.rz(rz_thetas[2*q_i])(qubits[q_i]), cirq.ry(ry_thetas[2*q_i])(qubits[q_i])] # Even indices in the theta vectors are for the first variational layer\n",
    "    \n",
    "    # In-between Entangling Layer (even control indices):\n",
    "    yield [cirq.CNOT(qubits[q_i], qubits[q_i+1]) for q_i in range(0, len(qubits)-1, 2)]\n",
    "    \n",
    "    #Variational Layer 2:\n",
    "    for q_i, _ in enumerate(qubits):\n",
    "        yield [cirq.rz(rz_thetas[2*q_i + 1])(qubits[q_i]), cirq.ry(ry_thetas[2*q_i + 1])(qubits[q_i])] # Odd indices in the theta vectors are for the second variational layer\n",
    "    \n",
    "    # Final Entangling Layer (odd control indices):\n",
    "    yield [cirq.CNOT(qubits[q_i], qubits[q_i+1]) for q_i in range(1, len(qubits)-1, 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               1               2               3\n",
      "│               │               │               │\n",
      "Rz(theta_0^(0)) Rz(theta_2^(0)) Rz(theta_4^(0)) Rz(theta_6^(0))\n",
      "│               │               │               │\n",
      "Ry(theta_0^(1)) Ry(theta_2^(1)) Ry(theta_4^(1)) Ry(theta_6^(1))\n",
      "│               │               │               │\n",
      "@───────────────X               @───────────────X\n",
      "│               │               │               │\n",
      "Rz(theta_1^(0)) Rz(theta_3^(0)) Rz(theta_5^(0)) Rz(theta_7^(0))\n",
      "│               │               │               │\n",
      "Ry(theta_1^(1)) Ry(theta_3^(1)) Ry(theta_5^(1)) Ry(theta_7^(1))\n",
      "│               │               │               │\n",
      "│               @───────────────X               │\n",
      "│               │               │               │\n"
     ]
    }
   ],
   "source": [
    "import sympy\n",
    "\n",
    "num_qubits = 4\n",
    "qubits = cirq.LineQubit.range(num_qubits)\n",
    "\n",
    "theta_0 = [sympy.Symbol(\"theta_{}^({})\".format(i, j)) for j in range(2) for i in range(2 * num_qubits)]\n",
    "\n",
    "circuit = cirq.Circuit(processing_step_circuit(qubits, theta_0))\n",
    "print(circuit.to_text_diagram(transpose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_function(Delta, phi):\n",
    "    \"\"\"\n",
    "    Returns the value of the encoding function for a given Delta and input vector phi\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "        Delta [float]: The value of the parameter Delta.\n",
    "        phi [list or ndarray]: A list of the variational parameters.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "        [ndarray]: The encoding function.\n",
    "    \"\"\"\n",
    "    weights = phi[:int(len(phi)/2)]\n",
    "    biases = phi[int(len(phi)/2):]\n",
    "    weights_z, weights_y = weights[:int(len(weights)/2)], weights[int(len(weights)/2):]\n",
    "    biases_z, biases_y = biases[:int(len(biases)/2)], biases[int(len(biases)/2):]\n",
    "    \n",
    "    return np.concatenate((weights_z * Delta + biases_z, weights_y * Delta + biases_y))\n",
    "\n",
    "def encoding_step_circuit(qubits, phi, Delta, encoding_function):\n",
    "    \"\"\"\n",
    "    Returns a circuit that applies the encoding step of the VQE algorithm.\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "        qubits [list]: Qubits to apply the circuit to.\n",
    "        phi [list or ndarray]: A list of the variational parameters.\n",
    "        Delta [float]: The value of the parameter Delta.\n",
    "        encoding_function [function]: The encoding function.\n",
    "    \n",
    "    Yields:\n",
    "    ----------\n",
    "        [list]: Gates in the circuit.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(phi) == 2 * 2 * 2 * len(qubits) # 2 paramterized layers per qubit with 2 fun\n",
    "    \n",
    "    yield processing_step_circuit(qubits, encoding_function(Delta, phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                           1                           2                           3\n",
      "│                           │                           │                           │\n",
      "Rz(Delta*w_0^(0) + p_0^(0)) Rz(Delta*w_2^(0) + p_2^(0)) Rz(Delta*w_4^(0) + p_4^(0)) Rz(Delta*w_6^(0) + p_6^(0))\n",
      "│                           │                           │                           │\n",
      "Ry(Delta*w_0^(1) + p_0^(1)) Ry(Delta*w_2^(1) + p_2^(1)) Ry(Delta*w_4^(1) + p_4^(1)) Ry(Delta*w_6^(1) + p_6^(1))\n",
      "│                           │                           │                           │\n",
      "@───────────────────────────X                           @───────────────────────────X\n",
      "│                           │                           │                           │\n",
      "Rz(Delta*w_1^(0) + p_1^(0)) Rz(Delta*w_3^(0) + p_3^(0)) Rz(Delta*w_5^(0) + p_5^(0)) Rz(Delta*w_7^(0) + p_7^(0))\n",
      "│                           │                           │                           │\n",
      "Ry(Delta*w_1^(1) + p_1^(1)) Ry(Delta*w_3^(1) + p_3^(1)) Ry(Delta*w_5^(1) + p_5^(1)) Ry(Delta*w_7^(1) + p_7^(1))\n",
      "│                           │                           │                           │\n",
      "│                           @───────────────────────────X                           │\n",
      "│                           │                           │                           │\n"
     ]
    }
   ],
   "source": [
    "import sympy\n",
    "\n",
    "num_qubits = 4\n",
    "qubits = cirq.LineQubit.range(num_qubits)\n",
    "\n",
    "phi_0 = np.array([sympy.Symbol(\"{}_{}^({})\".format(phi_type, i, j)) for phi_type in [\"w\", \"p\"] for j in range(2) for i in range(2 * num_qubits)])\n",
    "\n",
    "circuit = cirq.Circuit(encoding_step_circuit(qubits, phi_0, Delta=sympy.Symbol(\"Delta\") ,encoding_function=encoding_function))\n",
    "print(circuit.to_text_diagram(transpose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xxz_chain_hamiltonian_1D(qubits, hamilt_params):\n",
    "    \"\"\"\n",
    "    Returns tuple of generators for every hamiltonian term in the XXZ chain hamiltonian\n",
    "    such that each inidivdual term is given as (coefficient, gate set)\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "        qubits [list]: Qubits to apply the circuit to.\n",
    "        hamilt_params [list]: A list of the hamiltonian parameters.\n",
    "        \n",
    "    Returns:\n",
    "    ----------\n",
    "        [tuple]: Tuple of tuples of hamiltonian term coefficient and gate generators.\n",
    "    \"\"\"\n",
    "    \n",
    "    def x_interact_term():\n",
    "        for q_i, _ in enumerate(qubits):\n",
    "            yield cirq.X(qubits[q_i]) * cirq.X(qubits[(q_i+1) % len(qubits)])\n",
    "    \n",
    "    def y_interact_term():\n",
    "        for q_i, _ in enumerate(qubits):\n",
    "            yield cirq.Y(qubits[q_i]) * cirq.Y(qubits[(q_i+1) % len(qubits)])\n",
    "    \n",
    "    def z_interact_term():\n",
    "        for q_i, _ in enumerate(qubits):\n",
    "            yield cirq.Z(qubits[q_i]) * cirq.Z(qubits[(q_i+1) % len(qubits)])\n",
    "    \n",
    "    def z_align_term():\n",
    "        for q_i, _ in enumerate(qubits):\n",
    "            yield cirq.Z(qubits[q_i])\n",
    "\n",
    "    return (1.0, x_interact_term()), (1.0, y_interact_term()),  (hamilt_params[\"Delta\"],z_interact_term()), (hamilt_params[\"lambda\"], z_align_term())\n",
    "\n",
    "def get_XXZ_hamiltonian_measurement_basis_change(qubits, hamilt_params):\n",
    "    \"\"\"\n",
    "    Returns the gates for the measurement basis change needed to measure each commuting set of terms in the XXZ chain hamiltonian.\n",
    "    Equivalent to applying to every pauli-string the .to_z_basis_op() method.\n",
    "    Note that the following code is generalizable to other hamiltonians\n",
    "    However, the iteration over terms (i.e. term_tuples) assumes that all pauli strings within each term commute with each other\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "        qubits [list]: Qubits to apply the circuit to.\n",
    "        hamilt_params [list]: A list of the hamiltonian parameters.\n",
    "    \n",
    "    Yields:\n",
    "    ----------\n",
    "        [list]: Gates in the circuit.\n",
    "    \"\"\"\t\n",
    "    \n",
    "    term_tuples = xxz_chain_hamiltonian_1D(qubits, hamilt_params)\n",
    "    \n",
    "    for term_coeff, term_generator in term_tuples:\n",
    "        \n",
    "        measured_strings = [] # list of pauli operator objects\n",
    "        term_measurement_gateset = []\n",
    "        measured_qubits = []\n",
    "        \n",
    "        for pauli_str in term_generator:\n",
    "            measured_strings.append(pauli_str)\n",
    "            for qubit, operator in pauli_str.items():\n",
    "                if operator == cirq.X and qubit not in measured_qubits:\n",
    "                    term_measurement_gateset.append(cirq.H(qubit))\n",
    "                    measured_qubits.append(qubit)\n",
    "                elif operator == cirq.Y and qubit not in measured_qubits:\n",
    "                    term_measurement_gateset.append(cirq.inverse(cirq.S(qubit)))\n",
    "                    term_measurement_gateset.append(cirq.H(qubit))\n",
    "                    measured_qubits.append(qubit)\n",
    "\n",
    "        yield (term_coeff, measured_strings), term_measurement_gateset\n",
    "\n",
    "#def get_hamilt_term_fold_func(term_pauli_strings):\n",
    "#    def term_fold_func(meas_results):\n",
    "#        #Change from binary to -1 and 1\n",
    "#        meas_results = 1 - 2 * np.array([meas_results[q_i] for q_i in range(len(meas_results))]).astype(int)\n",
    "#        \n",
    "#        energy_tot = np.sum([np.prod([meas_results[q_i] for q_i, _ in enumerate(pauli_str.qubits)]) for pauli_str in term_pauli_strings])\n",
    "#        \n",
    "#        return energy_tot\n",
    "#    return term_fold_func\n",
    "\n",
    "def meas_hamilt_expectation(qubits, circuit_params, hamilt_params, n_meas_reps = 1000):\n",
    "    \"\"\"\n",
    "    Returns the expectation value of the hamiltonian on the state defined by the given circuit parameters.\n",
    "    Non-cheating version, i.e. the state vector is not computed but instead the expectation value is estimated by sampling measurement outcomes\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "        qubits [list]: Qubits to apply the circuit to.\n",
    "        circuit_params [list]: A list of the circuit parameters.\n",
    "        hamilt_params [list]: A list of the hamiltonian parameters.\n",
    "        n_meas_reps [int]: Number of measurement repetitions.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "        [float]: Expectation value of the hamiltonian.\n",
    "    \n",
    "    \"\"\"\n",
    "    energy_expect = 0.0\n",
    "    \n",
    "    #Iterate over terms with commuting pauli strings in the hamiltonian\n",
    "    for measured_term, term_basis_change_gates in get_XXZ_hamiltonian_measurement_basis_change(qubits, hamilt_params):\n",
    "        \n",
    "        term_coeff, measured_strings = measured_term\n",
    "\n",
    "        #Note that we have to re-initialize the circuit for each term, since those are generators\n",
    "        encoding_step = encoding_step_circuit(qubits, circuit_params[:int(8*len(qubits))], Delta=hamilt_params['Delta'] ,encoding_function=encoding_function)\n",
    "        processing_step = processing_step_circuit(qubits, circuit_params[int(8*len(qubits)):])\n",
    "\n",
    "        #Prepare, run and measure the circuit \n",
    "        term_basis_change_gates.extend([cirq.measure(q, key=f\"%d\" % q.x) for q in qubits])\n",
    "        \n",
    "        #OR: term_basis_change_gates = cirq.measure_paulistring_terms(cirq.PauliString({q_i: measured_strings[0]._qubit_pauli_map[qubits[0]] for q_i in qubits}), key_func = lambda q: f\"%d\" % q.x)\n",
    "\n",
    "        #print(\"%%%%%%%%%%%%%%%%%% Next %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "        term_meas_circuit = cirq.Circuit([encoding_step,\n",
    "                                    processing_step,\n",
    "                                    term_basis_change_gates,\n",
    "                                    ])\n",
    "        \n",
    "        #print(term_meas_circuit.to_text_diagram(transpose=True))\n",
    "        \n",
    "        #print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "        \n",
    "        \n",
    "        simulator = cirq.Simulator()\n",
    "        results = simulator.run(term_meas_circuit, repetitions=int(n_meas_reps))\n",
    "        \n",
    "        meas_outcomes_hist = results.multi_measurement_histogram(keys=[f\"%d\" % q.x for q in qubits])\n",
    "        assert n_meas_reps == int(np.sum(list(meas_outcomes_hist.values())))\n",
    "        \n",
    "        #print(\"Finished term measurements: \", meas_outcomes_hist)\n",
    "        \n",
    "        term_expect = 0.0\n",
    "        for pauli_str in measured_strings:\n",
    "            pauli_str_expect = 0.0\n",
    "            for meas_outcomes, count in meas_outcomes_hist.items():\n",
    "                assert len(meas_outcomes) == len(qubits)\n",
    "                pauli_str_eigval = np.prod([1 - 2 * meas_outcomes[q_i.x] for q_i in pauli_str.qubits])\n",
    "                #print(pauli_str.qubits,[q_i.x for q_i in pauli_str.qubits],[meas_outcomes[q_i.x] for q_i in pauli_str.qubits], [1 - 2 * meas_outcomes[q_i.x] for q_i in pauli_str.qubits])\n",
    "                pauli_str_expect += pauli_str_eigval * count/n_meas_reps\n",
    "            \n",
    "            term_expect += pauli_str_expect\n",
    "            \n",
    "        energy_expect += term_coeff * term_expect\n",
    "    \n",
    "    return energy_expect\n",
    "\n",
    "def direct_hamilt_expectation(qubits, circuit_params, hamilt_params):\n",
    "    \"\"\"\n",
    "    Returns the expectation value of the hamiltonian on the state defined by the given circuit parameters.\n",
    "    Cheating version, i.e. the state vector is computed exactly and the hamiltonian matrix representation is used to compute the expectation value directly; no sampling noise.\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "        qubits [list]: Qubits to apply the circuit to.\n",
    "        circuit_params [list]: A list of the circuit parameters.\n",
    "        hamilt_params [list]: A list of the hamiltonian parameters.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "        [float]: Expectation value of the hamiltonian.\n",
    "    \"\"\"\n",
    "    \n",
    "    energy_expect = 0.0\n",
    "\n",
    "    #Iterate over terms with commuting pauli strings in the hamiltonian\n",
    "    for measured_term, term_basis_change_gates in get_XXZ_hamiltonian_measurement_basis_change(qubits, hamilt_params):\n",
    "        \n",
    "        term_coeff, measured_strings = measured_term\n",
    "\n",
    "        #Note that we have to re-initialize the circuit for each term, since those are generators\n",
    "        encoding_step = encoding_step_circuit(qubits, circuit_params[:int(8*len(qubits))], Delta=hamilt_params['Delta'] ,encoding_function=encoding_function)\n",
    "        processing_step = processing_step_circuit(qubits, circuit_params[int(8*len(qubits)):])\n",
    "\n",
    "        pqc_circuit = cirq.Circuit([encoding_step, processing_step])\n",
    "        expval = lambda state: np.real(sum([state.conj() @ pauli_str.matrix(qubits) @ state for pauli_str in measured_strings]))\n",
    "        \n",
    "        simulator = cirq.Simulator()\n",
    "        fin_state_vec = simulator.simulate(pqc_circuit, qubit_order=[*qubits]).final_state_vector\n",
    "        #print(\"Term energy expect: \",expval(fin_state_vec), \" for state vector: \", fin_state_vec)\n",
    "        energy_expect += term_coeff*expval(fin_state_vec)\n",
    "    \n",
    "    return energy_expect\n",
    "\n",
    "\n",
    "def exact_hamilt_GS_energy(qubits, hamilt_params):\n",
    "    \"\"\"\n",
    "    Returns the exact ground state energy of the hamiltonian.\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "        qubits [list]: Qubits to apply the circuit to.\n",
    "        hamilt_params [list]: A list of the hamiltonian parameters.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "        [float]: Exact ground state energy of the hamiltonian.\n",
    "    \"\"\"\n",
    "    \n",
    "    hamilt_matrix = np.zeros((2**len(qubits), 2**len(qubits)), dtype=np.complex128)\n",
    "    \n",
    "    term_tuples = xxz_chain_hamiltonian_1D(qubits, hamilt_params)\n",
    "    for term_coeff, term_generator in term_tuples:\n",
    "        for pauli_str in term_generator:\n",
    "            hamilt_matrix += term_coeff*pauli_str.matrix(qubits)\n",
    "    \n",
    "    eigvals = np.linalg.eigvalsh(hamilt_matrix)\n",
    "    gs_energy = np.min(eigvals)\n",
    "    \n",
    "    return gs_energy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ Direct Hamiltonian Circuits ############\n",
      "\n",
      "The following term has coefficient 1.0 \n",
      "\n",
      "0               1               2\n",
      "│               │               │\n",
      "PauliString(+X)─X               │\n",
      "│               │               │\n",
      "│               PauliString(+X)─X\n",
      "│               │               │\n",
      "X───────────────┼───────────────PauliString(+X)\n",
      "│               │               │\n",
      "\n",
      "The following term has coefficient 1.0 \n",
      "\n",
      "0               1               2\n",
      "│               │               │\n",
      "PauliString(+Y)─Y               │\n",
      "│               │               │\n",
      "│               PauliString(+Y)─Y\n",
      "│               │               │\n",
      "Y───────────────┼───────────────PauliString(+Y)\n",
      "│               │               │\n",
      "\n",
      "The following term has coefficient 2.0 \n",
      "\n",
      "0               1               2\n",
      "│               │               │\n",
      "PauliString(+Z)─Z               │\n",
      "│               │               │\n",
      "│               PauliString(+Z)─Z\n",
      "│               │               │\n",
      "Z───────────────┼───────────────PauliString(+Z)\n",
      "│               │               │\n",
      "\n",
      "The following term has coefficient 2.0 \n",
      "\n",
      "0 1 2\n",
      "│ │ │\n",
      "Z Z Z\n",
      "│ │ │\n",
      "\n",
      "############ Measurement Basis Change Circuits ############\n",
      "\n",
      "The following term has coefficient 1.0 \n",
      "\n",
      "0                  1 2\n",
      "│                  │ │\n",
      "H                  H H\n",
      "│                  │ │\n",
      "M('Zmeasurements')─M─M\n",
      "│                  │ │\n",
      "\n",
      "The following term has coefficient 1.0 \n",
      "\n",
      "0                  1    2\n",
      "│                  │    │\n",
      "S^-1               S^-1 S^-1\n",
      "│                  │    │\n",
      "H                  H    H\n",
      "│                  │    │\n",
      "M('Zmeasurements')─M────M\n",
      "│                  │    │\n",
      "\n",
      "The following term has coefficient 2.0 \n",
      "\n",
      "0                  1 2\n",
      "│                  │ │\n",
      "M('Zmeasurements')─M─M\n",
      "│                  │ │\n",
      "\n",
      "The following term has coefficient 2.0 \n",
      "\n",
      "0                  1 2\n",
      "│                  │ │\n",
      "M('Zmeasurements')─M─M\n",
      "│                  │ │\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sympy\n",
    "\n",
    "num_qubits = 3\n",
    "qubits = cirq.LineQubit.range(num_qubits)\n",
    "\n",
    "hamilt_params = {}\n",
    "hamilt_params[\"lambda\"] =  2.0  # Transverse field strength\n",
    "hamilt_params[\"Delta\"] = 2.0 # Anisotropy parameter\n",
    "\n",
    "print(\"############ Direct Hamiltonian Circuits ############\")\n",
    "\n",
    "for term_coeff, term_generator in xxz_chain_hamiltonian_1D(qubits, hamilt_params):\n",
    "    print(\"\\nThe following term has coefficient\", term_coeff, \"\\n\")\n",
    "    circuit = cirq.Circuit(term_generator)\n",
    "    print(circuit.to_text_diagram(transpose=True))\n",
    "\n",
    "print(\"\\n############ Measurement Basis Change Circuits ############\")\n",
    "\n",
    "for measured_term, term_basis_change_gates in get_XXZ_hamiltonian_measurement_basis_change(qubits, hamilt_params):\n",
    "    term_coeff, measured_strings = measured_term\n",
    "    print(\"\\nThe following term has coefficient\", term_coeff, \"\\n\")\n",
    "    term_basis_change_gates.append(cirq.measure(*qubits, key='Zmeasurements'))\n",
    "    circuit = cirq.Circuit(term_basis_change_gates)\n",
    "    print(circuit.to_text_diagram(transpose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.487918414869853"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_qubits = 8\n",
    "qubits = cirq.LineQubit.range(num_qubits)\n",
    "\n",
    "hamilt_params = {}\n",
    "hamilt_params[\"lambda\"] =  0.75  # Transverse field strength\n",
    "hamilt_params[\"Delta\"] = 0.5 # Anisotropy parameter\n",
    "\n",
    "exact_hamilt_GS_energy(qubits, hamilt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19441500000000017\n",
      "0.19132160504954204\n"
     ]
    }
   ],
   "source": [
    "#np.random.seed(0)\n",
    "#np.random.seed(7) #43\n",
    "init_phi_params = np.random.normal(0, np.pi/2, 8*len(qubits))\t\n",
    "init_theta_params = np.random.normal(0, np.pi/2, 4*len(qubits))\n",
    "init_circuit_params = np.concatenate([init_phi_params, init_theta_params])\n",
    "\n",
    "print(meas_hamilt_expectation(qubits, init_circuit_params, hamilt_params,100000))\n",
    "print(direct_hamilt_expectation(qubits, init_circuit_params, hamilt_params))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct training set of hamiltonian expectation values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def construct_training_set(train_set_size, min_hamilt_params = {\"lambda\": 0.75, \"Delta\": -1.1}, max_hamilt_params = {\"lambda\": 0.75, \"Delta\": 1.1}, mode=\"equidistant\"):\n",
    "    \"\"\"\n",
    "    Constructs a training set of hamiltonian parameters.\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "        train_set_size [int]: Size of the training set.\n",
    "        min_hamilt_params [dict]: Dictionary of the minimum hamiltonian parameters.\n",
    "        max_hamilt_params [dict]: Dictionary of the maximum hamiltonian parameters.\n",
    "        mode [str]: Mode of the training set construction. Either \"equidistant\" or \"uniform_random\".\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "        [list]: List of hamiltonian parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert min_hamilt_params.keys() == max_hamilt_params.keys()\n",
    "\n",
    "    training_set = []\n",
    "    for i in range(train_set_size):\n",
    "        sample_i = {}\n",
    "        for param in min_hamilt_params.keys():\n",
    "            if mode == \"equidistant\":\n",
    "                sample_i[param] = min_hamilt_params[param] + i*(max_hamilt_params[param] - min_hamilt_params[param])/(train_set_size-1)\n",
    "            elif mode == \"uniform_random\": \n",
    "                sample_i[param] = np.random.uniform(min_hamilt_params[param], max_hamilt_params[param])\n",
    "        \n",
    "        training_set.append(sample_i)\n",
    "    \n",
    "    return training_set   \n",
    " \n",
    "def construct_test_set(test_set_size, min_hamilt_params = {\"lambda\": 0.75, \"Delta\": -1.1}, max_hamilt_params = {\"lambda\": 0.75, \"Delta\": 1.1}, mode=\"uniform_random\"):\n",
    "    \"\"\"\n",
    "    Constructs a test set of hamiltonian parameters.\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "        test_set_size [int]: Size of the test set.\n",
    "        min_hamilt_params [dict]: Dictionary of the minimum hamiltonian parameters.\n",
    "        max_hamilt_params [dict]: Dictionary of the maximum hamiltonian parameters.\n",
    "        mode [str]: Mode of the test set construction. Either \"equidistant\" or \"uniform_random\".\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "        [list]: List of hamiltonian parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    return construct_training_set(test_set_size, min_hamilt_params, max_hamilt_params, mode)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lambda': 0.75, 'Delta': -1.1}, {'lambda': 0.75, 'Delta': -0.8555555555555556}, {'lambda': 0.75, 'Delta': -0.6111111111111112}, {'lambda': 0.75, 'Delta': -0.3666666666666667}, {'lambda': 0.75, 'Delta': -0.12222222222222223}, {'lambda': 0.75, 'Delta': 0.12222222222222223}, {'lambda': 0.75, 'Delta': 0.3666666666666667}, {'lambda': 0.75, 'Delta': 0.6111111111111112}, {'lambda': 0.75, 'Delta': 0.8555555555555556}, {'lambda': 0.75, 'Delta': 1.1}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_set_size = 10\n",
    "min_hamilt_params = {\"lambda\": 0.75, \"Delta\": -1.1}\n",
    "max_hamilt_params = {\"lambda\": 0.75, \"Delta\": 1.1}\n",
    "\n",
    "print(construct_training_set(train_set_size=train_set_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import OptimizeResult\n",
    "def adam(\n",
    "    fun,\n",
    "    x0,\n",
    "    jac,\n",
    "    args=(),\n",
    "    learning_rate=0.04, #0.001,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    eps=1e-8,\n",
    "    startiter=0,\n",
    "    maxiter=10000,\n",
    "    callback=None,\n",
    "    **kwargs):\n",
    "    \"\"\"scipy.optimize.minimize compatible implementation of ADAM - [http://arxiv.org/pdf/1412.6980.pdf].\n",
    "    Adapted from autograd/misc/optimizers.py.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x0\n",
    "    m = np.zeros_like(x)\n",
    "    v = np.zeros_like(x)\n",
    "\n",
    "    for i in range(startiter, startiter + maxiter):\n",
    "        g = jac(x, *args)\n",
    "\n",
    "        if callback and callback(x):\n",
    "            break\n",
    "\n",
    "        m = (1 - beta1) * g + beta1 * m  # first  moment estimate.\n",
    "        v = (1 - beta2) * (g**2) + beta2 * v  # second moment estimate.\n",
    "        mhat = m / (1 - beta1**(i + 1))  # bias correction.\n",
    "        vhat = v / (1 - beta2**(i + 1))\n",
    "        x = x - learning_rate * mhat / (np.sqrt(vhat) + eps)\n",
    "\n",
    "    i += 1\n",
    "    return OptimizeResult(x=x, fun=fun(x, *args), jac=g, nit=i, nfev=i, success=True)\n",
    "\n",
    "#Write a function that implements adam with a decaying adaptive learning rate\n",
    "def adam_with_lr_decay(\n",
    "    fun,\n",
    "    x0,\n",
    "    jac,\n",
    "    args=(),\n",
    "    init_learning_rate=0.2, #0.001,\n",
    "    lr_decay_rate=0.6,\n",
    "    lr_decay_steps=10000,\n",
    "    lr_decay_type=\"exponential\",\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    eps=1e-8,\n",
    "    startiter=0,\n",
    "    maxiter=10000,\n",
    "    callback=None,\n",
    "    **kwargs):\n",
    "    \"\"\"scipy.optimize.minimize compatible implementation of ADAM with learning rate decay - [http://arxiv.org/pdf/1412.6980.pdf].\n",
    "    Adapted from autograd/misc/optimizers.py.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    m = np.zeros_like(x)\n",
    "    v = np.zeros_like(x)\n",
    "    lr = init_learning_rate\n",
    "\n",
    "    for i in range(startiter, startiter + maxiter):\n",
    "        g = jac(x, *args)\n",
    "\n",
    "        if callback and callback(x):\n",
    "            break\n",
    "\n",
    "        m = (1 - beta1) * g + beta1 * m\n",
    "        v = (1 - beta2) * (g**2) + beta2 * v\n",
    "        mhat = m / (1 - beta1**(i + 1))\n",
    "        vhat = v / (1 - beta2**(i + 1))\n",
    "        if lr_decay_type == \"exponential\":\n",
    "            lr = init_learning_rate * (lr_decay_rate ** (i/lr_decay_steps))\n",
    "        elif lr_decay_type == \"linear\":\n",
    "            lr = init_learning_rate/(i+1)\n",
    "        elif lr_decay_type == \"factor\":\n",
    "            lr *= lr_decay_rate\n",
    "        \n",
    "        x = x - lr * mhat / (np.sqrt(vhat) + eps)\n",
    "        \n",
    "    i += 1\n",
    "    return OptimizeResult(x=x, fun=fun(x, *args), jac=g, nit=i, nfev=i, success=True)\n",
    "\n",
    "\n",
    "def adamw_with_lr_decay(\n",
    "    fun,\n",
    "    x0,\n",
    "    jac,\n",
    "    args=(),\n",
    "    init_learning_rate=0.04,\n",
    "    lr_decay_rate=0.5,\n",
    "    lr_decay_steps=200,\n",
    "    lr_decay_type=\"exponential\",\n",
    "    weight_decay=0.01, # weight decay parameter for AdamW\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    eps=1e-8,\n",
    "    startiter=0,\n",
    "    maxiter=10000,\n",
    "    callback=None,\n",
    "    **kwargs):\n",
    "    \"\"\"scipy.optimize.minimize compatible implementation of ADAMW without learning rate decay - [http://arxiv.org/pdf/1412.6980.pdf].\n",
    "    Adapted from autograd/misc/optimizers.py.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x0\n",
    "    m = np.zeros_like(x)\n",
    "    v = np.zeros_like(x)\n",
    "    lr = init_learning_rate\n",
    "\n",
    "    for i in range(startiter, startiter + maxiter):\n",
    "        g = jac(x, *args)\n",
    "\n",
    "        if callback and callback(x):\n",
    "            break\n",
    "\n",
    "        m = (1 - beta1) * g + beta1 * m  # momentum\n",
    "        v = (1 - beta2) * (g**2) + beta2 * v  # RMSProp\n",
    "\n",
    "        mhat = m / (1 - beta1**(i + 1))  # bias correction\n",
    "        vhat = v / (1 - beta2**(i + 1))  # bias correction\n",
    "\n",
    "        if lr_decay_type == \"exponential\":\n",
    "            lr = init_learning_rate * (lr_decay_rate ** (i/lr_decay_steps))\n",
    "        elif lr_decay_type == \"linear\":\n",
    "            lr = init_learning_rate/(i+1)\n",
    "        elif lr_decay_type == \"factor\":\n",
    "            lr *= lr_decay_rate\n",
    "\n",
    "        step_size = lr * mhat / (np.sqrt(vhat) + eps)\n",
    "\n",
    "        x = (1 - weight_decay) * x - step_size  # AdamW update rule\n",
    "        \n",
    "    i += 1\n",
    "    return OptimizeResult(x=x, fun=fun(x, *args), jac=g, nit=i, nfev=i, success=True)\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_circuit_params(qubits, params_init_mode):\n",
    "    \"\"\"\n",
    "    Initialize the parameters of the circuit.\n",
    "    \n",
    "    Args:\n",
    "    ----------------\n",
    "        qubits [list]: list of qubits\n",
    "        params_init_mode [str]: mode of initialization of the parameters.\n",
    "            \"zero\": all parameters are initialized to zero\n",
    "            \"normal_random\": all parameters are initialized to a random value drawn from a normal distribution\n",
    "    \n",
    "    Returns:\n",
    "    ----------------\n",
    "        init_circuit_params [ndarray]: array of the initial parameters of the circuit\n",
    "    \"\"\"\n",
    "    \n",
    "    init_circuit_params = None\n",
    "    \n",
    "    if params_init_mode == \"zero\":\n",
    "        init_phi_params = np.zeros(8*len(qubits))\n",
    "        init_theta_params = np.zeros(4*len(qubits))\n",
    "        init_circuit_params = np.concatenate([init_phi_params, init_theta_params])\n",
    "    elif params_init_mode == \"normal_random\":\n",
    "        #init_phi_params = np.random.normal(-np.pi/2, np.pi/2, 8*len(qubits))\t\n",
    "        #init_theta_params = np.random.normal(-np.pi/2, np.pi/2, 4*len(qubits))\n",
    "        init_phi_params = np.random.normal(-0.02, 0.02, 8*len(qubits))\t\n",
    "        init_theta_params = np.random.normal(-0.02, 0.02, 4*len(qubits))\n",
    "        init_circuit_params = np.concatenate([init_phi_params, init_theta_params])\n",
    "    return init_circuit_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import time\n",
    "\n",
    "\n",
    "def meta_vqe_energy_loss(circuit_params, qubits, hamilt_params_training_set, n_meas_reps=1000):\n",
    "    \"\"\"\n",
    "    Compute the energy loss of the meta_vqe circuit.\n",
    "    \n",
    "    Args:\n",
    "    ----------------\n",
    "        circuit_params [ndarray]: array of the parameters of the circuit\n",
    "        qubits [list]: list of qubits\n",
    "        hamilt_params_training_set [list]: list of the parameters of the hamiltonians\n",
    "        n_meas_reps [int]: number of measurement repetitions\n",
    "\n",
    "    Returns:\n",
    "    ----------------\n",
    "        energy_loss [float]: energy loss of the meta_vqe circuit\n",
    "    \"\"\"\n",
    "    \n",
    "    energy_loss = 0\n",
    "    \n",
    "    #start_time = time.time()\n",
    "    for hamilt_params_sample in hamilt_params_training_set:\n",
    "        #energy_loss += meas_hamilt_expectation(qubits, circuit_params, hamilt_params_sample, n_meas_reps)\n",
    "        energy_loss += direct_hamilt_expectation(qubits, circuit_params, hamilt_params_sample)\n",
    "    \n",
    "    #print(\"Time taken for the meta_vqe loss evaluation: \", time.time() - start_time)\n",
    "    \n",
    "    return energy_loss\n",
    "\n",
    "\n",
    "def train_meta_vqe(qubits, hamilt_params_training_set, init_circuit_params=None, params_init_mode=\"normal_random\", n_meas_reps=1000):\n",
    "    \"\"\"\n",
    "    Train the meta_vqe circuit and return the trained PQC parameters.\n",
    "    \n",
    "    Args:\n",
    "    ----------------\n",
    "        qubits [list]: list of qubits\n",
    "        hamilt_params_training_set [list]: list of the parameters of the hamiltonians\n",
    "        init_circuit_params [ndarray]: array of the initial parameters of the circuit\n",
    "        params_init_mode [str]: mode of initialization of the parameters.\n",
    "\n",
    "    Returns:\n",
    "    ----------------\n",
    "        trained_circuit_params [ndarray]: array of the trained parameters of the circuit\n",
    "    \"\"\"\n",
    "    \n",
    "    if init_circuit_params is None:\n",
    "        init_circuit_params = initialize_circuit_params(qubits, params_init_mode)\n",
    "    \n",
    "    #train_loss_func = lambda circuit_params: meta_vqe_energy_loss(circuit_params, hamilt_params_training_set = hamilt_params_training_set)\n",
    "\n",
    "    def callbackF(Xi):\n",
    "        global Nfeval\n",
    "        print(\"Iter: \", Nfeval, \" Loss on train set: \", meta_vqe_energy_loss(Xi, qubits, hamilt_params_training_set, n_meas_reps=1000))\n",
    "        Nfeval += 1\n",
    "\n",
    "    print(\"Train loss should converge to \", sum(exact_hamilt_GS_energy(qubits, hamilt_params_training_set[i]) for i in range(len(hamilt_params_training_set))))\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    loss_args = (qubits, hamilt_params_training_set, n_meas_reps)\n",
    "    meta_vqe_loss_grad_fun = prepare_loss_gradient_approx(meta_vqe_energy_loss, stochastic_approx=True, stoch_shift_size_c=0.01)\n",
    "    #meta_vqe_loss_grad_fun = prepare_loss_gradient_approx(lambda x: meta_vqe_energy_loss(x, *loss_args), stochastic_approx=True, stoch_shift_size_c=0.001)\n",
    "    \n",
    "    optim_result = minimize(meta_vqe_energy_loss, #train_loss_func, \n",
    "                            args=loss_args,\n",
    "                            x0=init_circuit_params, \n",
    "                            method='L-BFGS-B', #adamw_with_lr_decay, #'L-BFGS-B', #'COBYLA', #adam_with_lr_decay,,#'COBYLA', \n",
    "                            callback=callbackF,\n",
    "                            #jac=meta_vqe_loss_grad_fun, #None,\n",
    "                            options={'maxiter': 12000},\n",
    "                            )\n",
    "    print(optim_result)\n",
    "    #print(optim_result.fun)\n",
    "    return optim_result.x\n",
    "\n",
    "def evaluate_meta_vqe(qubits, circuit_params, hamilt_params_test_set, n_meas_reps=1000):\n",
    "    \"\"\"\n",
    "    Evaluate the meta_vqe circuit on a test set of hamiltonian parameters.\n",
    "\n",
    "    Args:\n",
    "    ----------------\n",
    "        qubits [list]: list of qubits\n",
    "        circuit_params [ndarray]: array of the parameters of the circuit\n",
    "        hamilt_params_test_set [list]: list of the parameters of the hamiltonians\n",
    "        n_meas_reps [int]: number of measurement repetitions\n",
    "    \n",
    "    Returns:\n",
    "    ----------------\n",
    "        energy_expectations [list]: list of the energy expectations\n",
    "        exact_energy_expectations [list]: list of the exact energy expectations\n",
    "        abs_energy_errors [list]: list of the absolute energy errors\n",
    "    \"\"\"\n",
    "    \n",
    "    #Evaluate meta-VQE on test set:\n",
    "    energy_expectations = []\n",
    "    exact_energy_expectations = []\n",
    "    abs_energy_errors = []\n",
    "    \n",
    "    #Infere on test set:\n",
    "    for hamilt_params_sample in hamilt_params_test_set:\n",
    "        #sample_energy_expect = meas_hamilt_expectation(qubits, circuit_params, hamilt_params_sample, n_meas_reps)\n",
    "        sample_energy_expect =  direct_hamilt_expectation(qubits, circuit_params, hamilt_params_sample)\n",
    "        energy_expectations.append(sample_energy_expect)\n",
    "        sample_exact_energy_expect = exact_hamilt_GS_energy(qubits, hamilt_params_sample)\n",
    "        exact_energy_expectations.append(sample_exact_energy_expect)\n",
    "        abs_energy_errors.append(np.abs(sample_energy_expect - sample_exact_energy_expect))\n",
    "    \n",
    "    print(\"Meta-VQE absolute energy errors: \", abs_energy_errors)\n",
    "    return np.array(energy_expectations), np.array(abs_energy_errors)\n",
    "\n",
    "def meta_vqe(qubits, hamilt_params_training_set, hamilt_params_test_set):\n",
    "    \"\"\"\n",
    "    Convenience wrapper to train and evaluate the meta_vqe circuit.\n",
    "\n",
    "    Args:\n",
    "    ----------------\n",
    "        qubits [list]: list of qubits\n",
    "        hamilt_params_training_set [list]: list of the parameters of the hamiltonians\n",
    "        hamilt_params_test_set [list]: list of the parameters of the hamiltonians\n",
    "    \n",
    "    Returns:\n",
    "    ----------------\n",
    "        energy_expectations [list]: list of the energy expectations\n",
    "        abs_energy_errors [list]: list of the absolute energy errors\n",
    "    \"\"\"\n",
    "    \n",
    "    #Train meta-VQE:\n",
    "    opt_circuit_params = train_meta_vqe(qubits, hamilt_params_training_set, params_init_mode=\"normal_random\", n_meas_reps=1000)\n",
    "    \n",
    "    #Evaluate meta-VQE on test set:\n",
    "    energy_expectations, abs_energy_errors = evaluate_meta_vqe(qubits, opt_circuit_params, hamilt_params_test_set, n_meas_reps=1000)\n",
    "    \n",
    "    return energy_expectations, abs_energy_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss should converge to  -125.38941301973591\n",
      "Starting training...\n",
      "Iter:  1  Loss on train set:  59.494582553396995\n",
      "Iter:  2  Loss on train set:  59.49457959643691\n",
      "Iter:  3  Loss on train set:  59.494579587651344\n",
      "      fun: 59.494579587651344\n",
      " hess_inv: <96x96 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([-4.58587849e+01, -3.51306149e+01, -1.96685995e+01, -2.95134065e+01,\n",
      "        1.47565515e-02, -3.03259640e-03,  8.22808488e-03, -4.21280788e-03,\n",
      "        1.48186776e+02, -3.32231309e-01,  1.48234312e+02,  1.48186560e+02,\n",
      "        1.50261770e+02,  1.50262547e+02,  1.75834742e+00,  1.48169026e+02,\n",
      "        6.07300876e-03, -1.23804895e+00, -1.57233870e-01, -1.42883749e-01,\n",
      "       -1.93480787e-02, -6.00171290e-01, -1.66971859e-01, -1.24271082e-01,\n",
      "       -1.25258026e-01, -9.13205156e-01, -4.53667326e-01, -4.85259477e-01,\n",
      "       -2.03699102e-01, -4.02663147e-01, -2.86456512e+00, -2.86354691e+00,\n",
      "       -4.47456280e+01, -4.47206581e+01, -4.47439092e+01, -4.42845383e+01,\n",
      "       -1.58883296e+01,  1.54600457e+00, -1.47464519e+01, -1.08492770e-02,\n",
      "       -1.62893080e+01, -1.62409165e+01,  1.32275827e+02,  1.33419864e+02,\n",
      "        1.50836772e+02,  1.35270669e+02,  2.66193680e+00,  1.34958159e+02,\n",
      "        2.81232815e-02, -1.37220297e+00, -2.72814304e-01, -2.91906588e-01,\n",
      "        5.56738655e-02, -7.77760079e-02, -7.86995003e-01, -9.01824393e-01,\n",
      "       -3.52794416e-01, -1.43706416e+00, -9.58574020e-01, -9.88794113e-01,\n",
      "       -1.16940981e+00, -4.31683134e-01, -3.93799482e+00, -4.95664949e+00,\n",
      "       -3.99532055e+01,  1.13810685e+00, -6.11061942e+01, -1.33258709e+01,\n",
      "       -9.11684481e+01,  1.88997191e+00,  2.07945590e+00, -1.97313952e+01,\n",
      "        5.68377196e+01,  1.33677486e+02,  2.65335345e+00, -2.19336059e+01,\n",
      "        1.59502510e+02, -1.22639392e+00, -1.65447034e-01,  1.31186839e+02,\n",
      "       -9.48209333e-01, -4.27802149e-01, -1.20186030e+00, -1.43749475e+00,\n",
      "       -3.12027737e-01, -5.92878990e-01, -6.97199454e-01, -7.04109482e-01,\n",
      "       -9.59256852e-01, -2.28618546e-01, -6.14241458e-01, -5.35217026e-01,\n",
      "       -2.30799202e+00, -2.72732450e+00, -5.10684899e+00,  5.34151269e+01])\n",
      "  message: 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 2813\n",
      "      nit: 3\n",
      "     njev: 29\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([-2.95760451e-02, -2.88286531e-02, -3.79776490e-02, -2.43298022e-02,\n",
      "       -1.58312342e-02, -3.21400225e-02, -4.96116848e-02, -5.43256780e-02,\n",
      "       -6.92820091e-03, -5.72989697e-03, -7.29987644e-03, -1.47998138e-02,\n",
      "       -2.28621966e-02, -3.76551978e-06, -1.14201368e-02,  5.96736660e-03,\n",
      "       -3.62839299e-02, -5.41823320e-02, -3.61160406e-02, -9.55040622e-03,\n",
      "       -2.62135300e-02, -1.30780385e-02, -5.87782737e-03,  9.15232651e-03,\n",
      "       -3.59193289e-02, -3.19510165e-02,  2.07016597e-02, -6.25665655e-02,\n",
      "        4.13128928e-03, -5.60509943e-02, -3.92955924e-02, -3.01075158e-02,\n",
      "       -3.60438091e-02, -4.85087162e-02, -1.13696654e-02, -4.04656550e-02,\n",
      "       -6.11401553e-02,  4.24815138e-02, -8.08967181e-03, -8.41544739e-03,\n",
      "       -2.80611071e-02,  4.32372691e-03,  1.42995908e-02, -4.57700016e-02,\n",
      "       -5.23131969e-02, -4.33689419e-02, -1.58294681e-03, -2.30068371e-02,\n",
      "       -2.03403832e-02, -4.52061941e-02,  1.47345382e-02, -1.07125159e-03,\n",
      "        5.94603258e-03, -7.90581353e-03, -5.38802826e-03, -1.62879831e-02,\n",
      "       -3.78001408e-02, -4.73353995e-02, -1.62190536e-02, -3.24866203e-04,\n",
      "       -1.61129925e-02, -1.00959612e-02, -4.77689641e-02, -1.45643674e-02,\n",
      "       -5.68107688e-02,  1.31740240e-02, -2.56879978e-02, -1.14088824e-02,\n",
      "       -9.23727673e-03, -2.23817599e-02, -6.36741607e-02, -3.34795983e-03,\n",
      "       -2.03581277e-02, -3.06905379e-02, -4.89448158e-02, -3.44722062e-02,\n",
      "       -1.10298196e-02, -5.05572838e-02,  6.17570714e-03, -2.92202536e-02,\n",
      "       -1.52378822e-02,  1.34165380e-02, -6.03736647e-02, -1.56866255e-02,\n",
      "       -4.54286702e-02, -3.08746814e-02, -8.05168962e-03, -4.49230878e-02,\n",
      "       -2.92657156e-02, -1.35915587e-02, -3.06606843e-02, -7.60581676e-06,\n",
      "       -4.61857657e-02, -4.29105605e-02, -2.65189780e-02,  1.94045658e-02])\n",
      "Meta-VQE absolute energy errors:  [17.11216854121776, 20.460700390035186, 23.806721078402546, 12.012098380056976, 13.26089562695781, 20.350718940725763, 12.009266330251211, 19.876671295412024, 14.38616975227163, 25.701319198166303, 25.470201288247058, 24.07899410499439, 27.613610358696057, 25.4658058700467, 21.062103571077945, 29.495910139012906, 12.002271762051798, 25.74822218818246, 18.245281055020786, 25.188457091142983, 12.001794765710915, 23.527003008366822, 11.999984833370426, 25.24568131103669, 29.211326828058933, 16.597829088702536, 15.523472832812216, 11.999260170091738, 18.21450161740065, 28.71179321698872, 12.003065977353996, 28.772745745925597, 11.999521132994538, 19.334625992778328, 21.736084175474353, 12.003121385667557, 12.001389430170159, 12.000022960039537, 12.399250295092845, 11.998579672293953, 12.402546885872074, 14.759749176045105, 13.332473251530505, 22.421425041453404, 20.3200284871321, 17.57252022414, 12.003123643830454, 26.288481906326616, 13.385921385594878, 18.235631635475627, 27.628821950256455, 18.36872552500863, 12.007817813371378, 25.201842241527466, 22.012219564847253, 21.32946587102635, 26.849003885856657, 12.010179699202672, 12.748656989842267, 17.67842677781416, 23.235642315342183, 15.17284959411105, 12.003301510011898, 24.44606319263637, 17.76741078895092, 14.736145752752842, 24.140697035858103, 29.21579234308006, 29.566048419919802, 24.115928183986433, 22.822827980055916, 12.000749159969999, 15.52225124895552, 16.874312242683956, 27.643728266348283, 27.172090509221725, 29.047072981582097, 20.205652003520274, 24.117435812460926, 23.271736582245357, 15.92553122427159, 23.662232497550313, 12.003407996473392, 22.48934154254077, 20.878298774659974, 11.997915726368767, 12.010090358798777, 27.86108782646742, 25.476991495023128, 13.226747888041267, 24.7430804735759, 25.670785385678194, 24.015994217742335, 20.511094874453917, 12.821687398510669, 12.028527991181335, 12.008873593311392, 27.69504854332233, 13.729652053709795, 26.781471824387786]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 5.95975633e+00,  8.48638438e+00,  1.09222120e+01, -2.76768919e+00,\n",
       "         2.71404077e+00,  8.40460942e+00, -2.19575832e+00,  8.05126325e+00,\n",
       "         3.83541801e+00,  1.21306542e+01,  1.19844853e+01,  1.10973213e+01,\n",
       "         1.33270742e+01,  1.19817039e+01,  8.93225996e+00,  1.44826683e+01,\n",
       "        -4.45125480e-01,  1.21602716e+01,  6.82379002e+00,  1.18058331e+01,\n",
       "        -3.07230453e-01,  1.07385088e+01,  2.32441097e-01,  1.18421607e+01,\n",
       "         1.43093290e+01,  5.56413760e+00,  4.73026747e+00,  4.54056022e-01,\n",
       "         6.80044966e+00,  1.40038845e+01, -6.69848808e-01,  1.40412372e+01,\n",
       "         3.74036362e-01,  7.64544743e+00,  9.42941935e+00, -6.85270910e-01,\n",
       "        -1.88554007e-01,  2.20831752e-01,  1.79093385e+00,  6.63896040e-01,\n",
       "         1.79448640e+00,  4.13082200e+00,  2.79027279e+00,  9.93236153e+00,\n",
       "         8.38177644e+00,  6.31200027e+00, -6.85952960e-01,  1.25004604e+01,\n",
       "         2.84715528e+00,  6.81647347e+00,  1.33365009e+01,  6.91732633e+00,\n",
       "        -1.87509988e+00,  1.18143324e+01,  9.63236990e+00,  9.12978933e+00,\n",
       "         1.28514555e+01, -2.38790750e+00,  2.16653696e+00,  6.39279803e+00,\n",
       "         1.05266586e+01,  4.45578747e+00, -7.35319352e-01,  1.13326306e+01,\n",
       "         6.46061737e+00,  4.11220238e+00,  1.11369400e+01,  1.43120526e+01,\n",
       "         1.45253053e+01,  1.11210392e+01,  1.02257733e+01,  1.67490353e-03,\n",
       "         4.72931321e+00,  5.77707889e+00,  1.33457359e+01,  1.30528749e+01,\n",
       "         1.42090624e+01,  8.29663018e+00,  1.11220088e+01,  1.05529268e+01,\n",
       "         5.04356243e+00,  1.08290806e+01, -7.64628780e-01,  9.98206510e+00,\n",
       "         8.79621962e+00,  8.69210450e-01, -2.36925501e+00,  1.34802441e+01,\n",
       "         1.19887871e+01,  2.67764995e+00,  1.15223798e+01,  1.21113582e+01,\n",
       "         1.10568486e+01,  8.52383282e+00,  2.24481480e+00,  1.05975645e+00,\n",
       "        -2.11097462e+00,  1.33775201e+01,  3.21216055e+00,  1.28092710e+01]),\n",
       " array([17.11216854, 20.46070039, 23.80672108, 12.01209838, 13.26089563,\n",
       "        20.35071894, 12.00926633, 19.8766713 , 14.38616975, 25.7013192 ,\n",
       "        25.47020129, 24.0789941 , 27.61361036, 25.46580587, 21.06210357,\n",
       "        29.49591014, 12.00227176, 25.74822219, 18.24528106, 25.18845709,\n",
       "        12.00179477, 23.52700301, 11.99998483, 25.24568131, 29.21132683,\n",
       "        16.59782909, 15.52347283, 11.99926017, 18.21450162, 28.71179322,\n",
       "        12.00306598, 28.77274575, 11.99952113, 19.33462599, 21.73608418,\n",
       "        12.00312139, 12.00138943, 12.00002296, 12.3992503 , 11.99857967,\n",
       "        12.40254689, 14.75974918, 13.33247325, 22.42142504, 20.32002849,\n",
       "        17.57252022, 12.00312364, 26.28848191, 13.38592139, 18.23563164,\n",
       "        27.62882195, 18.36872553, 12.00781781, 25.20184224, 22.01221956,\n",
       "        21.32946587, 26.84900389, 12.0101797 , 12.74865699, 17.67842678,\n",
       "        23.23564232, 15.17284959, 12.00330151, 24.44606319, 17.76741079,\n",
       "        14.73614575, 24.14069704, 29.21579234, 29.56604842, 24.11592818,\n",
       "        22.82282798, 12.00074916, 15.52225125, 16.87431224, 27.64372827,\n",
       "        27.17209051, 29.04707298, 20.205652  , 24.11743581, 23.27173658,\n",
       "        15.92553122, 23.6622325 , 12.003408  , 22.48934154, 20.87829877,\n",
       "        11.99791573, 12.01009036, 27.86108783, 25.4769915 , 13.22674789,\n",
       "        24.74308047, 25.67078539, 24.01599422, 20.51109487, 12.8216874 ,\n",
       "        12.02852799, 12.00887359, 27.69504854, 13.72965205, 26.78147182]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qubits = cirq.LineQubit.range(8)\n",
    "\n",
    "Nfeval = 1  \n",
    "\n",
    "train_set_size = 10\n",
    "min_hamilt_params = {\"lambda\": 0.75, \"Delta\": -1.1}\n",
    "max_hamilt_params = {\"lambda\": 0.75, \"Delta\": 1.1}\n",
    "\n",
    "train_set = construct_training_set(train_set_size=train_set_size, mode=\"equidistant\")\n",
    "test_set = construct_test_set(test_set_size=100, mode=\"uniform_random\", min_hamilt_params=min_hamilt_params, max_hamilt_params=max_hamilt_params)\n",
    "\n",
    "meta_vqe(qubits, train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer sizes:  [32, 32, 16, 16]\n",
      "Train loss should converge to  -125.38941301973591\n",
      "Starting training...\n",
      "Running repetition:  0\n",
      "Unfrozen params:  16  Frozen params:  80\n",
      "Iter:  1  Loss on train set:  59.59967672618967\n",
      "Iter:  2  Loss on train set:  59.394431836798006\n",
      "Iter:  3  Loss on train set:  59.11632385704006\n",
      "Iter:  4  Loss on train set:  58.83675465883579\n",
      "Iter:  5  Loss on train set:  58.40185787056015\n",
      "Iter:  6  Loss on train set:  57.81619904253222\n",
      "Iter:  7  Loss on train set:  57.14359060399025\n",
      "Iter:  8  Loss on train set:  56.255244123437244\n",
      "Iter:  9  Loss on train set:  55.30922542503967\n",
      "Iter:  10  Loss on train set:  54.05975351500556\n",
      "Iter:  11  Loss on train set:  52.777767512180716\n",
      "Iter:  12  Loss on train set:  51.44109069706958\n",
      "Iter:  13  Loss on train set:  50.11264276976726\n",
      "Iter:  14  Loss on train set:  48.835835568424656\n",
      "Iter:  15  Loss on train set:  47.60791189628557\n",
      "Iter:  16  Loss on train set:  46.3996651584349\n",
      "Iter:  17  Loss on train set:  45.22996555627721\n",
      "Iter:  18  Loss on train set:  44.12130585561617\n",
      "Iter:  19  Loss on train set:  42.97931173442642\n",
      "Iter:  20  Loss on train set:  41.76804779527877\n",
      "Iter:  21  Loss on train set:  40.453574706243444\n",
      "Iter:  22  Loss on train set:  39.149990461381165\n",
      "Iter:  23  Loss on train set:  37.915258417317624\n",
      "Iter:  24  Loss on train set:  36.52659126509721\n",
      "Iter:  25  Loss on train set:  35.00493293117778\n",
      "Iter:  26  Loss on train set:  33.42404718354412\n",
      "Iter:  27  Loss on train set:  31.821636038844467\n",
      "Iter:  28  Loss on train set:  30.200094776725553\n",
      "Iter:  29  Loss on train set:  28.650033646242477\n",
      "Iter:  30  Loss on train set:  27.18682337993955\n",
      "Iter:  31  Loss on train set:  25.73777298096341\n",
      "Iter:  32  Loss on train set:  24.35346785764478\n",
      "Iter:  33  Loss on train set:  23.057672344687234\n",
      "Iter:  34  Loss on train set:  21.68395106253289\n",
      "Iter:  35  Loss on train set:  20.183392779269376\n",
      "Iter:  36  Loss on train set:  18.757106149896735\n",
      "Iter:  37  Loss on train set:  17.267871316754743\n",
      "Iter:  38  Loss on train set:  15.388543403705444\n",
      "Iter:  39  Loss on train set:  13.493656811430846\n",
      "Iter:  40  Loss on train set:  11.40426008274383\n",
      "Iter:  41  Loss on train set:  7.981251370324994\n",
      "Iter:  42  Loss on train set:  4.7526455262804514\n",
      "Iter:  43  Loss on train set:  1.5380399762901757\n",
      "Iter:  44  Loss on train set:  -2.109162182966525\n",
      "Iter:  45  Loss on train set:  -5.5335303390415636\n",
      "Iter:  46  Loss on train set:  -8.807409918838287\n",
      "Iter:  47  Loss on train set:  -11.993783824607375\n",
      "Iter:  48  Loss on train set:  -15.35998656943858\n",
      "Iter:  49  Loss on train set:  -18.420380340889995\n",
      "Iter:  50  Loss on train set:  -21.5129165462248\n",
      "Iter:  51  Loss on train set:  -24.298350002906698\n",
      "Iter:  52  Loss on train set:  -26.901442310682917\n",
      "Iter:  53  Loss on train set:  -29.275930201217317\n",
      "Iter:  54  Loss on train set:  -31.39070694768027\n",
      "Iter:  55  Loss on train set:  -33.883129266713034\n",
      "Iter:  56  Loss on train set:  -36.30766359930561\n",
      "Iter:  57  Loss on train set:  -39.503784117090824\n",
      "Iter:  58  Loss on train set:  -42.75758795078487\n",
      "Iter:  59  Loss on train set:  -46.17656699877344\n",
      "Iter:  60  Loss on train set:  -49.35483355175095\n",
      "Iter:  61  Loss on train set:  -52.18060291724154\n",
      "Iter:  62  Loss on train set:  -54.86972396078631\n",
      "Iter:  63  Loss on train set:  -57.30419376088172\n",
      "Iter:  64  Loss on train set:  -59.46647940253671\n",
      "Iter:  65  Loss on train set:  -61.52111593248387\n",
      "Iter:  66  Loss on train set:  -63.41808517239501\n",
      "Iter:  67  Loss on train set:  -65.07747972603534\n",
      "Iter:  68  Loss on train set:  -66.55445903207269\n",
      "Iter:  69  Loss on train set:  -67.85239918405382\n",
      "Iter:  70  Loss on train set:  -69.14387058970243\n",
      "Iter:  71  Loss on train set:  -70.88702146951475\n",
      "Iter:  72  Loss on train set:  -72.48872517054416\n",
      "Iter:  73  Loss on train set:  -73.87595331266063\n",
      "Iter:  74  Loss on train set:  -75.21798695688258\n",
      "Iter:  75  Loss on train set:  -76.35301497830665\n",
      "Iter:  76  Loss on train set:  -77.71058112110933\n",
      "Iter:  77  Loss on train set:  -79.00951858965566\n",
      "Iter:  78  Loss on train set:  -80.17912462211298\n",
      "Iter:  79  Loss on train set:  -81.13911691679928\n",
      "Iter:  80  Loss on train set:  -81.94287857425834\n",
      "Iter:  81  Loss on train set:  -82.59067388319139\n",
      "Iter:  82  Loss on train set:  -83.11323196910102\n",
      "Iter:  83  Loss on train set:  -83.56002352822455\n",
      "Iter:  84  Loss on train set:  -83.91235988087368\n",
      "Iter:  85  Loss on train set:  -84.20272926395248\n",
      "Iter:  86  Loss on train set:  -84.42877422118129\n",
      "Iter:  87  Loss on train set:  -84.62051996755228\n",
      "Iter:  88  Loss on train set:  -84.80361869766578\n",
      "Iter:  89  Loss on train set:  -84.98066092781005\n",
      "Iter:  90  Loss on train set:  -85.1699905522986\n",
      "Iter:  91  Loss on train set:  -85.3361671870787\n",
      "Iter:  92  Loss on train set:  -85.49657115900027\n",
      "Iter:  93  Loss on train set:  -85.63072530744462\n",
      "Iter:  94  Loss on train set:  -85.76924749881611\n",
      "Iter:  95  Loss on train set:  -85.88801086615625\n",
      "Iter:  96  Loss on train set:  -85.99363383397582\n",
      "Iter:  97  Loss on train set:  -86.08383789293413\n",
      "Iter:  98  Loss on train set:  -86.18001698079829\n",
      "Iter:  99  Loss on train set:  -86.27222181718899\n",
      "Iter:  100  Loss on train set:  -86.36732308782939\n",
      "Iter:  101  Loss on train set:  -86.48083568421697\n",
      "Iter:  102  Loss on train set:  -86.58411777232503\n",
      "Iter:  103  Loss on train set:  -86.72477879310743\n",
      "Iter:  104  Loss on train set:  -86.84016168997952\n",
      "Iter:  105  Loss on train set:  -87.01068830591113\n",
      "Iter:  106  Loss on train set:  -87.24597855681566\n",
      "Iter:  107  Loss on train set:  -87.4642215048732\n",
      "Iter:  108  Loss on train set:  -87.65631092907152\n",
      "Iter:  109  Loss on train set:  -87.8416388342415\n",
      "Iter:  110  Loss on train set:  -88.01526114491303\n",
      "Iter:  111  Loss on train set:  -88.16539404216914\n",
      "Iter:  112  Loss on train set:  -88.31866519625751\n",
      "Iter:  113  Loss on train set:  -88.46830766109026\n",
      "Iter:  114  Loss on train set:  -88.59851298536967\n",
      "Iter:  115  Loss on train set:  -88.73822763775911\n",
      "Iter:  116  Loss on train set:  -88.85361413770632\n",
      "Iter:  117  Loss on train set:  -88.94986266158462\n",
      "Iter:  118  Loss on train set:  -89.03117893056026\n",
      "Iter:  119  Loss on train set:  -89.09789726299019\n",
      "Iter:  120  Loss on train set:  -89.15357697773554\n",
      "Iter:  121  Loss on train set:  -89.20000680584641\n",
      "Iter:  122  Loss on train set:  -89.2934206473859\n",
      "Iter:  123  Loss on train set:  -89.37187187009908\n",
      "Iter:  124  Loss on train set:  -89.48659145504504\n",
      "Iter:  125  Loss on train set:  -89.58583796250906\n",
      "Iter:  126  Loss on train set:  -89.67012134130037\n",
      "Iter:  127  Loss on train set:  -89.76699530962617\n",
      "Iter:  128  Loss on train set:  -89.85817989389825\n",
      "Iter:  129  Loss on train set:  -89.96808393762232\n",
      "Iter:  130  Loss on train set:  -90.05997397723944\n",
      "Iter:  131  Loss on train set:  -90.13672975059109\n",
      "Iter:  132  Loss on train set:  -90.21347279026685\n",
      "Iter:  133  Loss on train set:  -90.27853627776508\n",
      "Iter:  134  Loss on train set:  -90.35679014157901\n",
      "Iter:  135  Loss on train set:  -90.44757462937095\n",
      "Iter:  136  Loss on train set:  -90.53688285885652\n",
      "Iter:  137  Loss on train set:  -90.60830047597744\n",
      "Iter:  138  Loss on train set:  -90.66842629396317\n",
      "Iter:  139  Loss on train set:  -90.72561679211921\n",
      "Iter:  140  Loss on train set:  -90.78591144139617\n",
      "Iter:  141  Loss on train set:  -90.833605598523\n",
      "Iter:  142  Loss on train set:  -90.89930506896663\n",
      "Iter:  143  Loss on train set:  -90.95215016215586\n",
      "Iter:  144  Loss on train set:  -90.99711645072445\n",
      "Iter:  145  Loss on train set:  -91.0456084695274\n",
      "Iter:  146  Loss on train set:  -91.0887554730985\n",
      "Iter:  147  Loss on train set:  -91.12599436978944\n",
      "Iter:  148  Loss on train set:  -91.15690874683004\n",
      "Iter:  149  Loss on train set:  -91.1831086634336\n",
      "Iter:  150  Loss on train set:  -91.20544296223127\n",
      "Iter:  151  Loss on train set:  -91.22563281726397\n",
      "Iter:  152  Loss on train set:  -91.24634450735715\n",
      "Iter:  153  Loss on train set:  -91.26895180419768\n",
      "Iter:  154  Loss on train set:  -91.28823889090617\n",
      "Iter:  155  Loss on train set:  -91.30466368239667\n",
      "Iter:  156  Loss on train set:  -91.31882416609935\n",
      "Iter:  157  Loss on train set:  -91.33436742391815\n",
      "Iter:  158  Loss on train set:  -91.35201623568557\n",
      "Iter:  159  Loss on train set:  -91.36703609767054\n",
      "Iter:  160  Loss on train set:  -91.37983009956102\n",
      "Iter:  161  Loss on train set:  -91.39277889578386\n",
      "Iter:  162  Loss on train set:  -91.40866029200124\n",
      "Iter:  163  Loss on train set:  -91.42254636905321\n",
      "Iter:  164  Loss on train set:  -91.44032887459498\n",
      "Iter:  165  Loss on train set:  -91.45814031844401\n",
      "Iter:  166  Loss on train set:  -91.47833568944063\n",
      "Iter:  167  Loss on train set:  -91.49623461699356\n",
      "Iter:  168  Loss on train set:  -91.5120708779935\n",
      "Iter:  169  Loss on train set:  -91.5281786994146\n",
      "Iter:  170  Loss on train set:  -91.55516149373818\n",
      "Iter:  171  Loss on train set:  -91.5821096538457\n",
      "Iter:  172  Loss on train set:  -91.61795753513682\n",
      "Iter:  173  Loss on train set:  -91.64719600303549\n",
      "Iter:  174  Loss on train set:  -91.67244804807596\n",
      "Iter:  175  Loss on train set:  -91.69346579532484\n",
      "Iter:  176  Loss on train set:  -91.71059093852219\n",
      "Iter:  177  Loss on train set:  -91.72446846006952\n",
      "Iter:  178  Loss on train set:  -91.73570781578749\n",
      "Iter:  179  Loss on train set:  -91.74567710402549\n",
      "Iter:  180  Loss on train set:  -91.75383363128198\n",
      "Iter:  181  Loss on train set:  -91.76079109214739\n",
      "Iter:  182  Loss on train set:  -91.7717651030892\n",
      "Iter:  183  Loss on train set:  -91.78155685487081\n",
      "Iter:  184  Loss on train set:  -91.79035549495134\n",
      "Iter:  185  Loss on train set:  -91.7980184594806\n",
      "Iter:  186  Loss on train set:  -91.80477901797278\n",
      "Iter:  187  Loss on train set:  -91.81275554453207\n",
      "Iter:  188  Loss on train set:  -91.81982928082009\n",
      "Iter:  189  Loss on train set:  -91.82743626562326\n",
      "Iter:  190  Loss on train set:  -91.83475927644116\n",
      "Iter:  191  Loss on train set:  -91.84311853270839\n",
      "Iter:  192  Loss on train set:  -91.85050137516717\n",
      "Iter:  193  Loss on train set:  -91.85725237481857\n",
      "Iter:  194  Loss on train set:  -91.86517269388652\n",
      "Iter:  195  Loss on train set:  -91.87247202446622\n",
      "Iter:  196  Loss on train set:  -91.8788805594692\n",
      "Iter:  197  Loss on train set:  -91.8849085976899\n",
      "Iter:  198  Loss on train set:  -91.8911775392859\n",
      "Iter:  199  Loss on train set:  -91.90553855372063\n",
      "Iter:  200  Loss on train set:  -91.91940994002422\n",
      "     fun: -91.93182540153066\n",
      "     jac: array([-1.59833649, -1.59833649, -1.59833649,  1.59833649,  1.59833649,\n",
      "        1.59833649, -1.59833649, -1.59833649, -1.59833649, -1.59833649,\n",
      "       -1.59833649,  1.59833649, -1.59833649,  1.59833649, -1.59833649,\n",
      "        1.59833649])\n",
      "    nfev: 200\n",
      "     nit: 200\n",
      " success: True\n",
      "       x: array([-0.06161088, -1.97575007, -1.13128167, -0.71980367,  0.63733827,\n",
      "       -1.42956371, -1.21004292, -0.66622007,  0.21114244, -1.74253137,\n",
      "       -0.71167509, -1.19923215, -0.83714184, -1.05879769,  1.29020041,\n",
      "        0.67949049])\n",
      "Unfrozen params:  16  Frozen params:  80\n",
      "Iter:  201  Loss on train set:  -91.93182540153066\n",
      "Iter:  202  Loss on train set:  -91.96526815680963\n",
      "Iter:  203  Loss on train set:  -91.91545028422001\n",
      "Iter:  204  Loss on train set:  -91.88629938526202\n",
      "Iter:  205  Loss on train set:  -91.88824092798745\n",
      "Iter:  206  Loss on train set:  -91.8819151346921\n",
      "Iter:  207  Loss on train set:  -91.86955891343291\n",
      "Iter:  208  Loss on train set:  -91.86969519235973\n",
      "Iter:  209  Loss on train set:  -91.8644171788495\n",
      "Iter:  210  Loss on train set:  -91.8624925245274\n",
      "Iter:  211  Loss on train set:  -91.856925145007\n",
      "Iter:  212  Loss on train set:  -91.85066119669956\n",
      "Iter:  213  Loss on train set:  -91.84245476509886\n",
      "Iter:  214  Loss on train set:  -91.83277177966224\n",
      "Iter:  215  Loss on train set:  -91.82369485421114\n",
      "Iter:  216  Loss on train set:  -91.81427271517028\n",
      "Iter:  217  Loss on train set:  -91.81080618571802\n",
      "Iter:  218  Loss on train set:  -91.81712480237823\n",
      "Iter:  219  Loss on train set:  -91.82746034986967\n",
      "Iter:  220  Loss on train set:  -91.8348988005952\n",
      "Iter:  221  Loss on train set:  -91.83896681036535\n",
      "Iter:  222  Loss on train set:  -91.83992836751075\n",
      "Iter:  223  Loss on train set:  -91.8385772202893\n",
      "Iter:  224  Loss on train set:  -91.83389073531735\n",
      "Iter:  225  Loss on train set:  -91.83881521301163\n",
      "Iter:  226  Loss on train set:  -91.83916284255152\n",
      "Iter:  227  Loss on train set:  -91.85857562864311\n",
      "Iter:  228  Loss on train set:  -91.87029359359649\n",
      "Iter:  229  Loss on train set:  -91.87320160549882\n",
      "Iter:  230  Loss on train set:  -91.88024183279896\n",
      "Iter:  231  Loss on train set:  -91.89168904560142\n",
      "Iter:  232  Loss on train set:  -91.89952381051876\n",
      "Iter:  233  Loss on train set:  -91.91097939971945\n",
      "Iter:  234  Loss on train set:  -91.9186792987436\n",
      "Iter:  235  Loss on train set:  -91.92462468679474\n",
      "Iter:  236  Loss on train set:  -91.92874587228962\n",
      "Iter:  237  Loss on train set:  -91.93190118447475\n",
      "Iter:  238  Loss on train set:  -91.93429970695666\n",
      "Iter:  239  Loss on train set:  -91.93890228553923\n",
      "Iter:  240  Loss on train set:  -91.94468228291697\n",
      "Iter:  241  Loss on train set:  -91.94842411089864\n",
      "Iter:  242  Loss on train set:  -91.95162507084035\n",
      "Iter:  243  Loss on train set:  -91.96001698544457\n",
      "Iter:  244  Loss on train set:  -91.96254759984255\n",
      "Iter:  245  Loss on train set:  -91.95992117994267\n",
      "Iter:  246  Loss on train set:  -91.9530769087638\n",
      "Iter:  247  Loss on train set:  -91.95172452717902\n",
      "Iter:  248  Loss on train set:  -91.9577040511244\n",
      "Iter:  249  Loss on train set:  -91.96438452934329\n",
      "Iter:  250  Loss on train set:  -91.96799639499972\n",
      "Iter:  251  Loss on train set:  -91.9709618753806\n",
      "Iter:  252  Loss on train set:  -91.96978846400187\n",
      "Iter:  253  Loss on train set:  -91.96547032541183\n",
      "Iter:  254  Loss on train set:  -91.96504437416299\n",
      "Iter:  255  Loss on train set:  -91.96148719653965\n",
      "Iter:  256  Loss on train set:  -91.95699586975168\n",
      "Iter:  257  Loss on train set:  -91.95122403250527\n",
      "Iter:  258  Loss on train set:  -91.94446756840802\n",
      "Iter:  259  Loss on train set:  -91.94282701566036\n",
      "Iter:  260  Loss on train set:  -91.93798740433205\n",
      "Iter:  261  Loss on train set:  -91.93096667023359\n",
      "Iter:  262  Loss on train set:  -91.93177019537053\n",
      "Iter:  263  Loss on train set:  -91.93112533573324\n",
      "Iter:  264  Loss on train set:  -91.9255916674079\n",
      "Iter:  265  Loss on train set:  -91.91541972157961\n",
      "Iter:  266  Loss on train set:  -91.92410658838868\n",
      "Iter:  267  Loss on train set:  -91.9451173083298\n",
      "Iter:  268  Loss on train set:  -91.96046660456203\n",
      "Iter:  269  Loss on train set:  -91.96911688727172\n",
      "Iter:  270  Loss on train set:  -91.9653692755835\n",
      "Iter:  271  Loss on train set:  -91.95278717382388\n",
      "Iter:  272  Loss on train set:  -91.93008696895333\n",
      "Iter:  273  Loss on train set:  -91.9098225854564\n",
      "Iter:  274  Loss on train set:  -91.89378801472901\n",
      "Iter:  275  Loss on train set:  -91.88808351816222\n",
      "Iter:  276  Loss on train set:  -91.88747112271173\n",
      "Iter:  277  Loss on train set:  -91.88383243611658\n",
      "Iter:  278  Loss on train set:  -91.88693665178738\n",
      "Iter:  279  Loss on train set:  -91.89737392132555\n",
      "Iter:  280  Loss on train set:  -91.91103248716288\n",
      "Iter:  281  Loss on train set:  -91.91325281207641\n",
      "Iter:  282  Loss on train set:  -91.90710483275683\n",
      "Iter:  283  Loss on train set:  -91.8954145555828\n",
      "Iter:  284  Loss on train set:  -91.87930794933567\n",
      "Iter:  285  Loss on train set:  -91.86313021762703\n",
      "Iter:  286  Loss on train set:  -91.87430011842828\n",
      "Iter:  287  Loss on train set:  -91.87838408719261\n",
      "Iter:  288  Loss on train set:  -91.8732821967\n",
      "Iter:  289  Loss on train set:  -91.87716670977\n",
      "Iter:  290  Loss on train set:  -91.87467988970964\n",
      "Iter:  291  Loss on train set:  -91.86678165556584\n",
      "Iter:  292  Loss on train set:  -91.85497488337808\n",
      "Iter:  293  Loss on train set:  -91.8407972068104\n",
      "Iter:  294  Loss on train set:  -91.82536496027578\n",
      "Iter:  295  Loss on train set:  -91.80704930768513\n",
      "Iter:  296  Loss on train set:  -91.7883759143377\n",
      "Iter:  297  Loss on train set:  -91.80073296148399\n",
      "Iter:  298  Loss on train set:  -91.80745626265225\n",
      "Iter:  299  Loss on train set:  -91.8158514903848\n",
      "Iter:  300  Loss on train set:  -91.82010321870601\n",
      "Iter:  301  Loss on train set:  -91.8240048629406\n",
      "Iter:  302  Loss on train set:  -91.82510942417271\n",
      "Iter:  303  Loss on train set:  -91.82507036687656\n",
      "Iter:  304  Loss on train set:  -91.82634639022584\n",
      "Iter:  305  Loss on train set:  -91.8197418540889\n",
      "Iter:  306  Loss on train set:  -91.83291267237945\n",
      "Iter:  307  Loss on train set:  -91.85057716093723\n",
      "Iter:  308  Loss on train set:  -91.867557598134\n",
      "Iter:  309  Loss on train set:  -91.87775747204384\n",
      "Iter:  310  Loss on train set:  -91.88231757652446\n",
      "Iter:  311  Loss on train set:  -91.88194091789006\n",
      "Iter:  312  Loss on train set:  -91.87809058717669\n",
      "Iter:  313  Loss on train set:  -91.87658036727613\n",
      "Iter:  314  Loss on train set:  -91.88372309218175\n",
      "Iter:  315  Loss on train set:  -91.89389770009524\n",
      "Iter:  316  Loss on train set:  -91.89882820710251\n",
      "Iter:  317  Loss on train set:  -91.9346972551536\n",
      "Iter:  318  Loss on train set:  -91.95401375681409\n",
      "Iter:  319  Loss on train set:  -91.97293901575108\n",
      "Iter:  320  Loss on train set:  -92.00788093091191\n",
      "Iter:  321  Loss on train set:  -92.0177114008076\n",
      "Iter:  322  Loss on train set:  -92.04343492322866\n",
      "Iter:  323  Loss on train set:  -92.0545792202152\n",
      "Iter:  324  Loss on train set:  -92.06099659354432\n",
      "Iter:  325  Loss on train set:  -92.05606513898451\n",
      "Iter:  326  Loss on train set:  -92.04204019436605\n",
      "Iter:  327  Loss on train set:  -92.04038653453756\n",
      "Iter:  328  Loss on train set:  -92.04066295725472\n",
      "Iter:  329  Loss on train set:  -92.03628581470323\n",
      "Iter:  330  Loss on train set:  -92.03416078647483\n",
      "Iter:  331  Loss on train set:  -92.03231668444334\n",
      "Iter:  332  Loss on train set:  -92.02250548552541\n",
      "Iter:  333  Loss on train set:  -92.01413987246596\n",
      "Iter:  334  Loss on train set:  -92.01367194348266\n",
      "Iter:  335  Loss on train set:  -92.02950083434172\n",
      "Iter:  336  Loss on train set:  -92.04824684946895\n",
      "Iter:  337  Loss on train set:  -92.0947931126173\n",
      "Iter:  338  Loss on train set:  -92.12596146202263\n",
      "Iter:  339  Loss on train set:  -92.15636101762948\n",
      "Iter:  340  Loss on train set:  -92.17197642530047\n",
      "Iter:  341  Loss on train set:  -92.17890996213036\n",
      "Iter:  342  Loss on train set:  -92.18245140940084\n",
      "Iter:  343  Loss on train set:  -92.18228019683359\n",
      "Iter:  344  Loss on train set:  -92.18531375063432\n",
      "Iter:  345  Loss on train set:  -92.18537602580577\n",
      "Iter:  346  Loss on train set:  -92.18331275232927\n",
      "Iter:  347  Loss on train set:  -92.19330850826677\n",
      "Iter:  348  Loss on train set:  -92.20893446649342\n",
      "Iter:  349  Loss on train set:  -92.2283079422561\n",
      "Iter:  350  Loss on train set:  -92.2390217148138\n",
      "Iter:  351  Loss on train set:  -92.2429515827101\n",
      "Iter:  352  Loss on train set:  -92.24140581556733\n",
      "Iter:  353  Loss on train set:  -92.23608083322654\n",
      "Iter:  354  Loss on train set:  -92.2349452767819\n",
      "Iter:  355  Loss on train set:  -92.24848279134702\n",
      "Iter:  356  Loss on train set:  -92.25776926710724\n",
      "Iter:  357  Loss on train set:  -92.26390557829826\n",
      "Iter:  358  Loss on train set:  -92.28314467404012\n",
      "Iter:  359  Loss on train set:  -92.29978819554432\n",
      "Iter:  360  Loss on train set:  -92.30993016104796\n",
      "Iter:  361  Loss on train set:  -92.31426977663422\n",
      "Iter:  362  Loss on train set:  -92.31635093294756\n",
      "Iter:  363  Loss on train set:  -92.32017711575806\n",
      "Iter:  364  Loss on train set:  -92.32899988252296\n",
      "Iter:  365  Loss on train set:  -92.33619965922531\n",
      "Iter:  366  Loss on train set:  -92.3423074734581\n",
      "Iter:  367  Loss on train set:  -92.34492179018221\n",
      "Iter:  368  Loss on train set:  -92.34417118834929\n",
      "Iter:  369  Loss on train set:  -92.34150175195657\n",
      "Iter:  370  Loss on train set:  -92.34234494571355\n",
      "Iter:  371  Loss on train set:  -92.35279401818305\n",
      "Iter:  372  Loss on train set:  -92.36236109686504\n",
      "Iter:  373  Loss on train set:  -92.3722396063027\n",
      "Iter:  374  Loss on train set:  -92.38226831757906\n",
      "Iter:  375  Loss on train set:  -92.38925852593198\n",
      "Iter:  376  Loss on train set:  -92.39132471017038\n",
      "Iter:  377  Loss on train set:  -92.39288133545162\n",
      "Iter:  378  Loss on train set:  -92.40249269649632\n",
      "Iter:  379  Loss on train set:  -92.40808424066728\n",
      "Iter:  380  Loss on train set:  -92.41164538214969\n",
      "Iter:  381  Loss on train set:  -92.41337194328325\n",
      "Iter:  382  Loss on train set:  -92.4225343209811\n",
      "Iter:  383  Loss on train set:  -92.42640563633668\n",
      "Iter:  384  Loss on train set:  -92.42650202275017\n",
      "Iter:  385  Loss on train set:  -92.43086597205945\n",
      "Iter:  386  Loss on train set:  -92.43756070930107\n",
      "Iter:  387  Loss on train set:  -92.44396437789803\n",
      "Iter:  388  Loss on train set:  -92.44583172487052\n",
      "Iter:  389  Loss on train set:  -92.44522551622212\n",
      "Iter:  390  Loss on train set:  -92.44077305146168\n",
      "Iter:  391  Loss on train set:  -92.43941787082876\n",
      "Iter:  392  Loss on train set:  -92.44202643765055\n",
      "Iter:  393  Loss on train set:  -92.44452419667945\n",
      "Iter:  394  Loss on train set:  -92.44673829660039\n",
      "Iter:  395  Loss on train set:  -92.45138861855297\n",
      "Iter:  396  Loss on train set:  -92.45569784991073\n",
      "Iter:  397  Loss on train set:  -92.46840962220476\n",
      "Iter:  398  Loss on train set:  -92.479915328207\n",
      "Iter:  399  Loss on train set:  -92.48739744151257\n",
      "Iter:  400  Loss on train set:  -92.49121937800719\n",
      "     fun: -92.49247621866651\n",
      "     jac: array([ 0.43390096,  0.43390096, -0.43390096,  0.43390096, -0.43390096,\n",
      "        0.43390096,  0.43390096,  0.43390096,  0.43390096, -0.43390096,\n",
      "        0.43390096, -0.43390096, -0.43390096, -0.43390096,  0.43390096,\n",
      "       -0.43390096])\n",
      "    nfev: 200\n",
      "     nit: 200\n",
      " success: True\n",
      "       x: array([ 0.22172908, -0.26423671,  0.22741971,  0.01836744, -0.63325731,\n",
      "       -0.06587835, -1.34525691,  0.01598162,  1.53531798,  0.21757107,\n",
      "        1.23124298, -0.01829303, -0.36836495, -0.01150799,  0.28956811,\n",
      "        0.03194558])\n",
      "Unfrozen params:  32  Frozen params:  64\n",
      "Iter:  401  Loss on train set:  -92.49247621866651\n",
      "Iter:  402  Loss on train set:  -92.24783538596478\n",
      "Iter:  403  Loss on train set:  -91.80805373955559\n",
      "Iter:  404  Loss on train set:  -91.06186395549862\n",
      "Iter:  405  Loss on train set:  -90.26471393197464\n",
      "Iter:  406  Loss on train set:  -89.71007806005865\n",
      "Iter:  407  Loss on train set:  -89.94610304471416\n",
      "Iter:  408  Loss on train set:  -90.11896134292573\n",
      "Iter:  409  Loss on train set:  -90.26455297983526\n",
      "Iter:  410  Loss on train set:  -90.58019506351532\n",
      "Iter:  411  Loss on train set:  -90.74387210509417\n",
      "Iter:  412  Loss on train set:  -90.81135769646009\n",
      "Iter:  413  Loss on train set:  -90.77758429012859\n",
      "Iter:  414  Loss on train set:  -90.66909028550491\n",
      "Iter:  415  Loss on train set:  -90.54193907476446\n",
      "Iter:  416  Loss on train set:  -90.34527089461385\n",
      "Iter:  417  Loss on train set:  -90.10418410290593\n",
      "Iter:  418  Loss on train set:  -89.89698698567013\n",
      "Iter:  419  Loss on train set:  -89.67814631018629\n",
      "Iter:  420  Loss on train set:  -89.49172569579109\n",
      "Iter:  421  Loss on train set:  -89.3019270140212\n",
      "Iter:  422  Loss on train set:  -89.11780928641657\n",
      "Iter:  423  Loss on train set:  -88.95389111643111\n",
      "Iter:  424  Loss on train set:  -89.17204951889451\n",
      "Iter:  425  Loss on train set:  -89.44668564359397\n",
      "Iter:  426  Loss on train set:  -89.63353774262198\n",
      "Iter:  427  Loss on train set:  -89.86800732088032\n",
      "Iter:  428  Loss on train set:  -90.00434833618928\n",
      "Iter:  429  Loss on train set:  -90.04542214587477\n",
      "Iter:  430  Loss on train set:  -90.01383048117746\n",
      "Iter:  431  Loss on train set:  -89.92379779416422\n",
      "Iter:  432  Loss on train set:  -89.84192133623912\n",
      "Iter:  433  Loss on train set:  -89.7214308440543\n",
      "Iter:  434  Loss on train set:  -89.71981818677051\n",
      "Iter:  435  Loss on train set:  -89.69859067744008\n",
      "Iter:  436  Loss on train set:  -89.65146850515777\n",
      "Iter:  437  Loss on train set:  -89.59049778750835\n",
      "Iter:  438  Loss on train set:  -89.53068883355138\n",
      "Iter:  439  Loss on train set:  -89.44936358441096\n",
      "Iter:  440  Loss on train set:  -89.43748731804688\n",
      "Iter:  441  Loss on train set:  -89.5731190647329\n",
      "Iter:  442  Loss on train set:  -89.67694415203202\n",
      "Iter:  443  Loss on train set:  -89.76743507406508\n",
      "Iter:  444  Loss on train set:  -89.94135478868748\n",
      "Iter:  445  Loss on train set:  -90.08784518057351\n",
      "Iter:  446  Loss on train set:  -90.21447254090938\n",
      "Iter:  447  Loss on train set:  -90.32406939180115\n",
      "Iter:  448  Loss on train set:  -90.52092470132158\n",
      "Iter:  449  Loss on train set:  -90.71626696643376\n",
      "Iter:  450  Loss on train set:  -90.9576635888766\n",
      "Iter:  451  Loss on train set:  -91.15513902857163\n",
      "Iter:  452  Loss on train set:  -91.372784037373\n",
      "Iter:  453  Loss on train set:  -91.52677698869272\n",
      "Iter:  454  Loss on train set:  -91.65208855946199\n",
      "Iter:  455  Loss on train set:  -91.72655199766788\n",
      "Iter:  456  Loss on train set:  -91.85805327004873\n",
      "Iter:  457  Loss on train set:  -91.9265200314992\n",
      "Iter:  458  Loss on train set:  -91.94920574521497\n",
      "Iter:  459  Loss on train set:  -91.94192607021171\n",
      "Iter:  460  Loss on train set:  -91.94086706958788\n",
      "Iter:  461  Loss on train set:  -91.93636146489136\n",
      "Iter:  462  Loss on train set:  -91.89925131165823\n",
      "Iter:  463  Loss on train set:  -91.83947909520947\n",
      "Iter:  464  Loss on train set:  -91.78199024855962\n",
      "Iter:  465  Loss on train set:  -91.70757954318339\n",
      "Iter:  466  Loss on train set:  -91.64711145738572\n",
      "Iter:  467  Loss on train set:  -91.58708930346623\n",
      "Iter:  468  Loss on train set:  -91.52456222408325\n",
      "Iter:  469  Loss on train set:  -91.68928035481571\n",
      "Iter:  470  Loss on train set:  -91.85322387888921\n",
      "Iter:  471  Loss on train set:  -92.20650704267987\n",
      "Iter:  472  Loss on train set:  -92.46614717976098\n",
      "Iter:  473  Loss on train set:  -92.64260974871905\n",
      "Iter:  474  Loss on train set:  -92.74906644305484\n",
      "Iter:  475  Loss on train set:  -92.88576803243366\n",
      "Iter:  476  Loss on train set:  -92.96528507946215\n",
      "Iter:  477  Loss on train set:  -93.01355906378346\n",
      "Iter:  478  Loss on train set:  -93.0145776312887\n",
      "Iter:  479  Loss on train set:  -92.982257396304\n",
      "Iter:  480  Loss on train set:  -92.92640741390585\n",
      "Iter:  481  Loss on train set:  -92.85415800133738\n",
      "Iter:  482  Loss on train set:  -92.7668415185054\n",
      "Iter:  483  Loss on train set:  -92.67319931872359\n",
      "Iter:  484  Loss on train set:  -92.62522954121481\n",
      "Iter:  485  Loss on train set:  -92.62802994261129\n",
      "Iter:  486  Loss on train set:  -92.6079917344786\n",
      "Iter:  487  Loss on train set:  -92.57668760153918\n",
      "Iter:  488  Loss on train set:  -92.53839997571787\n",
      "Iter:  489  Loss on train set:  -92.52452031767871\n",
      "Iter:  490  Loss on train set:  -92.49663932402822\n",
      "Iter:  491  Loss on train set:  -92.46132776107359\n",
      "Iter:  492  Loss on train set:  -92.43473309650055\n",
      "Iter:  493  Loss on train set:  -92.4579615000526\n",
      "Iter:  494  Loss on train set:  -92.50145243911768\n",
      "Iter:  495  Loss on train set:  -92.53026381638902\n",
      "Iter:  496  Loss on train set:  -92.73043398096097\n",
      "Iter:  497  Loss on train set:  -92.93177445936797\n",
      "Iter:  498  Loss on train set:  -93.06795242240057\n",
      "Iter:  499  Loss on train set:  -93.14947830854477\n",
      "Iter:  500  Loss on train set:  -93.19392248963186\n",
      "Iter:  501  Loss on train set:  -93.21042176037805\n",
      "Iter:  502  Loss on train set:  -93.21123781413165\n",
      "Iter:  503  Loss on train set:  -93.22130545449568\n",
      "Iter:  504  Loss on train set:  -93.2281400639895\n",
      "Iter:  505  Loss on train set:  -93.2860288622121\n",
      "Iter:  506  Loss on train set:  -93.37429318850457\n",
      "Iter:  507  Loss on train set:  -93.40995001969486\n",
      "Iter:  508  Loss on train set:  -93.40380098079567\n",
      "Iter:  509  Loss on train set:  -93.36575558822776\n",
      "Iter:  510  Loss on train set:  -93.35364446296926\n",
      "Iter:  511  Loss on train set:  -93.32913901949173\n",
      "Iter:  512  Loss on train set:  -93.30102418184309\n",
      "Iter:  513  Loss on train set:  -93.26281634847454\n",
      "Iter:  514  Loss on train set:  -93.28966242453842\n",
      "Iter:  515  Loss on train set:  -93.2990231717355\n",
      "Iter:  516  Loss on train set:  -93.32457142928516\n",
      "Iter:  517  Loss on train set:  -93.33291685229536\n",
      "Iter:  518  Loss on train set:  -93.341292236587\n",
      "Iter:  519  Loss on train set:  -93.39763569041274\n",
      "Iter:  520  Loss on train set:  -93.42997169953324\n",
      "Iter:  521  Loss on train set:  -93.44099585899235\n",
      "Iter:  522  Loss on train set:  -93.43280402640066\n",
      "Iter:  523  Loss on train set:  -93.4094491809668\n",
      "Iter:  524  Loss on train set:  -93.37957615004849\n",
      "Iter:  525  Loss on train set:  -93.36203544079521\n",
      "Iter:  526  Loss on train set:  -93.33483006996428\n",
      "Iter:  527  Loss on train set:  -93.34756825371247\n",
      "Iter:  528  Loss on train set:  -93.37419719233004\n",
      "Iter:  529  Loss on train set:  -93.43289442148372\n",
      "Iter:  530  Loss on train set:  -93.54115108885091\n",
      "Iter:  531  Loss on train set:  -93.60673820512697\n",
      "Iter:  532  Loss on train set:  -93.64172220601702\n",
      "Iter:  533  Loss on train set:  -93.65026103498124\n",
      "Iter:  534  Loss on train set:  -93.63944479154617\n",
      "Iter:  535  Loss on train set:  -93.6167552854182\n",
      "Iter:  536  Loss on train set:  -93.58330475529871\n",
      "Iter:  537  Loss on train set:  -93.54340028533332\n",
      "Iter:  538  Loss on train set:  -93.5100277719281\n",
      "Iter:  539  Loss on train set:  -93.49422367311976\n",
      "Iter:  540  Loss on train set:  -93.47535539950721\n",
      "Iter:  541  Loss on train set:  -93.53145703418302\n",
      "Iter:  542  Loss on train set:  -93.58268938260059\n",
      "Iter:  543  Loss on train set:  -93.61895241536475\n",
      "Iter:  544  Loss on train set:  -93.67155555492113\n",
      "Iter:  545  Loss on train set:  -93.70434664796947\n",
      "Iter:  546  Loss on train set:  -93.72856064795438\n",
      "Iter:  547  Loss on train set:  -93.73377858133158\n",
      "Iter:  548  Loss on train set:  -93.7278161234564\n",
      "Iter:  549  Loss on train set:  -93.72192523176285\n",
      "Iter:  550  Loss on train set:  -93.70928475396582\n",
      "Iter:  551  Loss on train set:  -93.70035033358872\n",
      "Iter:  552  Loss on train set:  -93.69451250590146\n",
      "Iter:  553  Loss on train set:  -93.70899997834498\n",
      "Iter:  554  Loss on train set:  -93.7092989305308\n",
      "Iter:  555  Loss on train set:  -93.70122406113052\n",
      "Iter:  556  Loss on train set:  -93.71519050150191\n",
      "Iter:  557  Loss on train set:  -93.72166206298846\n",
      "Iter:  558  Loss on train set:  -93.72464180155525\n",
      "Iter:  559  Loss on train set:  -93.73813357757436\n",
      "Iter:  560  Loss on train set:  -93.76538898558017\n",
      "Iter:  561  Loss on train set:  -93.79543138435363\n",
      "Iter:  562  Loss on train set:  -93.82256222778251\n",
      "Iter:  563  Loss on train set:  -93.8443418998238\n",
      "Iter:  564  Loss on train set:  -93.86100099151221\n",
      "Iter:  565  Loss on train set:  -93.88077213221985\n",
      "Iter:  566  Loss on train set:  -93.93392903885642\n",
      "Iter:  567  Loss on train set:  -93.97881633633108\n",
      "Iter:  568  Loss on train set:  -94.00672009292144\n",
      "Iter:  569  Loss on train set:  -94.0242558630834\n",
      "Iter:  570  Loss on train set:  -94.03419668665975\n",
      "Iter:  571  Loss on train set:  -94.03503440742813\n",
      "Iter:  572  Loss on train set:  -94.0288977375636\n",
      "Iter:  573  Loss on train set:  -94.0190782076068\n",
      "Iter:  574  Loss on train set:  -94.01038599918977\n",
      "Iter:  575  Loss on train set:  -94.00313424777569\n",
      "Iter:  576  Loss on train set:  -93.99444814118017\n",
      "Iter:  577  Loss on train set:  -94.00440397262699\n",
      "Iter:  578  Loss on train set:  -94.01382491320226\n",
      "Iter:  579  Loss on train set:  -94.01929435199902\n",
      "Iter:  580  Loss on train set:  -94.02985429330603\n",
      "Iter:  581  Loss on train set:  -94.03896819169134\n",
      "Iter:  582  Loss on train set:  -94.05864034450502\n",
      "Iter:  583  Loss on train set:  -94.07321263371959\n",
      "Iter:  584  Loss on train set:  -94.08271527330591\n",
      "Iter:  585  Loss on train set:  -94.08967897567683\n",
      "Iter:  586  Loss on train set:  -94.09330264170808\n",
      "Iter:  587  Loss on train set:  -94.09022206137271\n",
      "Iter:  588  Loss on train set:  -94.0841754450709\n",
      "Iter:  589  Loss on train set:  -94.07457194545736\n",
      "Iter:  590  Loss on train set:  -94.06423475355732\n",
      "Iter:  591  Loss on train set:  -94.05291559976472\n",
      "Iter:  592  Loss on train set:  -94.04047514856435\n",
      "Iter:  593  Loss on train set:  -94.02897188505538\n",
      "Iter:  594  Loss on train set:  -94.0172314770536\n",
      "Iter:  595  Loss on train set:  -94.00593467990316\n",
      "Iter:  596  Loss on train set:  -94.0227278099458\n",
      "Iter:  597  Loss on train set:  -94.04330429492313\n",
      "Iter:  598  Loss on train set:  -94.06704592713888\n",
      "Iter:  599  Loss on train set:  -94.0848945938848\n",
      "Iter:  600  Loss on train set:  -94.1255260589186\n",
      "     fun: -94.15735792756205\n",
      "     jac: array([-4.182524,  4.182524, -4.182524, -4.182524,  4.182524,  4.182524,\n",
      "        4.182524,  4.182524, -4.182524, -4.182524,  4.182524, -4.182524,\n",
      "        4.182524, -4.182524,  4.182524,  4.182524,  4.182524, -4.182524,\n",
      "        4.182524,  4.182524, -4.182524,  4.182524, -4.182524,  4.182524,\n",
      "       -4.182524, -4.182524, -4.182524, -4.182524,  4.182524,  4.182524,\n",
      "       -4.182524, -4.182524])\n",
      "    nfev: 200\n",
      "     nit: 200\n",
      " success: True\n",
      "       x: array([-0.06319518,  0.29487101, -0.28798552,  0.1115183 , -0.5976094 ,\n",
      "        0.05227527,  0.09319094,  0.02217189,  0.94933768,  0.27258229,\n",
      "       -1.10596005,  0.57537747, -0.05203168, -0.25139467,  0.23465604,\n",
      "        0.38263971,  0.11695983, -0.1542352 ,  0.20277187,  0.00501416,\n",
      "       -0.03405293, -0.09722894,  0.16114205, -0.22277511, -0.1456221 ,\n",
      "        0.03171206,  0.25626456, -0.47647748, -0.03609086, -0.01479226,\n",
      "        0.19009639, -0.1244917 ])\n",
      "Unfrozen params:  32  Frozen params:  64\n",
      "Iter:  601  Loss on train set:  -94.15735792756205\n",
      "Iter:  602  Loss on train set:  -94.0350929210389\n",
      "Iter:  603  Loss on train set:  -94.06046248285939\n",
      "Iter:  604  Loss on train set:  -94.26696519518194\n",
      "Iter:  605  Loss on train set:  -94.39953953966064\n",
      "Iter:  606  Loss on train set:  -94.84828234910508\n",
      "Iter:  607  Loss on train set:  -95.18591277991588\n",
      "Iter:  608  Loss on train set:  -95.44722161921668\n",
      "Iter:  609  Loss on train set:  -95.6445129297134\n",
      "Iter:  610  Loss on train set:  -95.79130216067848\n",
      "Iter:  611  Loss on train set:  -95.91191123015923\n",
      "Iter:  612  Loss on train set:  -96.00510154407755\n",
      "Iter:  613  Loss on train set:  -96.08013652043702\n",
      "Iter:  614  Loss on train set:  -96.18485311434713\n",
      "Iter:  615  Loss on train set:  -96.26797511164354\n",
      "Iter:  616  Loss on train set:  -96.3677691186898\n",
      "Iter:  617  Loss on train set:  -96.55347472165221\n",
      "Iter:  618  Loss on train set:  -96.71944517566688\n",
      "Iter:  619  Loss on train set:  -96.88488971439297\n",
      "Iter:  620  Loss on train set:  -97.01815156948336\n",
      "Iter:  621  Loss on train set:  -97.13666609184354\n",
      "Iter:  622  Loss on train set:  -97.23198270665331\n",
      "Iter:  623  Loss on train set:  -97.3194343314758\n",
      "Iter:  624  Loss on train set:  -97.40298566427872\n",
      "Iter:  625  Loss on train set:  -97.48763786313056\n",
      "Iter:  626  Loss on train set:  -97.55312817461686\n",
      "Iter:  627  Loss on train set:  -97.60337994320777\n",
      "Iter:  628  Loss on train set:  -97.64533987357446\n",
      "Iter:  629  Loss on train set:  -97.73612533426605\n",
      "Iter:  630  Loss on train set:  -97.80774317781471\n",
      "Iter:  631  Loss on train set:  -97.86778492320501\n",
      "Iter:  632  Loss on train set:  -97.91377790017597\n",
      "Iter:  633  Loss on train set:  -97.94769202214002\n",
      "Iter:  634  Loss on train set:  -97.97476369527253\n",
      "Iter:  635  Loss on train set:  -98.01479315726249\n",
      "Iter:  636  Loss on train set:  -98.06643448248509\n",
      "Iter:  637  Loss on train set:  -98.10921104903466\n",
      "Iter:  638  Loss on train set:  -98.1662358434202\n",
      "Iter:  639  Loss on train set:  -98.23747601959386\n",
      "Iter:  640  Loss on train set:  -98.31002856542194\n",
      "Iter:  641  Loss on train set:  -98.38568266156814\n",
      "Iter:  642  Loss on train set:  -98.44383638931261\n",
      "Iter:  643  Loss on train set:  -98.49442922597318\n",
      "Iter:  644  Loss on train set:  -98.55504635387598\n",
      "Iter:  645  Loss on train set:  -98.6038370839738\n",
      "Iter:  646  Loss on train set:  -98.64148137408807\n",
      "Iter:  647  Loss on train set:  -98.6685502642092\n",
      "Iter:  648  Loss on train set:  -98.6853817909488\n",
      "Iter:  649  Loss on train set:  -98.69766565820964\n",
      "Iter:  650  Loss on train set:  -98.70412366395573\n",
      "Iter:  651  Loss on train set:  -98.72790837253476\n",
      "Iter:  652  Loss on train set:  -98.75582379142396\n",
      "Iter:  653  Loss on train set:  -98.7755118289059\n",
      "Iter:  654  Loss on train set:  -98.80165137679327\n",
      "Iter:  655  Loss on train set:  -98.8184624510546\n",
      "Iter:  656  Loss on train set:  -98.82838674285264\n",
      "Iter:  657  Loss on train set:  -98.83287832447435\n",
      "Iter:  658  Loss on train set:  -98.84040679495641\n",
      "Iter:  659  Loss on train set:  -98.86181259324565\n",
      "Iter:  660  Loss on train set:  -98.8751502134749\n",
      "Iter:  661  Loss on train set:  -98.88159033452425\n",
      "Iter:  662  Loss on train set:  -98.88314162009613\n",
      "Iter:  663  Loss on train set:  -98.88418787605482\n",
      "Iter:  664  Loss on train set:  -98.88015479486748\n",
      "Iter:  665  Loss on train set:  -98.88493370214191\n",
      "Iter:  666  Loss on train set:  -98.89896452090743\n",
      "Iter:  667  Loss on train set:  -98.94111936328046\n",
      "Iter:  668  Loss on train set:  -98.96980754548444\n",
      "Iter:  669  Loss on train set:  -98.99681307041902\n",
      "Iter:  670  Loss on train set:  -99.01667317095401\n",
      "Iter:  671  Loss on train set:  -99.02881765673852\n",
      "Iter:  672  Loss on train set:  -99.04436103878305\n",
      "Iter:  673  Loss on train set:  -99.05389816936477\n",
      "Iter:  674  Loss on train set:  -99.060404160122\n",
      "Iter:  675  Loss on train set:  -99.08382675471084\n",
      "Iter:  676  Loss on train set:  -99.09543896161325\n",
      "Iter:  677  Loss on train set:  -99.1120520519202\n",
      "Iter:  678  Loss on train set:  -99.12075340055031\n",
      "Iter:  679  Loss on train set:  -99.11946824754159\n",
      "Iter:  680  Loss on train set:  -99.122559769541\n",
      "Iter:  681  Loss on train set:  -99.12382112434378\n",
      "Iter:  682  Loss on train set:  -99.12463491649855\n",
      "Iter:  683  Loss on train set:  -99.12300650861415\n",
      "Iter:  684  Loss on train set:  -99.12335406040484\n",
      "Iter:  685  Loss on train set:  -99.15109995211625\n",
      "Iter:  686  Loss on train set:  -99.17120781440033\n",
      "Iter:  687  Loss on train set:  -99.18504520045583\n",
      "Iter:  688  Loss on train set:  -99.19885185602709\n",
      "Iter:  689  Loss on train set:  -99.22391354631847\n",
      "Iter:  690  Loss on train set:  -99.24820612690168\n",
      "Iter:  691  Loss on train set:  -99.2702458479911\n",
      "Iter:  692  Loss on train set:  -99.28452485565317\n",
      "Iter:  693  Loss on train set:  -99.2925414844137\n",
      "Iter:  694  Loss on train set:  -99.29655836163906\n",
      "Iter:  695  Loss on train set:  -99.2982673925805\n",
      "Iter:  696  Loss on train set:  -99.29469428968804\n",
      "Iter:  697  Loss on train set:  -99.2890845418619\n",
      "Iter:  698  Loss on train set:  -99.27856898176547\n",
      "Iter:  699  Loss on train set:  -99.26471538698759\n",
      "Iter:  700  Loss on train set:  -99.28095269022769\n",
      "Iter:  701  Loss on train set:  -99.28565897899428\n",
      "Iter:  702  Loss on train set:  -99.29194426251195\n",
      "Iter:  703  Loss on train set:  -99.31029712071565\n",
      "Iter:  704  Loss on train set:  -99.39485001346851\n",
      "Iter:  705  Loss on train set:  -99.44378375658907\n",
      "Iter:  706  Loss on train set:  -99.46913769141185\n",
      "Iter:  707  Loss on train set:  -99.47459214651334\n",
      "Iter:  708  Loss on train set:  -99.46480274936424\n",
      "Iter:  709  Loss on train set:  -99.44331378750157\n",
      "Iter:  710  Loss on train set:  -99.41357705686866\n",
      "Iter:  711  Loss on train set:  -99.37808313465345\n",
      "Iter:  712  Loss on train set:  -99.36975620228073\n",
      "Iter:  713  Loss on train set:  -99.36504293085073\n",
      "Iter:  714  Loss on train set:  -99.36297006436075\n",
      "Iter:  715  Loss on train set:  -99.37142714875444\n",
      "Iter:  716  Loss on train set:  -99.37570428239742\n",
      "Iter:  717  Loss on train set:  -99.37269019035509\n",
      "Iter:  718  Loss on train set:  -99.37047232869486\n",
      "Iter:  719  Loss on train set:  -99.35720998919749\n",
      "Iter:  720  Loss on train set:  -99.34519555135935\n",
      "Iter:  721  Loss on train set:  -99.32735454922882\n",
      "Iter:  722  Loss on train set:  -99.30944433090765\n",
      "Iter:  723  Loss on train set:  -99.29485272505731\n",
      "Iter:  724  Loss on train set:  -99.27920813209444\n",
      "Iter:  725  Loss on train set:  -99.30670728276147\n",
      "Iter:  726  Loss on train set:  -99.33701229633422\n",
      "Iter:  727  Loss on train set:  -99.3544695426859\n",
      "Iter:  728  Loss on train set:  -99.35984788963411\n",
      "Iter:  729  Loss on train set:  -99.35564007892958\n",
      "Iter:  730  Loss on train set:  -99.35139639125288\n",
      "Iter:  731  Loss on train set:  -99.33833427554232\n",
      "Iter:  732  Loss on train set:  -99.32850103657114\n",
      "Iter:  733  Loss on train set:  -99.31771487502321\n",
      "Iter:  734  Loss on train set:  -99.29987095654285\n",
      "Iter:  735  Loss on train set:  -99.27739479106211\n",
      "Iter:  736  Loss on train set:  -99.2957511703026\n",
      "Iter:  737  Loss on train set:  -99.30459518123911\n",
      "Iter:  738  Loss on train set:  -99.3044568266736\n",
      "Iter:  739  Loss on train set:  -99.29591374717683\n",
      "Iter:  740  Loss on train set:  -99.29093235521017\n",
      "Iter:  741  Loss on train set:  -99.30148350518688\n",
      "Iter:  742  Loss on train set:  -99.30838980765809\n",
      "Iter:  743  Loss on train set:  -99.33272767377413\n",
      "Iter:  744  Loss on train set:  -99.34768368604742\n",
      "Iter:  745  Loss on train set:  -99.3567183241485\n",
      "Iter:  746  Loss on train set:  -99.35943788525233\n",
      "Iter:  747  Loss on train set:  -99.36058512088621\n",
      "Iter:  748  Loss on train set:  -99.35885379962862\n",
      "Iter:  749  Loss on train set:  -99.36310962205296\n",
      "Iter:  750  Loss on train set:  -99.37518810186492\n",
      "Iter:  751  Loss on train set:  -99.40685188566535\n",
      "Iter:  752  Loss on train set:  -99.43034740556736\n",
      "Iter:  753  Loss on train set:  -99.45565179190253\n",
      "Iter:  754  Loss on train set:  -99.47789581448899\n",
      "Iter:  755  Loss on train set:  -99.49867535110332\n",
      "Iter:  756  Loss on train set:  -99.51700843062383\n",
      "Iter:  757  Loss on train set:  -99.53297689143713\n",
      "Iter:  758  Loss on train set:  -99.54582517441663\n",
      "Iter:  759  Loss on train set:  -99.5711298526498\n",
      "Iter:  760  Loss on train set:  -99.58931690390475\n",
      "Iter:  761  Loss on train set:  -99.60536819358664\n",
      "Iter:  762  Loss on train set:  -99.61517783659637\n",
      "Iter:  763  Loss on train set:  -99.63658473363226\n",
      "Iter:  764  Loss on train set:  -99.6506416218195\n",
      "Iter:  765  Loss on train set:  -99.66450117462554\n",
      "Iter:  766  Loss on train set:  -99.67668417745345\n",
      "Iter:  767  Loss on train set:  -99.68887534922885\n",
      "Iter:  768  Loss on train set:  -99.69657183382964\n",
      "Iter:  769  Loss on train set:  -99.70071401285041\n",
      "Iter:  770  Loss on train set:  -99.70450836920999\n",
      "Iter:  771  Loss on train set:  -99.70616748970308\n",
      "Iter:  772  Loss on train set:  -99.7056516567078\n",
      "Iter:  773  Loss on train set:  -99.70376208090147\n",
      "Iter:  774  Loss on train set:  -99.70134421068917\n",
      "Iter:  775  Loss on train set:  -99.70279755217584\n",
      "Iter:  776  Loss on train set:  -99.70248925490255\n",
      "Iter:  777  Loss on train set:  -99.70198675960592\n",
      "Iter:  778  Loss on train set:  -99.70012860451644\n",
      "Iter:  779  Loss on train set:  -99.69747206004004\n",
      "Iter:  780  Loss on train set:  -99.69460666492327\n",
      "Iter:  781  Loss on train set:  -99.69111059740274\n",
      "Iter:  782  Loss on train set:  -99.69397328628048\n",
      "Iter:  783  Loss on train set:  -99.70126276791896\n",
      "Iter:  784  Loss on train set:  -99.70776380892801\n",
      "Iter:  785  Loss on train set:  -99.71249891636\n",
      "Iter:  786  Loss on train set:  -99.72208919927306\n",
      "Iter:  787  Loss on train set:  -99.72857955187607\n",
      "Iter:  788  Loss on train set:  -99.73617715290905\n",
      "Iter:  789  Loss on train set:  -99.7407308373678\n",
      "Iter:  790  Loss on train set:  -99.74568172431043\n",
      "Iter:  791  Loss on train set:  -99.75365714954647\n",
      "Iter:  792  Loss on train set:  -99.75847603804038\n",
      "Iter:  793  Loss on train set:  -99.76802402678385\n",
      "Iter:  794  Loss on train set:  -99.77491205691146\n",
      "Iter:  795  Loss on train set:  -99.77877105887288\n",
      "Iter:  796  Loss on train set:  -99.77810570130045\n",
      "Iter:  797  Loss on train set:  -99.7735827098825\n",
      "Iter:  798  Loss on train set:  -99.76739820181413\n",
      "Iter:  799  Loss on train set:  -99.76126750315079\n",
      "Iter:  800  Loss on train set:  -99.75710731122356\n",
      "     fun: -99.75313941357359\n",
      "     jac: array([-1.11511742,  1.11511742, -1.11511742,  1.11511742,  1.11511742,\n",
      "       -1.11511742,  1.11511742,  1.11511742, -1.11511742, -1.11511742,\n",
      "       -1.11511742,  1.11511742, -1.11511742, -1.11511742, -1.11511742,\n",
      "        1.11511742,  1.11511742, -1.11511742, -1.11511742, -1.11511742,\n",
      "        1.11511742,  1.11511742, -1.11511742,  1.11511742,  1.11511742,\n",
      "        1.11511742, -1.11511742,  1.11511742, -1.11511742, -1.11511742,\n",
      "        1.11511742,  1.11511742])\n",
      "    nfev: 200\n",
      "     nit: 200\n",
      " success: True\n",
      "       x: array([ 0.69804408,  0.16896195, -0.74517346, -0.40920404,  0.45966432,\n",
      "       -0.47890682, -0.16986087,  0.2203663 , -0.12056539, -0.32746328,\n",
      "       -0.47118298, -0.34392757, -0.56127353,  0.03303192, -0.77172711,\n",
      "       -0.0575852 ,  0.07808579, -0.65809047,  0.06465939, -0.54451711,\n",
      "        0.19977526,  0.43001215, -0.50897523,  0.31224489, -0.1413924 ,\n",
      "       -0.11864855, -0.53491973,  0.74458126, -0.04562669, -0.62820873,\n",
      "        0.15457917, -0.36970248])\n",
      "Running repetition:  1\n",
      "Unfrozen params:  16  Frozen params:  80\n",
      "Iter:  801  Loss on train set:  -99.75313941357359\n",
      "Iter:  802  Loss on train set:  -99.75218238472321\n",
      "Iter:  803  Loss on train set:  -99.49562161222573\n",
      "Iter:  804  Loss on train set:  -99.59835265391682\n",
      "Iter:  805  Loss on train set:  -99.57238534542942\n",
      "Iter:  806  Loss on train set:  -99.50594509074551\n",
      "Iter:  807  Loss on train set:  -99.42698342868452\n",
      "Iter:  808  Loss on train set:  -99.43777106489821\n",
      "Iter:  809  Loss on train set:  -99.37607327244415\n",
      "Iter:  810  Loss on train set:  -99.29032103692492\n",
      "Iter:  811  Loss on train set:  -99.1731928775439\n",
      "Iter:  812  Loss on train set:  -99.46973664074757\n",
      "Iter:  813  Loss on train set:  -99.64752838354609\n",
      "Iter:  814  Loss on train set:  -99.72192665291666\n",
      "Iter:  815  Loss on train set:  -99.72376141222851\n",
      "Iter:  816  Loss on train set:  -99.67219603406907\n",
      "Iter:  817  Loss on train set:  -99.65411520945264\n",
      "Iter:  818  Loss on train set:  -99.6033585053615\n",
      "Iter:  819  Loss on train set:  -99.53183348393867\n",
      "Iter:  820  Loss on train set:  -99.5142047612576\n",
      "Iter:  821  Loss on train set:  -99.533467520515\n",
      "Iter:  822  Loss on train set:  -99.56304613413818\n",
      "Iter:  823  Loss on train set:  -99.6107722648745\n",
      "Iter:  824  Loss on train set:  -99.6496840263693\n",
      "Iter:  825  Loss on train set:  -99.69015824697202\n",
      "Iter:  826  Loss on train set:  -99.72757612114341\n",
      "Iter:  827  Loss on train set:  -99.7964961775866\n",
      "Iter:  828  Loss on train set:  -99.86519800043294\n",
      "Iter:  829  Loss on train set:  -99.92808600084778\n",
      "Iter:  830  Loss on train set:  -99.98503670884024\n",
      "Iter:  831  Loss on train set:  -100.05586061108446\n",
      "Iter:  832  Loss on train set:  -100.16526748752092\n",
      "Iter:  833  Loss on train set:  -100.27497964461705\n",
      "Iter:  834  Loss on train set:  -100.36214025412242\n",
      "Iter:  835  Loss on train set:  -100.44004353434963\n",
      "Iter:  836  Loss on train set:  -100.50265920941652\n",
      "Iter:  837  Loss on train set:  -100.54471046015028\n",
      "Iter:  838  Loss on train set:  -100.56968767556918\n",
      "Iter:  839  Loss on train set:  -100.58107675501336\n",
      "Iter:  840  Loss on train set:  -100.58144639844919\n",
      "Iter:  841  Loss on train set:  -100.60920429076393\n",
      "Iter:  842  Loss on train set:  -100.62853294997653\n",
      "Iter:  843  Loss on train set:  -100.63782465138246\n",
      "Iter:  844  Loss on train set:  -100.64595241001012\n",
      "Iter:  845  Loss on train set:  -100.71875606711366\n",
      "Iter:  846  Loss on train set:  -100.78745181362184\n",
      "Iter:  847  Loss on train set:  -100.86394265013911\n",
      "Iter:  848  Loss on train set:  -100.92214108803547\n",
      "Iter:  849  Loss on train set:  -101.0325371048922\n",
      "Iter:  850  Loss on train set:  -101.12544361131285\n",
      "Iter:  851  Loss on train set:  -101.19564992015802\n",
      "Iter:  852  Loss on train set:  -101.23621563940844\n",
      "Iter:  853  Loss on train set:  -101.25703807433592\n",
      "Iter:  854  Loss on train set:  -101.31791338779865\n",
      "Iter:  855  Loss on train set:  -101.34852905551554\n",
      "Iter:  856  Loss on train set:  -101.35230053618199\n",
      "Iter:  857  Loss on train set:  -101.33537902359845\n",
      "Iter:  858  Loss on train set:  -101.32240363296498\n",
      "Iter:  859  Loss on train set:  -101.3036942914191\n",
      "Iter:  860  Loss on train set:  -101.30085245596817\n",
      "Iter:  861  Loss on train set:  -101.28797815120554\n",
      "Iter:  862  Loss on train set:  -101.27573625242285\n",
      "Iter:  863  Loss on train set:  -101.2603065107044\n",
      "Iter:  864  Loss on train set:  -101.25330907282202\n",
      "Iter:  865  Loss on train set:  -101.24315734626165\n",
      "Iter:  866  Loss on train set:  -101.22206208736362\n",
      "Iter:  867  Loss on train set:  -101.19773314588178\n",
      "Iter:  868  Loss on train set:  -101.2013393440654\n",
      "Iter:  869  Loss on train set:  -101.20329927535013\n",
      "Iter:  870  Loss on train set:  -101.19174228386267\n",
      "Iter:  871  Loss on train set:  -101.17243737001192\n",
      "Iter:  872  Loss on train set:  -101.14080464110427\n",
      "Iter:  873  Loss on train set:  -101.10053920264431\n",
      "Iter:  874  Loss on train set:  -101.05975668564658\n",
      "Iter:  875  Loss on train set:  -101.0402822105305\n",
      "Iter:  876  Loss on train set:  -101.03562928091398\n",
      "Iter:  877  Loss on train set:  -101.0240081321397\n",
      "Iter:  878  Loss on train set:  -101.06261729226452\n",
      "Iter:  879  Loss on train set:  -101.08764772590064\n",
      "Iter:  880  Loss on train set:  -101.12054382111566\n",
      "Iter:  881  Loss on train set:  -101.15895792938015\n",
      "Iter:  882  Loss on train set:  -101.18409373453638\n",
      "Iter:  883  Loss on train set:  -101.31753499243099\n",
      "Iter:  884  Loss on train set:  -101.39014659020825\n",
      "Iter:  885  Loss on train set:  -101.42199976859435\n",
      "Iter:  886  Loss on train set:  -101.44932471834767\n",
      "Iter:  887  Loss on train set:  -101.4566307323104\n",
      "Iter:  888  Loss on train set:  -101.44744250616054\n",
      "Iter:  889  Loss on train set:  -101.42563131812017\n",
      "Iter:  890  Loss on train set:  -101.39682495994657\n",
      "Iter:  891  Loss on train set:  -101.37422951158615\n",
      "Iter:  892  Loss on train set:  -101.35194206102503\n",
      "Iter:  893  Loss on train set:  -101.3533993299375\n",
      "Iter:  894  Loss on train set:  -101.34890249690956\n",
      "Iter:  895  Loss on train set:  -101.43156139538982\n",
      "Iter:  896  Loss on train set:  -101.48612009731127\n",
      "Iter:  897  Loss on train set:  -101.50762486100929\n",
      "Iter:  898  Loss on train set:  -101.50471209231321\n",
      "Iter:  899  Loss on train set:  -101.4986406854416\n",
      "Iter:  900  Loss on train set:  -101.47576862496629\n",
      "Iter:  901  Loss on train set:  -101.43991355707693\n",
      "Iter:  902  Loss on train set:  -101.39592966090198\n",
      "Iter:  903  Loss on train set:  -101.36914531950059\n",
      "Iter:  904  Loss on train set:  -101.36218169682697\n",
      "Iter:  905  Loss on train set:  -101.37079509228319\n",
      "Iter:  906  Loss on train set:  -101.37893680320649\n",
      "Iter:  907  Loss on train set:  -101.38062898435732\n",
      "Iter:  908  Loss on train set:  -101.38446970398546\n",
      "Iter:  909  Loss on train set:  -101.39167497221182\n",
      "Iter:  910  Loss on train set:  -101.44784692939433\n",
      "Iter:  911  Loss on train set:  -101.47235711537627\n",
      "Iter:  912  Loss on train set:  -101.47079711049581\n",
      "Iter:  913  Loss on train set:  -101.45202964113115\n",
      "Iter:  914  Loss on train set:  -101.45123856313884\n",
      "Iter:  915  Loss on train set:  -101.45835791931124\n",
      "Iter:  916  Loss on train set:  -101.4662628462533\n",
      "Iter:  917  Loss on train set:  -101.46209545695227\n",
      "Iter:  918  Loss on train set:  -101.45721337027821\n",
      "Iter:  919  Loss on train set:  -101.44507804872055\n",
      "Iter:  920  Loss on train set:  -101.42967813946913\n",
      "Iter:  921  Loss on train set:  -101.41180612427384\n",
      "Iter:  922  Loss on train set:  -101.40075598451035\n",
      "Iter:  923  Loss on train set:  -101.39270310056608\n",
      "Iter:  924  Loss on train set:  -101.38363035414591\n",
      "Iter:  925  Loss on train set:  -101.37864173068371\n",
      "Iter:  926  Loss on train set:  -101.40322952470987\n",
      "Iter:  927  Loss on train set:  -101.42635505759111\n",
      "Iter:  928  Loss on train set:  -101.44506685021844\n",
      "Iter:  929  Loss on train set:  -101.45349097236311\n",
      "Iter:  930  Loss on train set:  -101.45564686886426\n",
      "Iter:  931  Loss on train set:  -101.45885052190162\n",
      "Iter:  932  Loss on train set:  -101.45857396301831\n",
      "Iter:  933  Loss on train set:  -101.45271469101507\n",
      "Iter:  934  Loss on train set:  -101.44424562878956\n",
      "Iter:  935  Loss on train set:  -101.43343017346774\n",
      "Iter:  936  Loss on train set:  -101.42190245760851\n",
      "Iter:  937  Loss on train set:  -101.41072093402218\n",
      "Iter:  938  Loss on train set:  -101.40573600440783\n",
      "Iter:  939  Loss on train set:  -101.41400625761175\n",
      "Iter:  940  Loss on train set:  -101.4312297184338\n",
      "Iter:  941  Loss on train set:  -101.43457137794516\n",
      "Iter:  942  Loss on train set:  -101.42703519331414\n",
      "Iter:  943  Loss on train set:  -101.41104309529862\n",
      "Iter:  944  Loss on train set:  -101.41055282524468\n",
      "Iter:  945  Loss on train set:  -101.39570082828256\n",
      "Iter:  946  Loss on train set:  -101.37948437838509\n",
      "Iter:  947  Loss on train set:  -101.36616252435422\n",
      "Iter:  948  Loss on train set:  -101.39275485885643\n",
      "Iter:  949  Loss on train set:  -101.39802658680253\n",
      "Iter:  950  Loss on train set:  -101.38865935856468\n",
      "Iter:  951  Loss on train set:  -101.38075116834915\n",
      "Iter:  952  Loss on train set:  -101.36303674204309\n",
      "Iter:  953  Loss on train set:  -101.36684206577692\n",
      "Iter:  954  Loss on train set:  -101.34733922753418\n",
      "Iter:  955  Loss on train set:  -101.29321111578399\n",
      "Iter:  956  Loss on train set:  -101.20578948842581\n",
      "Iter:  957  Loss on train set:  -101.17593210845084\n",
      "Iter:  958  Loss on train set:  -101.1133456275022\n",
      "Iter:  959  Loss on train set:  -101.07014740946718\n",
      "Iter:  960  Loss on train set:  -101.09047875869776\n",
      "Iter:  961  Loss on train set:  -101.11899138430171\n",
      "Iter:  962  Loss on train set:  -101.21611335797138\n",
      "Iter:  963  Loss on train set:  -101.29237350166308\n",
      "Iter:  964  Loss on train set:  -101.32099585533189\n",
      "Iter:  965  Loss on train set:  -101.3393868201169\n",
      "Iter:  966  Loss on train set:  -101.33197340126917\n",
      "Iter:  967  Loss on train set:  -101.31905031275139\n",
      "Iter:  968  Loss on train set:  -101.29029730711326\n",
      "Iter:  969  Loss on train set:  -101.32541614293213\n",
      "Iter:  970  Loss on train set:  -101.34150367617038\n",
      "Iter:  971  Loss on train set:  -101.4021218527079\n",
      "Iter:  972  Loss on train set:  -101.43445881597859\n",
      "Iter:  973  Loss on train set:  -101.44594201765123\n",
      "Iter:  974  Loss on train set:  -101.44001314411827\n",
      "Iter:  975  Loss on train set:  -101.42134113336714\n",
      "Iter:  976  Loss on train set:  -101.39574645709413\n",
      "Iter:  977  Loss on train set:  -101.36549375497809\n",
      "Iter:  978  Loss on train set:  -101.33424378125748\n",
      "Iter:  979  Loss on train set:  -101.30662923531489\n",
      "Iter:  980  Loss on train set:  -101.27643739730485\n",
      "Iter:  981  Loss on train set:  -101.25215960558604\n",
      "Iter:  982  Loss on train set:  -101.22125350888528\n",
      "Iter:  983  Loss on train set:  -101.19488031446177\n",
      "Iter:  984  Loss on train set:  -101.17946591313932\n",
      "Iter:  985  Loss on train set:  -101.16470006305002\n",
      "Iter:  986  Loss on train set:  -101.16222075504677\n",
      "Iter:  987  Loss on train set:  -101.15863040946036\n",
      "Iter:  988  Loss on train set:  -101.14823857724973\n",
      "Iter:  989  Loss on train set:  -101.13274759487842\n",
      "Iter:  990  Loss on train set:  -101.10752756975451\n",
      "Iter:  991  Loss on train set:  -101.08313406366938\n",
      "Iter:  992  Loss on train set:  -101.0987997036482\n",
      "Iter:  993  Loss on train set:  -101.14301813433053\n",
      "Iter:  994  Loss on train set:  -101.17574313924325\n",
      "Iter:  995  Loss on train set:  -101.1902206761584\n",
      "Iter:  996  Loss on train set:  -101.19456454708873\n",
      "Iter:  997  Loss on train set:  -101.18344441870298\n",
      "Iter:  998  Loss on train set:  -101.17468661242597\n",
      "Iter:  999  Loss on train set:  -101.1584076045416\n",
      "Iter:  1000  Loss on train set:  -101.13686620737533\n",
      "     fun: -101.12257955615992\n",
      "     jac: array([-3.7749838, -3.7749838, -3.7749838,  3.7749838, -3.7749838,\n",
      "        3.7749838,  3.7749838,  3.7749838,  3.7749838,  3.7749838,\n",
      "       -3.7749838, -3.7749838,  3.7749838, -3.7749838, -3.7749838,\n",
      "        3.7749838])\n",
      "    nfev: 200\n",
      "     nit: 200\n",
      " success: True\n",
      "       x: array([-0.27054913, -2.19662632, -0.88249736, -1.00420969,  0.54356547,\n",
      "       -1.37156714, -1.42697101, -0.38077661,  0.11626892, -1.97315675,\n",
      "       -0.4154062 , -1.68233081, -1.0848763 , -0.70781395,  1.38419502,\n",
      "        0.61858245])\n",
      "Unfrozen params:  16  Frozen params:  80\n",
      "Iter:  1001  Loss on train set:  -101.12257955615992\n",
      "Iter:  1002  Loss on train set:  -101.17522946841754\n",
      "Iter:  1003  Loss on train set:  -101.19412317001836\n",
      "Iter:  1004  Loss on train set:  -101.1646447879107\n",
      "Iter:  1005  Loss on train set:  -101.18756182745821\n",
      "Iter:  1006  Loss on train set:  -101.20516215312901\n",
      "Iter:  1007  Loss on train set:  -101.20607291774729\n",
      "Iter:  1008  Loss on train set:  -101.18709711808486\n",
      "Iter:  1009  Loss on train set:  -101.18743104610971\n",
      "Iter:  1010  Loss on train set:  -101.21248156295827\n",
      "Iter:  1011  Loss on train set:  -101.31159378403939\n",
      "Iter:  1012  Loss on train set:  -101.38953362297516\n",
      "Iter:  1013  Loss on train set:  -101.4480616662945\n",
      "Iter:  1014  Loss on train set:  -101.48739837660882\n",
      "Iter:  1015  Loss on train set:  -101.51378787267444\n",
      "Iter:  1016  Loss on train set:  -101.5430506424435\n",
      "Iter:  1017  Loss on train set:  -101.56330219844043\n",
      "Iter:  1018  Loss on train set:  -101.57904120447733\n",
      "Iter:  1019  Loss on train set:  -101.599003754622\n",
      "Iter:  1020  Loss on train set:  -101.62049847598189\n",
      "Iter:  1021  Loss on train set:  -101.62414302527765\n",
      "Iter:  1022  Loss on train set:  -101.617551945284\n",
      "Iter:  1023  Loss on train set:  -101.6003890994912\n",
      "Iter:  1024  Loss on train set:  -101.5747192285414\n",
      "Iter:  1025  Loss on train set:  -101.6351180466937\n",
      "Iter:  1026  Loss on train set:  -101.69141874240987\n",
      "Iter:  1027  Loss on train set:  -101.75006452495793\n",
      "Iter:  1028  Loss on train set:  -101.7934716788422\n",
      "Iter:  1029  Loss on train set:  -101.82897036910464\n",
      "Iter:  1030  Loss on train set:  -101.87870484063973\n",
      "Iter:  1031  Loss on train set:  -101.9145218135431\n",
      "Iter:  1032  Loss on train set:  -101.93873300263861\n",
      "Iter:  1033  Loss on train set:  -101.95473085808793\n",
      "Iter:  1034  Loss on train set:  -101.96380768657016\n",
      "Iter:  1035  Loss on train set:  -101.9726459224763\n",
      "Iter:  1036  Loss on train set:  -101.97784250977544\n",
      "Iter:  1037  Loss on train set:  -101.97773996810577\n",
      "Iter:  1038  Loss on train set:  -101.97263751152198\n",
      "Iter:  1039  Loss on train set:  -101.97973417649195\n",
      "Iter:  1040  Loss on train set:  -101.99069908913008\n",
      "Iter:  1041  Loss on train set:  -101.99785551719513\n",
      "Iter:  1042  Loss on train set:  -102.00515150463798\n",
      "Iter:  1043  Loss on train set:  -102.00557139514203\n",
      "Iter:  1044  Loss on train set:  -102.01804366716331\n",
      "Iter:  1045  Loss on train set:  -102.022442411591\n",
      "Iter:  1046  Loss on train set:  -102.03532963638602\n",
      "Iter:  1047  Loss on train set:  -102.03631346743339\n",
      "Iter:  1048  Loss on train set:  -102.04944051440174\n",
      "Iter:  1049  Loss on train set:  -102.05660353565432\n",
      "Iter:  1050  Loss on train set:  -102.05970952827747\n",
      "Iter:  1051  Loss on train set:  -102.06201541303886\n",
      "Iter:  1052  Loss on train set:  -102.0620749141047\n",
      "Iter:  1053  Loss on train set:  -102.05905136908146\n",
      "Iter:  1054  Loss on train set:  -102.06240860725252\n",
      "Iter:  1055  Loss on train set:  -102.0645665740869\n",
      "Iter:  1056  Loss on train set:  -102.08053061025733\n",
      "Iter:  1057  Loss on train set:  -102.09373913313235\n",
      "Iter:  1058  Loss on train set:  -102.10986813303796\n",
      "Iter:  1059  Loss on train set:  -102.12455552573115\n",
      "Iter:  1060  Loss on train set:  -102.13514397431409\n",
      "Iter:  1061  Loss on train set:  -102.14176066875038\n",
      "Iter:  1062  Loss on train set:  -102.14644841911587\n",
      "Iter:  1063  Loss on train set:  -102.15435707614475\n",
      "Iter:  1064  Loss on train set:  -102.16128204051796\n",
      "Iter:  1065  Loss on train set:  -102.16719185117991\n",
      "Iter:  1066  Loss on train set:  -102.17265111029826\n",
      "Iter:  1067  Loss on train set:  -102.17779272850359\n",
      "Iter:  1068  Loss on train set:  -102.18182011837321\n",
      "Iter:  1069  Loss on train set:  -102.19080372320276\n",
      "Iter:  1070  Loss on train set:  -102.20027306183196\n",
      "Iter:  1071  Loss on train set:  -102.20878558422118\n",
      "Iter:  1072  Loss on train set:  -102.21549914210416\n",
      "Iter:  1073  Loss on train set:  -102.22392564851178\n",
      "Iter:  1074  Loss on train set:  -102.23133660981742\n",
      "Iter:  1075  Loss on train set:  -102.23858751886033\n",
      "Iter:  1076  Loss on train set:  -102.24322926053517\n",
      "Iter:  1077  Loss on train set:  -102.24575175360224\n",
      "Iter:  1078  Loss on train set:  -102.2469062458743\n",
      "Iter:  1079  Loss on train set:  -102.2471952922244\n",
      "Iter:  1080  Loss on train set:  -102.2468339592723\n",
      "Iter:  1081  Loss on train set:  -102.25318794528843\n",
      "Iter:  1082  Loss on train set:  -102.26007805052188\n",
      "Iter:  1083  Loss on train set:  -102.26555332459189\n",
      "Iter:  1084  Loss on train set:  -102.27009680268344\n",
      "Iter:  1085  Loss on train set:  -102.27407946236094\n",
      "Iter:  1086  Loss on train set:  -102.2776159203843\n",
      "Iter:  1087  Loss on train set:  -102.28463630999862\n",
      "Iter:  1088  Loss on train set:  -102.29040468542112\n",
      "Iter:  1089  Loss on train set:  -102.2967944573224\n",
      "Iter:  1090  Loss on train set:  -102.30158413237007\n",
      "Iter:  1091  Loss on train set:  -102.30498448747763\n",
      "Iter:  1092  Loss on train set:  -102.30737733048866\n",
      "Iter:  1093  Loss on train set:  -102.30890247747215\n",
      "Iter:  1094  Loss on train set:  -102.30982988560734\n",
      "Iter:  1095  Loss on train set:  -102.31068189310172\n",
      "Iter:  1096  Loss on train set:  -102.31112795650141\n",
      "Iter:  1097  Loss on train set:  -102.31121778827801\n",
      "Iter:  1098  Loss on train set:  -102.31220983458859\n",
      "Iter:  1099  Loss on train set:  -102.31447186544312\n",
      "Iter:  1100  Loss on train set:  -102.31609448451898\n",
      "Iter:  1101  Loss on train set:  -102.31759007635185\n",
      "Iter:  1102  Loss on train set:  -102.32024421014943\n",
      "Iter:  1103  Loss on train set:  -102.32213291751226\n",
      "Iter:  1104  Loss on train set:  -102.3237433262317\n",
      "Iter:  1105  Loss on train set:  -102.3247340124114\n",
      "Iter:  1106  Loss on train set:  -102.32623173755431\n",
      "Iter:  1107  Loss on train set:  -102.3279912779402\n",
      "Iter:  1108  Loss on train set:  -102.32938126582782\n",
      "Iter:  1109  Loss on train set:  -102.3305121432582\n",
      "Iter:  1110  Loss on train set:  -102.3316890076402\n",
      "Iter:  1111  Loss on train set:  -102.33256965775699\n",
      "Iter:  1112  Loss on train set:  -102.33423741593435\n",
      "Iter:  1113  Loss on train set:  -102.33565821704066\n",
      "Iter:  1114  Loss on train set:  -102.33689574104434\n",
      "Iter:  1115  Loss on train set:  -102.3378051881098\n",
      "Iter:  1116  Loss on train set:  -102.33840997391913\n",
      "Iter:  1117  Loss on train set:  -102.33893872979606\n",
      "Iter:  1118  Loss on train set:  -102.33969202399545\n",
      "Iter:  1119  Loss on train set:  -102.34036623326274\n",
      "Iter:  1120  Loss on train set:  -102.34097290428541\n",
      "Iter:  1121  Loss on train set:  -102.34151547985707\n",
      "Iter:  1122  Loss on train set:  -102.3417839035675\n",
      "Iter:  1123  Loss on train set:  -102.3418537967592\n",
      "Iter:  1124  Loss on train set:  -102.3421519751533\n",
      "Iter:  1125  Loss on train set:  -102.34229642676469\n",
      "Iter:  1126  Loss on train set:  -102.34243641392774\n",
      "Iter:  1127  Loss on train set:  -102.34277054006218\n",
      "Iter:  1128  Loss on train set:  -102.34299856215135\n",
      "Iter:  1129  Loss on train set:  -102.34432818367472\n",
      "Iter:  1130  Loss on train set:  -102.34534449027353\n",
      "Iter:  1131  Loss on train set:  -102.34598614704535\n",
      "Iter:  1132  Loss on train set:  -102.34646416379303\n",
      "Iter:  1133  Loss on train set:  -102.34713268096708\n",
      "Iter:  1134  Loss on train set:  -102.34782272554224\n",
      "Iter:  1135  Loss on train set:  -102.34848022843003\n",
      "Iter:  1136  Loss on train set:  -102.34993503110739\n",
      "Iter:  1137  Loss on train set:  -102.35116155708323\n",
      "Iter:  1138  Loss on train set:  -102.3520546902581\n",
      "Iter:  1139  Loss on train set:  -102.35281264209488\n",
      "Iter:  1140  Loss on train set:  -102.35334857075951\n",
      "Iter:  1141  Loss on train set:  -102.35440692840152\n",
      "Iter:  1142  Loss on train set:  -102.35588390038545\n",
      "Iter:  1143  Loss on train set:  -102.35769075749968\n",
      "Iter:  1144  Loss on train set:  -102.36149508733939\n",
      "Iter:  1145  Loss on train set:  -102.36368288793369\n",
      "Iter:  1146  Loss on train set:  -102.36421819999676\n",
      "Iter:  1147  Loss on train set:  -102.36365115844585\n",
      "Iter:  1148  Loss on train set:  -102.36243771252988\n",
      "Iter:  1149  Loss on train set:  -102.36072556984479\n",
      "Iter:  1150  Loss on train set:  -102.35870415176167\n",
      "Iter:  1151  Loss on train set:  -102.35814071595951\n",
      "Iter:  1152  Loss on train set:  -102.35775420197132\n",
      "Iter:  1153  Loss on train set:  -102.35692121674067\n",
      "Iter:  1154  Loss on train set:  -102.35824313085615\n",
      "Iter:  1155  Loss on train set:  -102.3592995001887\n",
      "Iter:  1156  Loss on train set:  -102.36133782505169\n",
      "Iter:  1157  Loss on train set:  -102.36286060323691\n",
      "Iter:  1158  Loss on train set:  -102.36561797247056\n",
      "Iter:  1159  Loss on train set:  -102.36779856763927\n",
      "Iter:  1160  Loss on train set:  -102.37294226268014\n",
      "Iter:  1161  Loss on train set:  -102.3763950680434\n",
      "Iter:  1162  Loss on train set:  -102.3785501631322\n",
      "Iter:  1163  Loss on train set:  -102.3804190932273\n",
      "Iter:  1164  Loss on train set:  -102.38156074244382\n",
      "Iter:  1165  Loss on train set:  -102.38196548938484\n",
      "Iter:  1166  Loss on train set:  -102.38163184241881\n",
      "Iter:  1167  Loss on train set:  -102.38112015387895\n",
      "Iter:  1168  Loss on train set:  -102.38297354634354\n",
      "Iter:  1169  Loss on train set:  -102.38434819443378\n",
      "Iter:  1170  Loss on train set:  -102.38523862412458\n",
      "Iter:  1171  Loss on train set:  -102.38576787083429\n",
      "Iter:  1172  Loss on train set:  -102.3859483074091\n",
      "Iter:  1173  Loss on train set:  -102.38580193486328\n",
      "Iter:  1174  Loss on train set:  -102.38765798472585\n",
      "Iter:  1175  Loss on train set:  -102.38927923168592\n",
      "Iter:  1176  Loss on train set:  -102.39034669543365\n",
      "Iter:  1177  Loss on train set:  -102.39072075052603\n",
      "Iter:  1178  Loss on train set:  -102.39172471714117\n",
      "Iter:  1179  Loss on train set:  -102.39452731749078\n",
      "Iter:  1180  Loss on train set:  -102.39596967989554\n",
      "Iter:  1181  Loss on train set:  -102.39550342357387\n",
      "Iter:  1182  Loss on train set:  -102.39381310421298\n",
      "Iter:  1183  Loss on train set:  -102.39130077323141\n",
      "Iter:  1184  Loss on train set:  -102.38797739347193\n",
      "Iter:  1185  Loss on train set:  -102.38484520403169\n",
      "Iter:  1186  Loss on train set:  -102.3814755363193\n",
      "Iter:  1187  Loss on train set:  -102.37809933579828\n",
      "Iter:  1188  Loss on train set:  -102.37685023966155\n",
      "Iter:  1189  Loss on train set:  -102.37490743041619\n",
      "Iter:  1190  Loss on train set:  -102.37407000032216\n",
      "Iter:  1191  Loss on train set:  -102.37531774132898\n",
      "Iter:  1192  Loss on train set:  -102.37598412623316\n",
      "Iter:  1193  Loss on train set:  -102.37617287650572\n",
      "Iter:  1194  Loss on train set:  -102.37618982532352\n",
      "Iter:  1195  Loss on train set:  -102.38776514742887\n",
      "Iter:  1196  Loss on train set:  -102.39462664213285\n",
      "Iter:  1197  Loss on train set:  -102.3959565900347\n",
      "Iter:  1198  Loss on train set:  -102.39623979608264\n",
      "Iter:  1199  Loss on train set:  -102.39169983316069\n",
      "Iter:  1200  Loss on train set:  -102.38488945191716\n",
      "     fun: -102.37710158777666\n",
      "     jac: array([ 0.89035894, -0.89035894, -0.89035894,  0.89035894,  0.89035894,\n",
      "       -0.89035894, -0.89035894,  0.89035894,  0.89035894,  0.89035894,\n",
      "       -0.89035894, -0.89035894, -0.89035894,  0.89035894,  0.89035894,\n",
      "        0.89035894])\n",
      "    nfev: 200\n",
      "     nit: 200\n",
      " success: True\n",
      "       x: array([ 0.09128361,  0.0060747 , -0.04112284,  0.00953874, -0.09436942,\n",
      "        0.01543685, -1.9337899 ,  0.03732041,  2.06352864,  0.15657026,\n",
      "        0.87304052, -0.12124765, -0.02771368, -0.00628759,  0.1972308 ,\n",
      "       -0.04117118])\n",
      "Unfrozen params:  32  Frozen params:  64\n",
      "Iter:  1201  Loss on train set:  -102.37710158777666\n",
      "Iter:  1202  Loss on train set:  -102.27158908166658\n",
      "Iter:  1203  Loss on train set:  -102.0253044351879\n",
      "Iter:  1204  Loss on train set:  -102.09177716007868\n",
      "Iter:  1205  Loss on train set:  -102.05585508750954\n",
      "Iter:  1206  Loss on train set:  -101.93784918692371\n",
      "Iter:  1207  Loss on train set:  -101.77408025284335\n",
      "Iter:  1208  Loss on train set:  -101.68196464659383\n",
      "Iter:  1209  Loss on train set:  -101.66266146348562\n",
      "Iter:  1210  Loss on train set:  -101.5721206485315\n",
      "Iter:  1211  Loss on train set:  -101.42623921679352\n",
      "Iter:  1212  Loss on train set:  -101.3361142604387\n",
      "Iter:  1213  Loss on train set:  -101.4828571370034\n",
      "Iter:  1214  Loss on train set:  -101.64180440667512\n",
      "Iter:  1215  Loss on train set:  -101.72580971272934\n",
      "Iter:  1216  Loss on train set:  -101.74253207247565\n",
      "Iter:  1217  Loss on train set:  -101.89129342121262\n",
      "Iter:  1218  Loss on train set:  -102.02147214209906\n",
      "Iter:  1219  Loss on train set:  -102.08253766376399\n",
      "Iter:  1220  Loss on train set:  -102.1716825560272\n",
      "Iter:  1221  Loss on train set:  -102.17600336767532\n",
      "Iter:  1222  Loss on train set:  -102.12552388970349\n",
      "Iter:  1223  Loss on train set:  -102.0270247623986\n",
      "Iter:  1224  Loss on train set:  -101.98246108125329\n",
      "Iter:  1225  Loss on train set:  -101.91636038519118\n",
      "Iter:  1226  Loss on train set:  -101.83611939735788\n",
      "Iter:  1227  Loss on train set:  -101.74678827831242\n",
      "Iter:  1228  Loss on train set:  -101.70075837150272\n",
      "Iter:  1229  Loss on train set:  -101.70647704110384\n",
      "Iter:  1230  Loss on train set:  -101.68397470873093\n",
      "Iter:  1231  Loss on train set:  -101.70715771063792\n",
      "Iter:  1232  Loss on train set:  -101.70294447557424\n",
      "Iter:  1233  Loss on train set:  -101.67358481078502\n",
      "Iter:  1234  Loss on train set:  -101.68109930741576\n",
      "Iter:  1235  Loss on train set:  -101.67428256713431\n",
      "Iter:  1236  Loss on train set:  -101.6685604754127\n",
      "Iter:  1237  Loss on train set:  -101.68999497048004\n",
      "Iter:  1238  Loss on train set:  -101.69764499618327\n",
      "Iter:  1239  Loss on train set:  -101.68883124516375\n",
      "Iter:  1240  Loss on train set:  -101.75066519471979\n",
      "Iter:  1241  Loss on train set:  -101.76429566552198\n",
      "Iter:  1242  Loss on train set:  -101.76714004741325\n",
      "Iter:  1243  Loss on train set:  -101.76706095729395\n",
      "Iter:  1244  Loss on train set:  -101.73124137664868\n",
      "Iter:  1245  Loss on train set:  -101.7064337026537\n",
      "Iter:  1246  Loss on train set:  -101.64247849218523\n",
      "Iter:  1247  Loss on train set:  -101.65742810281719\n",
      "Iter:  1248  Loss on train set:  -101.59636114795317\n",
      "Iter:  1249  Loss on train set:  -101.48418113590508\n",
      "Iter:  1250  Loss on train set:  -101.33803191341764\n",
      "Iter:  1251  Loss on train set:  -101.2104561926377\n",
      "Iter:  1252  Loss on train set:  -101.53197492133114\n",
      "Iter:  1253  Loss on train set:  -101.80524090833849\n",
      "Iter:  1254  Loss on train set:  -102.00149147538454\n",
      "Iter:  1255  Loss on train set:  -102.17196199933578\n",
      "Iter:  1256  Loss on train set:  -102.26360402755526\n",
      "Iter:  1257  Loss on train set:  -102.29471892088951\n",
      "Iter:  1258  Loss on train set:  -102.28146016955833\n",
      "Iter:  1259  Loss on train set:  -102.23548547931465\n",
      "Iter:  1260  Loss on train set:  -102.22402881011267\n",
      "Iter:  1261  Loss on train set:  -102.20518144135845\n",
      "Iter:  1262  Loss on train set:  -102.14736080682786\n",
      "Iter:  1263  Loss on train set:  -102.11026834187082\n",
      "Iter:  1264  Loss on train set:  -102.05429576823832\n",
      "Iter:  1265  Loss on train set:  -102.03090247332734\n",
      "Iter:  1266  Loss on train set:  -102.03373645719199\n",
      "Iter:  1267  Loss on train set:  -102.03481998337243\n",
      "Iter:  1268  Loss on train set:  -102.04825774438405\n",
      "Iter:  1269  Loss on train set:  -102.0719571712475\n",
      "Iter:  1270  Loss on train set:  -102.07264821446701\n",
      "Iter:  1271  Loss on train set:  -102.10035287584029\n",
      "Iter:  1272  Loss on train set:  -102.11586142831895\n",
      "Iter:  1273  Loss on train set:  -102.13943373849197\n",
      "Iter:  1274  Loss on train set:  -102.13511933259441\n",
      "Iter:  1275  Loss on train set:  -102.10824637415121\n",
      "Iter:  1276  Loss on train set:  -102.06607491461754\n",
      "Iter:  1277  Loss on train set:  -102.01221195927042\n",
      "Iter:  1278  Loss on train set:  -101.95930482266988\n",
      "Iter:  1279  Loss on train set:  -101.91804563077609\n",
      "Iter:  1280  Loss on train set:  -101.8712831968987\n",
      "Iter:  1281  Loss on train set:  -101.8446619236675\n",
      "Iter:  1282  Loss on train set:  -101.870773621649\n",
      "Iter:  1283  Loss on train set:  -101.93829094128392\n",
      "Iter:  1284  Loss on train set:  -102.09480979536464\n",
      "Iter:  1285  Loss on train set:  -102.1750359323561\n",
      "Iter:  1286  Loss on train set:  -102.19958650466413\n",
      "Iter:  1287  Loss on train set:  -102.18055937589487\n",
      "Iter:  1288  Loss on train set:  -102.12931366181917\n",
      "Iter:  1289  Loss on train set:  -102.14257500556471\n",
      "Iter:  1290  Loss on train set:  -102.14510376462526\n",
      "Iter:  1291  Loss on train set:  -102.19786125482813\n",
      "Iter:  1292  Loss on train set:  -102.21683820729898\n",
      "Iter:  1293  Loss on train set:  -102.22090075280643\n",
      "Iter:  1294  Loss on train set:  -102.19653757929984\n",
      "Iter:  1295  Loss on train set:  -102.14679471093292\n",
      "Iter:  1296  Loss on train set:  -102.17363757534582\n",
      "Iter:  1297  Loss on train set:  -102.17679471087808\n",
      "Iter:  1298  Loss on train set:  -102.18513454776398\n",
      "Iter:  1299  Loss on train set:  -102.26999679648489\n",
      "Iter:  1300  Loss on train set:  -102.32406597588489\n",
      "Iter:  1301  Loss on train set:  -102.36131326507738\n",
      "Iter:  1302  Loss on train set:  -102.42854009549684\n",
      "Iter:  1303  Loss on train set:  -102.46952214704923\n",
      "Iter:  1304  Loss on train set:  -102.49444420490028\n",
      "Iter:  1305  Loss on train set:  -102.50423861714341\n",
      "Iter:  1306  Loss on train set:  -102.50556736061813\n",
      "Iter:  1307  Loss on train set:  -102.50170183862654\n",
      "Iter:  1308  Loss on train set:  -102.49852816424067\n",
      "Iter:  1309  Loss on train set:  -102.49293915679858\n",
      "Iter:  1310  Loss on train set:  -102.48433769996413\n",
      "Iter:  1311  Loss on train set:  -102.47314863579163\n",
      "Iter:  1312  Loss on train set:  -102.47961093055125\n",
      "Iter:  1313  Loss on train set:  -102.50600277811508\n",
      "Iter:  1314  Loss on train set:  -102.51300473960703\n",
      "Iter:  1315  Loss on train set:  -102.5117957968177\n",
      "Iter:  1316  Loss on train set:  -102.49196032398504\n",
      "Iter:  1317  Loss on train set:  -102.46073321587203\n",
      "Iter:  1318  Loss on train set:  -102.48701332906563\n",
      "Iter:  1319  Loss on train set:  -102.50440384517546\n",
      "Iter:  1320  Loss on train set:  -102.50967452357212\n",
      "Iter:  1321  Loss on train set:  -102.52478485165437\n",
      "Iter:  1322  Loss on train set:  -102.52276088325107\n",
      "Iter:  1323  Loss on train set:  -102.50812285824178\n",
      "Iter:  1324  Loss on train set:  -102.51311393879827\n",
      "Iter:  1325  Loss on train set:  -102.50129484785792\n",
      "Iter:  1326  Loss on train set:  -102.50374288725597\n",
      "Iter:  1327  Loss on train set:  -102.5027996853633\n",
      "Iter:  1328  Loss on train set:  -102.54903771392185\n",
      "Iter:  1329  Loss on train set:  -102.57253998162318\n",
      "Iter:  1330  Loss on train set:  -102.57884052080612\n",
      "Iter:  1331  Loss on train set:  -102.57229535906824\n",
      "Iter:  1332  Loss on train set:  -102.55849021839241\n",
      "Iter:  1333  Loss on train set:  -102.54531388945823\n",
      "Iter:  1334  Loss on train set:  -102.53282542094804\n",
      "Iter:  1335  Loss on train set:  -102.51969296929654\n",
      "Iter:  1336  Loss on train set:  -102.52452299854494\n",
      "Iter:  1337  Loss on train set:  -102.5390420962705\n",
      "Iter:  1338  Loss on train set:  -102.55076702883787\n",
      "Iter:  1339  Loss on train set:  -102.5689296207496\n",
      "Iter:  1340  Loss on train set:  -102.58416581406667\n",
      "Iter:  1341  Loss on train set:  -102.59535164471518\n",
      "Iter:  1342  Loss on train set:  -102.60194977691673\n",
      "Iter:  1343  Loss on train set:  -102.60242007898967\n",
      "Iter:  1344  Loss on train set:  -102.5945446490643\n",
      "Iter:  1345  Loss on train set:  -102.59554378974921\n",
      "Iter:  1346  Loss on train set:  -102.64249010905088\n",
      "Iter:  1347  Loss on train set:  -102.67824664258431\n",
      "Iter:  1348  Loss on train set:  -102.70168924596224\n",
      "Iter:  1349  Loss on train set:  -102.72135165097433\n",
      "Iter:  1350  Loss on train set:  -102.72568100744729\n",
      "Iter:  1351  Loss on train set:  -102.73375354357898\n",
      "Iter:  1352  Loss on train set:  -102.73273068726695\n",
      "Iter:  1353  Loss on train set:  -102.72328299656489\n",
      "Iter:  1354  Loss on train set:  -102.70770991346818\n",
      "Iter:  1355  Loss on train set:  -102.68908038887227\n",
      "Iter:  1356  Loss on train set:  -102.66915993560526\n",
      "Iter:  1357  Loss on train set:  -102.64707632487196\n",
      "Iter:  1358  Loss on train set:  -102.62450863032507\n",
      "Iter:  1359  Loss on train set:  -102.60070352024297\n",
      "Iter:  1360  Loss on train set:  -102.58024460192115\n",
      "Iter:  1361  Loss on train set:  -102.55956678000139\n",
      "Iter:  1362  Loss on train set:  -102.54331088119143\n",
      "Iter:  1363  Loss on train set:  -102.53204313909697\n",
      "Iter:  1364  Loss on train set:  -102.5181437650489\n",
      "Iter:  1365  Loss on train set:  -102.50239499129763\n",
      "Iter:  1366  Loss on train set:  -102.49149687429036\n",
      "Iter:  1367  Loss on train set:  -102.49272188230074\n",
      "Iter:  1368  Loss on train set:  -102.50451298234455\n",
      "Iter:  1369  Loss on train set:  -102.54079457094855\n",
      "Iter:  1370  Loss on train set:  -102.56979487862372\n",
      "Iter:  1371  Loss on train set:  -102.5732751658111\n",
      "Iter:  1372  Loss on train set:  -102.60755118846683\n",
      "Iter:  1373  Loss on train set:  -102.62037314114622\n",
      "Iter:  1374  Loss on train set:  -102.61882349489262\n",
      "Iter:  1375  Loss on train set:  -102.60685514847749\n",
      "Iter:  1376  Loss on train set:  -102.59579111446467\n",
      "Iter:  1377  Loss on train set:  -102.5796542600666\n",
      "Iter:  1378  Loss on train set:  -102.56066530567732\n",
      "Iter:  1379  Loss on train set:  -102.56201483556279\n",
      "Iter:  1380  Loss on train set:  -102.56506206419817\n",
      "Iter:  1381  Loss on train set:  -102.55729840353324\n",
      "Iter:  1382  Loss on train set:  -102.53747993368415\n",
      "Iter:  1383  Loss on train set:  -102.51818211773521\n",
      "Iter:  1384  Loss on train set:  -102.50161210919539\n",
      "Iter:  1385  Loss on train set:  -102.48344877606385\n",
      "Iter:  1386  Loss on train set:  -102.47735184235583\n",
      "Iter:  1387  Loss on train set:  -102.4654490458327\n",
      "Iter:  1388  Loss on train set:  -102.44980235577837\n",
      "Iter:  1389  Loss on train set:  -102.47535483660903\n",
      "Iter:  1390  Loss on train set:  -102.55416888059605\n",
      "Iter:  1391  Loss on train set:  -102.59478469597187\n",
      "Iter:  1392  Loss on train set:  -102.62394928019131\n",
      "Iter:  1393  Loss on train set:  -102.63985018986727\n",
      "Iter:  1394  Loss on train set:  -102.64876131120123\n",
      "Iter:  1395  Loss on train set:  -102.63634267012986\n",
      "Iter:  1396  Loss on train set:  -102.61633054271645\n",
      "Iter:  1397  Loss on train set:  -102.57644675063715\n",
      "Iter:  1398  Loss on train set:  -102.5628043935558\n",
      "Iter:  1399  Loss on train set:  -102.52553131955234\n",
      "Iter:  1400  Loss on train set:  -102.46565302715149\n",
      "     fun: -102.38902965304135\n",
      "     jac: array([ 1.60846208,  1.60846208,  1.60846208,  1.60846208,  1.60846208,\n",
      "       -1.60846208,  1.60846208, -1.60846208,  1.60846208, -1.60846208,\n",
      "        1.60846208,  1.60846208,  1.60846208, -1.60846208, -1.60846208,\n",
      "        1.60846208, -1.60846208, -1.60846208,  1.60846208,  1.60846208,\n",
      "        1.60846208,  1.60846208, -1.60846208, -1.60846208,  1.60846208,\n",
      "       -1.60846208,  1.60846208,  1.60846208, -1.60846208,  1.60846208,\n",
      "       -1.60846208, -1.60846208])\n",
      "    nfev: 200\n",
      "     nit: 200\n",
      " success: True\n",
      "       x: array([ 0.08885243,  0.25910562, -0.74728467, -0.05046309, -0.16276394,\n",
      "        0.13637753, -0.76205262, -0.12777886,  1.89380929,  0.09395194,\n",
      "       -1.56272727,  0.285973  ,  0.27264006, -0.6838298 , -0.17756727,\n",
      "        0.15089131,  0.07513738,  0.0029205 ,  0.0815129 ,  0.26544784,\n",
      "        0.03334851, -0.0832621 ,  0.14754275, -0.30068169,  0.02206776,\n",
      "       -0.06556241,  0.58005337, -0.91479511,  0.07430891, -0.04264954,\n",
      "       -0.0989596 ,  0.14242597])\n",
      "Unfrozen params:  32  Frozen params:  64\n",
      "Iter:  1401  Loss on train set:  -102.38902965304135\n",
      "Iter:  1402  Loss on train set:  -102.46621593302885\n",
      "Iter:  1403  Loss on train set:  -102.38984080792189\n",
      "Iter:  1404  Loss on train set:  -102.43486324009997\n",
      "Iter:  1405  Loss on train set:  -102.47023600290618\n",
      "Iter:  1406  Loss on train set:  -102.46483160271438\n",
      "Iter:  1407  Loss on train set:  -102.43869124168644\n",
      "Iter:  1408  Loss on train set:  -102.34277006899039\n",
      "Iter:  1409  Loss on train set:  -102.28081621393903\n",
      "Iter:  1410  Loss on train set:  -102.24938876500545\n",
      "Iter:  1411  Loss on train set:  -102.22895086638871\n",
      "Iter:  1412  Loss on train set:  -102.297859219863\n",
      "Iter:  1413  Loss on train set:  -102.33890317252795\n",
      "Iter:  1414  Loss on train set:  -102.36776413707638\n",
      "Iter:  1415  Loss on train set:  -102.48970657690724\n",
      "Iter:  1416  Loss on train set:  -102.56501724212991\n",
      "Iter:  1417  Loss on train set:  -102.61592822240812\n",
      "Iter:  1418  Loss on train set:  -102.62280628790046\n",
      "Iter:  1419  Loss on train set:  -102.60165351407196\n",
      "Iter:  1420  Loss on train set:  -102.63848103017952\n",
      "Iter:  1421  Loss on train set:  -102.66530727781227\n",
      "Iter:  1422  Loss on train set:  -102.68462455404718\n",
      "Iter:  1423  Loss on train set:  -102.68575124607825\n",
      "Iter:  1424  Loss on train set:  -102.67219703259958\n",
      "Iter:  1425  Loss on train set:  -102.64691348265873\n",
      "Iter:  1426  Loss on train set:  -102.61322022526178\n",
      "Iter:  1427  Loss on train set:  -102.6008452488993\n",
      "Iter:  1428  Loss on train set:  -102.62477761525452\n",
      "Iter:  1429  Loss on train set:  -102.65519531472717\n",
      "Iter:  1430  Loss on train set:  -102.6717078456753\n",
      "Iter:  1431  Loss on train set:  -102.6833905566584\n",
      "Iter:  1432  Loss on train set:  -102.68601914196684\n",
      "Iter:  1433  Loss on train set:  -102.70633753872674\n",
      "Iter:  1434  Loss on train set:  -102.72103338785533\n",
      "Iter:  1435  Loss on train set:  -102.72747180227674\n",
      "Iter:  1436  Loss on train set:  -102.73179328028019\n",
      "Iter:  1437  Loss on train set:  -102.74033956098819\n",
      "Iter:  1438  Loss on train set:  -102.73949951458499\n",
      "Iter:  1439  Loss on train set:  -102.73104609115846\n",
      "Iter:  1440  Loss on train set:  -102.71420721132648\n",
      "Iter:  1441  Loss on train set:  -102.70597995483365\n",
      "Iter:  1442  Loss on train set:  -102.68923333953663\n",
      "Iter:  1443  Loss on train set:  -102.66615832135832\n",
      "Iter:  1444  Loss on train set:  -102.64162692838956\n",
      "Iter:  1445  Loss on train set:  -102.61470386574216\n",
      "Iter:  1446  Loss on train set:  -102.60868798092065\n",
      "Iter:  1447  Loss on train set:  -102.61783907988047\n",
      "Iter:  1448  Loss on train set:  -102.60153106447797\n",
      "Iter:  1449  Loss on train set:  -102.56723363475265\n",
      "Iter:  1450  Loss on train set:  -102.54812485834142\n",
      "Iter:  1451  Loss on train set:  -102.53866357074438\n",
      "Iter:  1452  Loss on train set:  -102.53188253471761\n",
      "Iter:  1453  Loss on train set:  -102.51558657258099\n",
      "Iter:  1454  Loss on train set:  -102.51482508953069\n",
      "Iter:  1455  Loss on train set:  -102.5029822850944\n",
      "Iter:  1456  Loss on train set:  -102.49578159663703\n",
      "Iter:  1457  Loss on train set:  -102.51104752687516\n",
      "Iter:  1458  Loss on train set:  -102.52877860032027\n",
      "Iter:  1459  Loss on train set:  -102.51608562919492\n",
      "Iter:  1460  Loss on train set:  -102.47291092836019\n",
      "Iter:  1461  Loss on train set:  -102.4620072451479\n",
      "Iter:  1462  Loss on train set:  -102.44277260458284\n",
      "Iter:  1463  Loss on train set:  -102.41069534656205\n",
      "Iter:  1464  Loss on train set:  -102.39300684618782\n",
      "Iter:  1465  Loss on train set:  -102.36317345369358\n",
      "Iter:  1466  Loss on train set:  -102.32386984975075\n",
      "Iter:  1467  Loss on train set:  -102.27146690984297\n",
      "Iter:  1468  Loss on train set:  -102.34528828069766\n",
      "Iter:  1469  Loss on train set:  -102.39397506782585\n",
      "Iter:  1470  Loss on train set:  -102.45407504372434\n",
      "Iter:  1471  Loss on train set:  -102.50057055548608\n",
      "Iter:  1472  Loss on train set:  -102.56994650231108\n",
      "Iter:  1473  Loss on train set:  -102.62674013075441\n",
      "Iter:  1474  Loss on train set:  -102.67590002128549\n",
      "Iter:  1475  Loss on train set:  -102.69914003369036\n",
      "Iter:  1476  Loss on train set:  -102.77075059622288\n",
      "Iter:  1477  Loss on train set:  -102.85524931206389\n",
      "Iter:  1478  Loss on train set:  -102.90230938367822\n",
      "Iter:  1479  Loss on train set:  -102.9289149769981\n",
      "Iter:  1480  Loss on train set:  -102.93900492277083\n",
      "Iter:  1481  Loss on train set:  -102.92784669288281\n",
      "Iter:  1482  Loss on train set:  -102.90117355749112\n",
      "Iter:  1483  Loss on train set:  -102.86422688945663\n",
      "Iter:  1484  Loss on train set:  -102.84577072528707\n",
      "Iter:  1485  Loss on train set:  -102.83428412081334\n",
      "Iter:  1486  Loss on train set:  -102.83526638910865\n",
      "Iter:  1487  Loss on train set:  -102.82791324172857\n",
      "Iter:  1488  Loss on train set:  -102.82451755532409\n",
      "Iter:  1489  Loss on train set:  -102.81808862962146\n",
      "Iter:  1490  Loss on train set:  -102.8117311534828\n",
      "Iter:  1491  Loss on train set:  -102.8036456333364\n",
      "Iter:  1492  Loss on train set:  -102.7923760120359\n",
      "Iter:  1493  Loss on train set:  -102.77840715568759\n",
      "Iter:  1494  Loss on train set:  -102.76257154724784\n",
      "Iter:  1495  Loss on train set:  -102.75978759001902\n",
      "Iter:  1496  Loss on train set:  -102.75776728871674\n",
      "Iter:  1497  Loss on train set:  -102.75587253383817\n",
      "Iter:  1498  Loss on train set:  -102.75358846287142\n",
      "Iter:  1499  Loss on train set:  -102.76933539764588\n",
      "Iter:  1500  Loss on train set:  -102.78099434840605\n",
      "Iter:  1501  Loss on train set:  -102.80759694315265\n",
      "Iter:  1502  Loss on train set:  -102.83609627564644\n",
      "Iter:  1503  Loss on train set:  -102.87384178142354\n",
      "Iter:  1504  Loss on train set:  -102.91207940949754\n",
      "Iter:  1505  Loss on train set:  -102.92497708959993\n",
      "Iter:  1506  Loss on train set:  -102.91373732373181\n",
      "Iter:  1507  Loss on train set:  -102.88387643729179\n",
      "Iter:  1508  Loss on train set:  -102.84043214043402\n",
      "Iter:  1509  Loss on train set:  -102.79366003565447\n",
      "Iter:  1510  Loss on train set:  -102.75015538778864\n",
      "Iter:  1511  Loss on train set:  -102.79291874334618\n",
      "Iter:  1512  Loss on train set:  -102.85279999626786\n",
      "Iter:  1513  Loss on train set:  -102.87370715776612\n",
      "Iter:  1514  Loss on train set:  -102.87531884460033\n",
      "Iter:  1515  Loss on train set:  -102.84199659325373\n",
      "Iter:  1516  Loss on train set:  -102.80263725628599\n",
      "Iter:  1517  Loss on train set:  -102.75763145480705\n",
      "Iter:  1518  Loss on train set:  -102.70291150609393\n",
      "Iter:  1519  Loss on train set:  -102.65061331954726\n",
      "Iter:  1520  Loss on train set:  -102.60723441072061\n",
      "Iter:  1521  Loss on train set:  -102.58742016495032\n",
      "Iter:  1522  Loss on train set:  -102.58538973521361\n",
      "Iter:  1523  Loss on train set:  -102.5952725993712\n",
      "Iter:  1524  Loss on train set:  -102.63322239102514\n",
      "Iter:  1525  Loss on train set:  -102.69744133558198\n",
      "Iter:  1526  Loss on train set:  -102.78188858776443\n",
      "Iter:  1527  Loss on train set:  -102.81876433462271\n",
      "Iter:  1528  Loss on train set:  -102.7980890514372\n",
      "Iter:  1529  Loss on train set:  -102.73847133505505\n",
      "Iter:  1530  Loss on train set:  -102.65175196277289\n",
      "Iter:  1531  Loss on train set:  -102.54684443548528\n",
      "Iter:  1532  Loss on train set:  -102.46724462136274\n",
      "Iter:  1533  Loss on train set:  -102.38080139701285\n",
      "Iter:  1534  Loss on train set:  -102.29571209461335\n",
      "Iter:  1535  Loss on train set:  -102.25524743852841\n",
      "Iter:  1536  Loss on train set:  -102.285063071641\n",
      "Iter:  1537  Loss on train set:  -102.32395439139354\n",
      "Iter:  1538  Loss on train set:  -102.34019981697622\n",
      "Iter:  1539  Loss on train set:  -102.4051514620076\n",
      "Iter:  1540  Loss on train set:  -102.45767942651055\n",
      "Iter:  1541  Loss on train set:  -102.47196385466418\n",
      "Iter:  1542  Loss on train set:  -102.46684074357537\n",
      "Iter:  1543  Loss on train set:  -102.4543909834623\n",
      "Iter:  1544  Loss on train set:  -102.44871034811945\n",
      "Iter:  1545  Loss on train set:  -102.40119874707645\n",
      "Iter:  1546  Loss on train set:  -102.32099298312235\n",
      "Iter:  1547  Loss on train set:  -102.29056950233951\n",
      "Iter:  1548  Loss on train set:  -102.23808214972154\n",
      "Iter:  1549  Loss on train set:  -102.19551809474336\n",
      "Iter:  1550  Loss on train set:  -102.12703444355874\n",
      "Iter:  1551  Loss on train set:  -102.06616193905529\n",
      "Iter:  1552  Loss on train set:  -102.0465919550413\n",
      "Iter:  1553  Loss on train set:  -102.02590461741295\n",
      "Iter:  1554  Loss on train set:  -101.99204282081143\n",
      "Iter:  1555  Loss on train set:  -102.00285813245623\n",
      "Iter:  1556  Loss on train set:  -101.99394205814093\n",
      "Iter:  1557  Loss on train set:  -101.98396868280918\n",
      "Iter:  1558  Loss on train set:  -101.98689239493577\n",
      "Iter:  1559  Loss on train set:  -102.06412488604182\n",
      "Iter:  1560  Loss on train set:  -102.0920507118965\n",
      "Iter:  1561  Loss on train set:  -102.08326867106807\n",
      "Iter:  1562  Loss on train set:  -102.06116845386455\n",
      "Iter:  1563  Loss on train set:  -102.02352945094147\n",
      "Iter:  1564  Loss on train set:  -101.97734738030469\n",
      "Iter:  1565  Loss on train set:  -101.92704237432449\n",
      "Iter:  1566  Loss on train set:  -101.9012832183476\n",
      "Iter:  1567  Loss on train set:  -101.9119736071241\n",
      "Iter:  1568  Loss on train set:  -101.91638223172129\n",
      "Iter:  1569  Loss on train set:  -101.94147778292867\n",
      "Iter:  1570  Loss on train set:  -101.95861327302585\n",
      "Iter:  1571  Loss on train set:  -101.97688203123948\n",
      "Iter:  1572  Loss on train set:  -101.99723027847575\n",
      "Iter:  1573  Loss on train set:  -102.01010912903487\n",
      "Iter:  1574  Loss on train set:  -102.01743848608722\n",
      "Iter:  1575  Loss on train set:  -102.02198700573543\n",
      "Iter:  1576  Loss on train set:  -102.02765170698197\n",
      "Iter:  1577  Loss on train set:  -102.04755400103217\n",
      "Iter:  1578  Loss on train set:  -102.07471031658451\n",
      "Iter:  1579  Loss on train set:  -102.15696255512339\n",
      "Iter:  1580  Loss on train set:  -102.22491524803306\n",
      "Iter:  1581  Loss on train set:  -102.30323135091461\n",
      "Iter:  1582  Loss on train set:  -102.35767698191064\n",
      "Iter:  1583  Loss on train set:  -102.3863712269028\n",
      "Iter:  1584  Loss on train set:  -102.39945251583502\n",
      "Iter:  1585  Loss on train set:  -102.41981868528747\n",
      "Iter:  1586  Loss on train set:  -102.43553369100161\n",
      "Iter:  1587  Loss on train set:  -102.43971233254584\n",
      "Iter:  1588  Loss on train set:  -102.48464764656232\n",
      "Iter:  1589  Loss on train set:  -102.52257513615096\n",
      "Iter:  1590  Loss on train set:  -102.53103767337659\n",
      "Iter:  1591  Loss on train set:  -102.53742601892814\n",
      "Iter:  1592  Loss on train set:  -102.53335001305331\n",
      "Iter:  1593  Loss on train set:  -102.50952577456522\n",
      "Iter:  1594  Loss on train set:  -102.47335982840531\n",
      "Iter:  1595  Loss on train set:  -102.42824487887782\n",
      "Iter:  1596  Loss on train set:  -102.38707823494016\n",
      "Iter:  1597  Loss on train set:  -102.37905401849679\n",
      "Iter:  1598  Loss on train set:  -102.36588013840517\n",
      "Iter:  1599  Loss on train set:  -102.32697155095009\n",
      "Iter:  1600  Loss on train set:  -102.32440436929869\n",
      "     fun: -102.30917611732285\n",
      "     jac: array([-2.80009996, -2.80009996, -2.80009996,  2.80009996, -2.80009996,\n",
      "        2.80009996, -2.80009996, -2.80009996, -2.80009996,  2.80009996,\n",
      "       -2.80009996, -2.80009996, -2.80009996,  2.80009996, -2.80009996,\n",
      "       -2.80009996,  2.80009996,  2.80009996, -2.80009996,  2.80009996,\n",
      "        2.80009996,  2.80009996,  2.80009996, -2.80009996,  2.80009996,\n",
      "        2.80009996, -2.80009996, -2.80009996, -2.80009996, -2.80009996,\n",
      "       -2.80009996,  2.80009996])\n",
      "    nfev: 200\n",
      "     nit: 200\n",
      " success: True\n",
      "       x: array([ 1.29463952,  0.27800678, -0.66966424, -0.51303384,  1.1616922 ,\n",
      "        0.33010712,  0.32189707, -0.20874963,  0.75373494, -0.31588129,\n",
      "       -1.54054631, -0.31461484, -1.35224275,  0.02863737,  0.13092319,\n",
      "        0.0338368 ,  0.24695109, -0.70766794, -0.3990405 , -0.1932501 ,\n",
      "        0.07026235,  0.59762307, -0.44812656,  0.36027736, -0.19125189,\n",
      "       -0.12751281, -0.85938776,  1.0909225 ,  0.08644895, -0.78111565,\n",
      "        0.31273086, -0.75540923])\n",
      "Running repetition:  2\n",
      "Unfrozen params:  16  Frozen params:  80\n",
      "Iter:  1601  Loss on train set:  -102.30917611732285\n",
      "Iter:  1602  Loss on train set:  -102.40690684519407\n",
      "Iter:  1603  Loss on train set:  -102.30649646276581\n",
      "Iter:  1604  Loss on train set:  -102.17072254269982\n",
      "Iter:  1605  Loss on train set:  -101.99964305820481\n",
      "Iter:  1606  Loss on train set:  -101.90444227765761\n",
      "Iter:  1607  Loss on train set:  -101.82711034425684\n",
      "Iter:  1608  Loss on train set:  -101.61284607381523\n",
      "Iter:  1609  Loss on train set:  -101.30511163971575\n",
      "Iter:  1610  Loss on train set:  -101.39984675510996\n",
      "Iter:  1611  Loss on train set:  -101.4470378341496\n",
      "Iter:  1612  Loss on train set:  -101.46114894236763\n",
      "Iter:  1613  Loss on train set:  -101.54642534330338\n",
      "Iter:  1614  Loss on train set:  -101.58628095630084\n",
      "Iter:  1615  Loss on train set:  -101.58749899175268\n",
      "Iter:  1616  Loss on train set:  -101.61651873915494\n",
      "Iter:  1617  Loss on train set:  -101.59144024338758\n",
      "Iter:  1618  Loss on train set:  -101.59197189172994\n",
      "Iter:  1619  Loss on train set:  -101.5585574897516\n",
      "Iter:  1620  Loss on train set:  -101.52016874512668\n",
      "Iter:  1621  Loss on train set:  -101.42279987711163\n",
      "Iter:  1622  Loss on train set:  -101.32390504571501\n",
      "Iter:  1623  Loss on train set:  -101.20043590111695\n",
      "Iter:  1624  Loss on train set:  -101.06799658569733\n",
      "Iter:  1625  Loss on train set:  -100.93731746265033\n",
      "Iter:  1626  Loss on train set:  -100.80597959621424\n",
      "Iter:  1627  Loss on train set:  -100.67139859794935\n",
      "Iter:  1628  Loss on train set:  -100.54789055609368\n",
      "Iter:  1629  Loss on train set:  -100.4482633714465\n",
      "Iter:  1630  Loss on train set:  -100.34778123840319\n",
      "Iter:  1631  Loss on train set:  -100.36364700293004\n",
      "Iter:  1632  Loss on train set:  -100.46706252856879\n",
      "Iter:  1633  Loss on train set:  -100.53412065330508\n",
      "Iter:  1634  Loss on train set:  -100.5821949324889\n",
      "Iter:  1635  Loss on train set:  -100.68658222991432\n",
      "Iter:  1636  Loss on train set:  -100.74815061328286\n",
      "Iter:  1637  Loss on train set:  -100.79946533041864\n",
      "Iter:  1638  Loss on train set:  -100.83862344497771\n",
      "Iter:  1639  Loss on train set:  -100.94512659068907\n",
      "Iter:  1640  Loss on train set:  -101.03168552645754\n",
      "Iter:  1641  Loss on train set:  -101.09326885926022\n",
      "Iter:  1642  Loss on train set:  -101.2051473704983\n",
      "Iter:  1643  Loss on train set:  -101.26226732091939\n",
      "Iter:  1644  Loss on train set:  -101.38041078960437\n",
      "Iter:  1645  Loss on train set:  -101.53791009305807\n",
      "Iter:  1646  Loss on train set:  -101.66743498434558\n",
      "Iter:  1647  Loss on train set:  -101.75780206078645\n",
      "Iter:  1648  Loss on train set:  -101.77254103116859\n",
      "Iter:  1649  Loss on train set:  -101.75733015096098\n",
      "Iter:  1650  Loss on train set:  -101.68694580848556\n",
      "Iter:  1651  Loss on train set:  -101.64534512090886\n",
      "Iter:  1652  Loss on train set:  -101.58487259794138\n",
      "Iter:  1653  Loss on train set:  -101.57832162320352\n",
      "Iter:  1654  Loss on train set:  -101.60122470294924\n",
      "Iter:  1655  Loss on train set:  -101.58961589754571\n",
      "Iter:  1656  Loss on train set:  -101.75652902033059\n",
      "Iter:  1657  Loss on train set:  -101.94225856333291\n",
      "Iter:  1658  Loss on train set:  -102.07564774719815\n",
      "Iter:  1659  Loss on train set:  -102.1700645016976\n",
      "Iter:  1660  Loss on train set:  -102.22283405038591\n",
      "Iter:  1661  Loss on train set:  -102.25969465336061\n",
      "Iter:  1662  Loss on train set:  -102.28084143949329\n",
      "Iter:  1663  Loss on train set:  -102.28382460065353\n",
      "Iter:  1664  Loss on train set:  -102.29422851420432\n",
      "Iter:  1665  Loss on train set:  -102.29856735260313\n",
      "Iter:  1666  Loss on train set:  -102.29586396903622\n",
      "Iter:  1667  Loss on train set:  -102.28674876343555\n",
      "Iter:  1668  Loss on train set:  -102.27311789385166\n",
      "Iter:  1669  Loss on train set:  -102.25898628185467\n",
      "Iter:  1670  Loss on train set:  -102.2433818656983\n",
      "Iter:  1671  Loss on train set:  -102.22465234945784\n",
      "Iter:  1672  Loss on train set:  -102.2119777568342\n",
      "Iter:  1673  Loss on train set:  -102.2078468496308\n",
      "Iter:  1674  Loss on train set:  -102.2114969125626\n",
      "Iter:  1675  Loss on train set:  -102.20473841324429\n",
      "Iter:  1676  Loss on train set:  -102.19118474301487\n",
      "Iter:  1677  Loss on train set:  -102.21331513658433\n",
      "Iter:  1678  Loss on train set:  -102.22850963825405\n",
      "Iter:  1679  Loss on train set:  -102.25630327048493\n",
      "Iter:  1680  Loss on train set:  -102.30063662033271\n",
      "Iter:  1681  Loss on train set:  -102.34379172243892\n",
      "Iter:  1682  Loss on train set:  -102.3457560603867\n",
      "Iter:  1683  Loss on train set:  -102.3105163846742\n",
      "Iter:  1684  Loss on train set:  -102.2512220018738\n",
      "Iter:  1685  Loss on train set:  -102.18233774664365\n",
      "Iter:  1686  Loss on train set:  -102.15611199722065\n",
      "Iter:  1687  Loss on train set:  -102.11607237249187\n",
      "Iter:  1688  Loss on train set:  -102.06558653663998\n",
      "Iter:  1689  Loss on train set:  -102.05377995418449\n",
      "Iter:  1690  Loss on train set:  -102.08146834492618\n",
      "Iter:  1691  Loss on train set:  -102.09936880454448\n",
      "Iter:  1692  Loss on train set:  -102.10908345080429\n",
      "Iter:  1693  Loss on train set:  -102.10664723590662\n",
      "Iter:  1694  Loss on train set:  -102.1371125341459\n",
      "Iter:  1695  Loss on train set:  -102.13569346560622\n",
      "Iter:  1696  Loss on train set:  -102.11510304030725\n",
      "Iter:  1697  Loss on train set:  -102.08514278662373\n",
      "Iter:  1698  Loss on train set:  -102.06566161279095\n",
      "Iter:  1699  Loss on train set:  -102.03038944914208\n",
      "Iter:  1700  Loss on train set:  -102.0143177532915\n",
      "Iter:  1701  Loss on train set:  -101.98742524232057\n",
      "Iter:  1702  Loss on train set:  -101.94608406809637\n",
      "Iter:  1703  Loss on train set:  -101.91157493565485\n",
      "Iter:  1704  Loss on train set:  -101.88293446147699\n",
      "Iter:  1705  Loss on train set:  -101.85136841948675\n",
      "Iter:  1706  Loss on train set:  -101.8838003468823\n",
      "Iter:  1707  Loss on train set:  -102.02877158107799\n",
      "Iter:  1708  Loss on train set:  -102.16465982038325\n",
      "Iter:  1709  Loss on train set:  -102.24340095283436\n",
      "Iter:  1710  Loss on train set:  -102.26361264242256\n",
      "Iter:  1711  Loss on train set:  -102.3071400281743\n",
      "Iter:  1712  Loss on train set:  -102.29800987140979\n",
      "Iter:  1713  Loss on train set:  -102.25812634743392\n",
      "Iter:  1714  Loss on train set:  -102.20039600001343\n",
      "Iter:  1715  Loss on train set:  -102.13892782823167\n",
      "Iter:  1716  Loss on train set:  -102.09170527827393\n",
      "Iter:  1717  Loss on train set:  -102.03282368826582\n",
      "Iter:  1718  Loss on train set:  -102.04970784079116\n",
      "Iter:  1719  Loss on train set:  -102.12401872565353\n",
      "Iter:  1720  Loss on train set:  -102.18008425574475\n",
      "Iter:  1721  Loss on train set:  -102.21774282673255\n",
      "Iter:  1722  Loss on train set:  -102.24278521352204\n",
      "Iter:  1723  Loss on train set:  -102.25216283418155\n",
      "Iter:  1724  Loss on train set:  -102.24978096039277\n",
      "Iter:  1725  Loss on train set:  -102.25671935434029\n",
      "Iter:  1726  Loss on train set:  -102.26042984717789\n",
      "Iter:  1727  Loss on train set:  -102.29247749885006\n",
      "Iter:  1728  Loss on train set:  -102.30918443667495\n",
      "Iter:  1729  Loss on train set:  -102.34747225438477\n",
      "Iter:  1730  Loss on train set:  -102.35345013285809\n",
      "Iter:  1731  Loss on train set:  -102.33237404544761\n",
      "Iter:  1732  Loss on train set:  -102.29679011352565\n",
      "Iter:  1733  Loss on train set:  -102.26433744317437\n",
      "Iter:  1734  Loss on train set:  -102.24966401516602\n",
      "Iter:  1735  Loss on train set:  -102.23971136390436\n",
      "Iter:  1736  Loss on train set:  -102.29258494725327\n",
      "Iter:  1737  Loss on train set:  -102.33807504881902\n",
      "Iter:  1738  Loss on train set:  -102.36754768995722\n",
      "Iter:  1739  Loss on train set:  -102.39043867766593\n",
      "Iter:  1740  Loss on train set:  -102.42664835460295\n",
      "Iter:  1741  Loss on train set:  -102.44900573422521\n",
      "Iter:  1742  Loss on train set:  -102.46540273676015\n",
      "Iter:  1743  Loss on train set:  -102.47681685165071\n",
      "Iter:  1744  Loss on train set:  -102.48086169269729\n",
      "Iter:  1745  Loss on train set:  -102.47661426790772\n",
      "Iter:  1746  Loss on train set:  -102.51557312568903\n",
      "Iter:  1747  Loss on train set:  -102.54906918113008\n",
      "Iter:  1748  Loss on train set:  -102.57426588734678\n",
      "Iter:  1749  Loss on train set:  -102.5902912757037\n",
      "Iter:  1750  Loss on train set:  -102.61686335898035\n",
      "Iter:  1751  Loss on train set:  -102.63531994370327\n",
      "Iter:  1752  Loss on train set:  -102.65182744064998\n",
      "Iter:  1753  Loss on train set:  -102.67475774761931\n",
      "Iter:  1754  Loss on train set:  -102.6905673257476\n",
      "Iter:  1755  Loss on train set:  -102.6991079038937\n",
      "Iter:  1756  Loss on train set:  -102.70473060741573\n",
      "Iter:  1757  Loss on train set:  -102.70614929051449\n",
      "Iter:  1758  Loss on train set:  -102.70655239972248\n",
      "Iter:  1759  Loss on train set:  -102.7071373620951\n",
      "Iter:  1760  Loss on train set:  -102.71199106280702\n",
      "Iter:  1761  Loss on train set:  -102.71987868915438\n",
      "Iter:  1762  Loss on train set:  -102.72290213325508\n",
      "Iter:  1763  Loss on train set:  -102.72357413703989\n",
      "Iter:  1764  Loss on train set:  -102.72758207785404\n",
      "Iter:  1765  Loss on train set:  -102.73209851822264\n",
      "Iter:  1766  Loss on train set:  -102.75501767319862\n",
      "Iter:  1767  Loss on train set:  -102.76610907547234\n",
      "Iter:  1768  Loss on train set:  -102.76926930440064\n",
      "Iter:  1769  Loss on train set:  -102.76682172490088\n",
      "Iter:  1770  Loss on train set:  -102.75939531059659\n",
      "Iter:  1771  Loss on train set:  -102.74850262952297\n",
      "Iter:  1772  Loss on train set:  -102.73610154710146\n",
      "Iter:  1773  Loss on train set:  -102.72518904724342\n",
      "Iter:  1774  Loss on train set:  -102.71394966742234\n",
      "Iter:  1775  Loss on train set:  -102.70422113534099\n",
      "Iter:  1776  Loss on train set:  -102.71440783516069\n",
      "Iter:  1777  Loss on train set:  -102.7043643553736\n",
      "Iter:  1778  Loss on train set:  -102.67867663185719\n",
      "Iter:  1779  Loss on train set:  -102.6436580281832\n",
      "Iter:  1780  Loss on train set:  -102.6146078492183\n",
      "Iter:  1781  Loss on train set:  -102.57668559156514\n",
      "Iter:  1782  Loss on train set:  -102.5563345317411\n",
      "Iter:  1783  Loss on train set:  -102.64078024626632\n",
      "Iter:  1784  Loss on train set:  -102.69173154574548\n",
      "Iter:  1785  Loss on train set:  -102.71317177450607\n",
      "Iter:  1786  Loss on train set:  -102.71472372658289\n",
      "Iter:  1787  Loss on train set:  -102.70160040801358\n",
      "Iter:  1788  Loss on train set:  -102.67423779165411\n",
      "Iter:  1789  Loss on train set:  -102.64110873002504\n",
      "Iter:  1790  Loss on train set:  -102.61586370223154\n",
      "Iter:  1791  Loss on train set:  -102.59525088265927\n",
      "Iter:  1792  Loss on train set:  -102.57201560921851\n",
      "Iter:  1793  Loss on train set:  -102.58939830953827\n",
      "Iter:  1794  Loss on train set:  -102.62557490195762\n",
      "Iter:  1795  Loss on train set:  -102.686756146176\n",
      "Iter:  1796  Loss on train set:  -102.71025244534091\n",
      "Iter:  1797  Loss on train set:  -102.72266279546413\n",
      "Iter:  1798  Loss on train set:  -102.71068057869249\n",
      "Iter:  1799  Loss on train set:  -102.67642275000327\n",
      "Iter:  1800  Loss on train set:  -102.61965197241263\n",
      "     fun: -102.5522050115038\n",
      "     jac: array([-2.26667789,  2.26667789, -2.26667789, -2.26667789, -2.26667789,\n",
      "       -2.26667789, -2.26667789,  2.26667789,  2.26667789, -2.26667789,\n",
      "       -2.26667789, -2.26667789,  2.26667789,  2.26667789,  2.26667789,\n",
      "       -2.26667789])\n",
      "    nfev: 200\n",
      "     nit: 200\n",
      " success: True\n",
      "       x: array([-0.3844977 , -2.20552714, -0.87034505, -1.07479074,  0.58504493,\n",
      "       -1.26690023, -1.33451703, -0.51782176,  0.14831096, -1.88862494,\n",
      "       -0.4282058 , -1.76751145, -0.75403521, -0.8301811 ,  1.370875  ,\n",
      "        0.70619236])\n",
      "Unfrozen params:  16  Frozen params:  80\n",
      "Iter:  1801  Loss on train set:  -102.5522050115038\n",
      "Iter:  1802  Loss on train set:  -102.47279198516547\n",
      "Iter:  1803  Loss on train set:  -102.43168675692714\n",
      "Iter:  1804  Loss on train set:  -102.48022461321631\n",
      "Iter:  1805  Loss on train set:  -102.46722515652824\n",
      "Iter:  1806  Loss on train set:  -102.41594101714917\n",
      "Iter:  1807  Loss on train set:  -102.38061455824813\n",
      "Iter:  1808  Loss on train set:  -102.34008151172223\n",
      "Iter:  1809  Loss on train set:  -102.31556017158158\n",
      "Iter:  1810  Loss on train set:  -102.29707687099621\n",
      "Iter:  1811  Loss on train set:  -102.36647449718262\n",
      "Iter:  1812  Loss on train set:  -102.40399641956549\n",
      "Iter:  1813  Loss on train set:  -102.41856277708158\n",
      "Iter:  1814  Loss on train set:  -102.40229451987899\n",
      "Iter:  1815  Loss on train set:  -102.36892539830546\n",
      "Iter:  1816  Loss on train set:  -102.32199134019764\n",
      "Iter:  1817  Loss on train set:  -102.26719744114072\n",
      "Iter:  1818  Loss on train set:  -102.22110139872484\n",
      "Iter:  1819  Loss on train set:  -102.16926926481167\n",
      "Iter:  1820  Loss on train set:  -102.11490126764026\n",
      "Iter:  1821  Loss on train set:  -102.06075206912129\n",
      "Iter:  1822  Loss on train set:  -102.00557325430185\n",
      "Iter:  1823  Loss on train set:  -101.98518789062234\n",
      "Iter:  1824  Loss on train set:  -102.07077836208799\n",
      "Iter:  1825  Loss on train set:  -102.11708033785241\n",
      "Iter:  1826  Loss on train set:  -102.16320013723086\n",
      "Iter:  1827  Loss on train set:  -102.19178921798473\n",
      "Iter:  1828  Loss on train set:  -102.20421859829662\n",
      "Iter:  1829  Loss on train set:  -102.21652845174357\n",
      "Iter:  1830  Loss on train set:  -102.2189521385994\n",
      "Iter:  1831  Loss on train set:  -102.23443249889979\n",
      "Iter:  1832  Loss on train set:  -102.24095526997635\n",
      "Iter:  1833  Loss on train set:  -102.23238654746866\n",
      "Iter:  1834  Loss on train set:  -102.20905707108734\n",
      "Iter:  1835  Loss on train set:  -102.19004251230919\n",
      "Iter:  1836  Loss on train set:  -102.16375470107518\n",
      "Iter:  1837  Loss on train set:  -102.131953154922\n",
      "Iter:  1838  Loss on train set:  -102.13806801225768\n",
      "Iter:  1839  Loss on train set:  -102.17842921927904\n",
      "Iter:  1840  Loss on train set:  -102.21559052075432\n",
      "Iter:  1841  Loss on train set:  -102.23992967251137\n",
      "Iter:  1842  Loss on train set:  -102.24838282582283\n",
      "Iter:  1843  Loss on train set:  -102.25255475235363\n",
      "Iter:  1844  Loss on train set:  -102.24535798574875\n",
      "Iter:  1845  Loss on train set:  -102.23065494933542\n",
      "Iter:  1846  Loss on train set:  -102.20646779144472\n",
      "Iter:  1847  Loss on train set:  -102.24447899586929\n",
      "Iter:  1848  Loss on train set:  -102.31720760989742\n",
      "Iter:  1849  Loss on train set:  -102.34912689876535\n",
      "Iter:  1850  Loss on train set:  -102.34250327620614\n",
      "Iter:  1851  Loss on train set:  -102.31564224513454\n",
      "Iter:  1852  Loss on train set:  -102.2856294193513\n",
      "Iter:  1853  Loss on train set:  -102.27756796022851\n",
      "Iter:  1854  Loss on train set:  -102.2765696302287\n",
      "Iter:  1855  Loss on train set:  -102.25485070858825\n",
      "Iter:  1856  Loss on train set:  -102.22325302078318\n",
      "Iter:  1857  Loss on train set:  -102.18728821551717\n",
      "Iter:  1858  Loss on train set:  -102.20278425418583\n",
      "Iter:  1859  Loss on train set:  -102.19565552127993\n",
      "Iter:  1860  Loss on train set:  -102.18417082624892\n",
      "Iter:  1861  Loss on train set:  -102.17865301786216\n",
      "Iter:  1862  Loss on train set:  -102.19540152964004\n",
      "Iter:  1863  Loss on train set:  -102.19576286505004\n",
      "Iter:  1864  Loss on train set:  -102.20209587986486\n",
      "Iter:  1865  Loss on train set:  -102.20825705511885\n",
      "Iter:  1866  Loss on train set:  -102.17985130233313\n",
      "Iter:  1867  Loss on train set:  -102.1255942390161\n",
      "Iter:  1868  Loss on train set:  -102.05224766799603\n",
      "Iter:  1869  Loss on train set:  -101.99479523471507\n",
      "Iter:  1870  Loss on train set:  -101.92579357946569\n",
      "Iter:  1871  Loss on train set:  -101.8550601206159\n",
      "Iter:  1872  Loss on train set:  -101.78041022832215\n",
      "Iter:  1873  Loss on train set:  -101.8158513280896\n",
      "Iter:  1874  Loss on train set:  -101.84024007242303\n",
      "Iter:  1875  Loss on train set:  -101.87661032310939\n",
      "Iter:  1876  Loss on train set:  -101.89803678064771\n",
      "Iter:  1877  Loss on train set:  -101.90764096058723\n",
      "Iter:  1878  Loss on train set:  -101.91188854240829\n",
      "Iter:  1879  Loss on train set:  -101.9443119440526\n",
      "Iter:  1880  Loss on train set:  -101.95718247815216\n",
      "Iter:  1881  Loss on train set:  -101.95469220670705\n",
      "Iter:  1882  Loss on train set:  -101.94079934016094\n",
      "Iter:  1883  Loss on train set:  -101.92465480949771\n",
      "Iter:  1884  Loss on train set:  -101.90759714406403\n",
      "Iter:  1885  Loss on train set:  -101.89199826583972\n",
      "Iter:  1886  Loss on train set:  -101.86985923988107\n",
      "Iter:  1887  Loss on train set:  -101.85111169526515\n",
      "Iter:  1888  Loss on train set:  -101.86590447448857\n",
      "Iter:  1889  Loss on train set:  -101.89309114493962\n",
      "Iter:  1890  Loss on train set:  -101.91676389870402\n",
      "Iter:  1891  Loss on train set:  -101.94061866612039\n",
      "Iter:  1892  Loss on train set:  -101.96329085979653\n",
      "Iter:  1893  Loss on train set:  -101.98023039238166\n",
      "Iter:  1894  Loss on train set:  -101.99186725819466\n",
      "Iter:  1895  Loss on train set:  -102.01715512993643\n",
      "Iter:  1896  Loss on train set:  -102.051442254926\n",
      "Iter:  1897  Loss on train set:  -102.10965517005961\n",
      "Iter:  1898  Loss on train set:  -102.15405121633766\n",
      "Iter:  1899  Loss on train set:  -102.19907503248346\n",
      "Iter:  1900  Loss on train set:  -102.23036747868566\n",
      "Iter:  1901  Loss on train set:  -102.24719769892414\n",
      "Iter:  1902  Loss on train set:  -102.27703397795192\n",
      "Iter:  1903  Loss on train set:  -102.29861320496961\n",
      "Iter:  1904  Loss on train set:  -102.31445721391694\n",
      "Iter:  1905  Loss on train set:  -102.32977656099598\n",
      "Iter:  1906  Loss on train set:  -102.34155963639719\n",
      "Iter:  1907  Loss on train set:  -102.3495172940629\n",
      "Iter:  1908  Loss on train set:  -102.35819546015672\n",
      "Iter:  1909  Loss on train set:  -102.36494682081673\n",
      "Iter:  1910  Loss on train set:  -102.37125256596423\n",
      "Iter:  1911  Loss on train set:  -102.37749963391356\n",
      "Iter:  1912  Loss on train set:  -102.38295229118981\n",
      "Iter:  1913  Loss on train set:  -102.39955081396302\n",
      "Iter:  1914  Loss on train set:  -102.43260064580073\n",
      "Iter:  1915  Loss on train set:  -102.45606343882088\n",
      "Iter:  1916  Loss on train set:  -102.47358025217542\n",
      "Iter:  1917  Loss on train set:  -102.48863700655578\n",
      "Iter:  1918  Loss on train set:  -102.49871173930984\n",
      "Iter:  1919  Loss on train set:  -102.51488876673199\n",
      "Iter:  1920  Loss on train set:  -102.52368207333235\n",
      "Iter:  1921  Loss on train set:  -102.5314220177056\n",
      "Iter:  1922  Loss on train set:  -102.53500286739532\n",
      "Iter:  1923  Loss on train set:  -102.5418293688356\n",
      "Iter:  1924  Loss on train set:  -102.55694979825338\n",
      "Iter:  1925  Loss on train set:  -102.57443088528801\n",
      "Iter:  1926  Loss on train set:  -102.5875804692578\n",
      "Iter:  1927  Loss on train set:  -102.5986660924201\n",
      "Iter:  1928  Loss on train set:  -102.60887705491342\n",
      "Iter:  1929  Loss on train set:  -102.62228743552184\n",
      "Iter:  1930  Loss on train set:  -102.63194644530145\n",
      "Iter:  1931  Loss on train set:  -102.63553007318369\n",
      "Iter:  1932  Loss on train set:  -102.63402147033005\n",
      "Iter:  1933  Loss on train set:  -102.62965607583955\n",
      "Iter:  1934  Loss on train set:  -102.62233718961782\n",
      "Iter:  1935  Loss on train set:  -102.61728335617566\n",
      "Iter:  1936  Loss on train set:  -102.62173157414489\n",
      "Iter:  1937  Loss on train set:  -102.62818425503845\n",
      "Iter:  1938  Loss on train set:  -102.63422541277077\n",
      "Iter:  1939  Loss on train set:  -102.63850381785817\n",
      "Iter:  1940  Loss on train set:  -102.65220248129836\n",
      "Iter:  1941  Loss on train set:  -102.66151288490175\n",
      "Iter:  1942  Loss on train set:  -102.66584143560533\n",
      "Iter:  1943  Loss on train set:  -102.66707773891861\n",
      "Iter:  1944  Loss on train set:  -102.66596104168008\n",
      "Iter:  1945  Loss on train set:  -102.66870087086073\n",
      "Iter:  1946  Loss on train set:  -102.6668232805103\n",
      "Iter:  1947  Loss on train set:  -102.66322029746293\n",
      "Iter:  1948  Loss on train set:  -102.66060198531359\n",
      "Iter:  1949  Loss on train set:  -102.65520705942257\n",
      "Iter:  1950  Loss on train set:  -102.64739476776127\n",
      "Iter:  1951  Loss on train set:  -102.64132487938004\n",
      "Iter:  1952  Loss on train set:  -102.65197326905869\n",
      "Iter:  1953  Loss on train set:  -102.66237987501381\n",
      "Iter:  1954  Loss on train set:  -102.6711498274655\n",
      "Iter:  1955  Loss on train set:  -102.67841459614188\n",
      "Iter:  1956  Loss on train set:  -102.68806346721914\n",
      "Iter:  1957  Loss on train set:  -102.69833067644848\n",
      "Iter:  1958  Loss on train set:  -102.70620387249306\n",
      "Iter:  1959  Loss on train set:  -102.71197153342584\n",
      "Iter:  1960  Loss on train set:  -102.71561774232126\n",
      "Iter:  1961  Loss on train set:  -102.71941495684497\n",
      "Iter:  1962  Loss on train set:  -102.73697529984184\n",
      "Iter:  1963  Loss on train set:  -102.74877696957027\n",
      "Iter:  1964  Loss on train set:  -102.75638861214945\n",
      "Iter:  1965  Loss on train set:  -102.76203251361524\n",
      "Iter:  1966  Loss on train set:  -102.76370881343735\n",
      "Iter:  1967  Loss on train set:  -102.76285203346976\n",
      "Iter:  1968  Loss on train set:  -102.759645138579\n",
      "Iter:  1969  Loss on train set:  -102.75495504049019\n",
      "Iter:  1970  Loss on train set:  -102.74938421849801\n",
      "Iter:  1971  Loss on train set:  -102.74521794716972\n",
      "Iter:  1972  Loss on train set:  -102.74386027593556\n",
      "Iter:  1973  Loss on train set:  -102.74146634254579\n",
      "Iter:  1974  Loss on train set:  -102.73816951632412\n",
      "Iter:  1975  Loss on train set:  -102.74027032904519\n",
      "Iter:  1976  Loss on train set:  -102.74379434339714\n",
      "Iter:  1977  Loss on train set:  -102.74471098933107\n",
      "Iter:  1978  Loss on train set:  -102.74584319240233\n",
      "Iter:  1979  Loss on train set:  -102.74300238130138\n",
      "Iter:  1980  Loss on train set:  -102.73740380632543\n",
      "Iter:  1981  Loss on train set:  -102.7301869227463\n",
      "Iter:  1982  Loss on train set:  -102.72275291289564\n",
      "Iter:  1983  Loss on train set:  -102.71519472025876\n",
      "Iter:  1984  Loss on train set:  -102.7080419793843\n",
      "Iter:  1985  Loss on train set:  -102.70193167256858\n",
      "Iter:  1986  Loss on train set:  -102.70533904380913\n",
      "Iter:  1987  Loss on train set:  -102.70941735170462\n",
      "Iter:  1988  Loss on train set:  -102.71201928871301\n",
      "Iter:  1989  Loss on train set:  -102.71382942900321\n",
      "Iter:  1990  Loss on train set:  -102.71838675469533\n",
      "Iter:  1991  Loss on train set:  -102.72146808419714\n",
      "Iter:  1992  Loss on train set:  -102.7243854836022\n",
      "Iter:  1993  Loss on train set:  -102.723748343274\n",
      "Iter:  1994  Loss on train set:  -102.72473900609397\n",
      "Iter:  1995  Loss on train set:  -102.72294658808264\n",
      "Iter:  1996  Loss on train set:  -102.73304077973876\n",
      "Iter:  1997  Loss on train set:  -102.74168475727762\n",
      "Iter:  1998  Loss on train set:  -102.74364117297128\n",
      "Iter:  1999  Loss on train set:  -102.74123428479062\n",
      "Iter:  2000  Loss on train set:  -102.74617394820815\n",
      "     fun: -102.74867615477939\n",
      "     jac: array([-1.07385081,  1.07385081,  1.07385081, -1.07385081,  1.07385081,\n",
      "       -1.07385081,  1.07385081, -1.07385081, -1.07385081,  1.07385081,\n",
      "       -1.07385081,  1.07385081,  1.07385081,  1.07385081,  1.07385081,\n",
      "        1.07385081])\n",
      "    nfev: 200\n",
      "     nit: 200\n",
      " success: True\n",
      "       x: array([-0.1505405 ,  0.17288337, -0.04357547, -0.05192296,  0.06545901,\n",
      "       -0.03957284, -2.49648315, -0.03133729,  1.94825532,  0.48294913,\n",
      "        0.89016138, -0.03229993,  0.01273231, -0.12457967,  0.0407551 ,\n",
      "        0.0803304 ])\n",
      "Unfrozen params:  32  Frozen params:  64\n",
      "Iter:  2001  Loss on train set:  -102.74867615477939\n",
      "Iter:  2002  Loss on train set:  -102.94072318059287\n",
      "Iter:  2003  Loss on train set:  -102.80808690325907\n",
      "Iter:  2004  Loss on train set:  -102.68270314390877\n",
      "Iter:  2005  Loss on train set:  -102.52124681270143\n",
      "Iter:  2006  Loss on train set:  -102.34578340508831\n",
      "Iter:  2007  Loss on train set:  -102.30004510939261\n",
      "Iter:  2008  Loss on train set:  -102.24090688870064\n",
      "Iter:  2009  Loss on train set:  -102.21776101043744\n",
      "Iter:  2010  Loss on train set:  -102.19049827006826\n",
      "Iter:  2011  Loss on train set:  -102.15699222557276\n",
      "Iter:  2012  Loss on train set:  -102.12090930709802\n",
      "Iter:  2013  Loss on train set:  -102.10880144038977\n",
      "Iter:  2014  Loss on train set:  -102.1498424366275\n",
      "Iter:  2015  Loss on train set:  -102.22896108810941\n",
      "Iter:  2016  Loss on train set:  -102.26381859859778\n",
      "Iter:  2017  Loss on train set:  -102.26906538441634\n",
      "Iter:  2018  Loss on train set:  -102.25052790564384\n",
      "Iter:  2019  Loss on train set:  -102.26443248390909\n",
      "Iter:  2020  Loss on train set:  -102.26040184709257\n",
      "Iter:  2021  Loss on train set:  -102.29292603465436\n",
      "Iter:  2022  Loss on train set:  -102.27811556168018\n",
      "Iter:  2023  Loss on train set:  -102.23125974242987\n",
      "Iter:  2024  Loss on train set:  -102.19367599922445\n",
      "Iter:  2025  Loss on train set:  -102.11753344153259\n",
      "Iter:  2026  Loss on train set:  -102.01182050370804\n",
      "Iter:  2027  Loss on train set:  -101.89230876841341\n",
      "Iter:  2028  Loss on train set:  -101.77686823063723\n",
      "Iter:  2029  Loss on train set:  -101.66598227912154\n",
      "Iter:  2030  Loss on train set:  -101.5458235179652\n",
      "Iter:  2031  Loss on train set:  -101.51011934376831\n",
      "Iter:  2032  Loss on train set:  -101.43470845046465\n",
      "Iter:  2033  Loss on train set:  -101.40624751961204\n",
      "Iter:  2034  Loss on train set:  -101.47617608677793\n",
      "Iter:  2035  Loss on train set:  -101.46497195377032\n",
      "Iter:  2036  Loss on train set:  -101.38626342804017\n",
      "Iter:  2037  Loss on train set:  -101.37872849101417\n",
      "Iter:  2038  Loss on train set:  -101.38962639626244\n",
      "Iter:  2039  Loss on train set:  -101.35100329202038\n",
      "Iter:  2040  Loss on train set:  -101.62430411523877\n",
      "Iter:  2041  Loss on train set:  -101.81438734514744\n",
      "Iter:  2042  Loss on train set:  -102.01873955874203\n",
      "Iter:  2043  Loss on train set:  -102.15152498775169\n",
      "Iter:  2044  Loss on train set:  -102.32625400761961\n",
      "Iter:  2045  Loss on train set:  -102.42144532423376\n",
      "Iter:  2046  Loss on train set:  -102.4666448536549\n",
      "Iter:  2047  Loss on train set:  -102.46965151788083\n",
      "Iter:  2048  Loss on train set:  -102.43543179528784\n",
      "Iter:  2049  Loss on train set:  -102.37555046616892\n",
      "Iter:  2050  Loss on train set:  -102.29228154621643\n",
      "Iter:  2051  Loss on train set:  -102.20121400450421\n",
      "Iter:  2052  Loss on train set:  -102.09579455345501\n",
      "Iter:  2053  Loss on train set:  -101.98758687254669\n",
      "Iter:  2054  Loss on train set:  -101.87481914785613\n",
      "Iter:  2055  Loss on train set:  -101.7884029060323\n",
      "Iter:  2056  Loss on train set:  -101.72347220901983\n",
      "Iter:  2057  Loss on train set:  -101.70413908290365\n",
      "Iter:  2058  Loss on train set:  -101.69748456084866\n",
      "Iter:  2059  Loss on train set:  -101.72531961934669\n",
      "Iter:  2060  Loss on train set:  -101.73303721465969\n",
      "Iter:  2061  Loss on train set:  -101.79660663135179\n",
      "Iter:  2062  Loss on train set:  -101.8114869147015\n",
      "Iter:  2063  Loss on train set:  -101.76974286576998\n",
      "Iter:  2064  Loss on train set:  -101.7341410465985\n",
      "Iter:  2065  Loss on train set:  -101.68193604098376\n",
      "Iter:  2066  Loss on train set:  -101.61883576211925\n",
      "Iter:  2067  Loss on train set:  -101.54397083003562\n",
      "Iter:  2068  Loss on train set:  -101.46696625908578\n",
      "Iter:  2069  Loss on train set:  -101.38532241869056\n",
      "Iter:  2070  Loss on train set:  -101.31190796464217\n",
      "Iter:  2071  Loss on train set:  -101.2353132385613\n",
      "Iter:  2072  Loss on train set:  -101.23767167522104\n",
      "Iter:  2073  Loss on train set:  -101.22701971577564\n",
      "Iter:  2074  Loss on train set:  -101.22705107106695\n",
      "Iter:  2075  Loss on train set:  -101.24715675328223\n",
      "Iter:  2076  Loss on train set:  -101.25388762170064\n",
      "Iter:  2077  Loss on train set:  -101.32591397854078\n",
      "Iter:  2078  Loss on train set:  -101.38372531166966\n",
      "Iter:  2079  Loss on train set:  -101.50096696892909\n",
      "Iter:  2080  Loss on train set:  -101.60198084112889\n",
      "Iter:  2081  Loss on train set:  -101.79211537480057\n",
      "Iter:  2082  Loss on train set:  -101.94896362941904\n",
      "Iter:  2083  Loss on train set:  -102.06412382782871\n",
      "Iter:  2084  Loss on train set:  -102.14694335941589\n",
      "Iter:  2085  Loss on train set:  -102.23072970960611\n",
      "Iter:  2086  Loss on train set:  -102.27859617036728\n",
      "Iter:  2087  Loss on train set:  -102.33446464842538\n",
      "Iter:  2088  Loss on train set:  -102.38302832683031\n",
      "Iter:  2089  Loss on train set:  -102.40324735517774\n",
      "Iter:  2090  Loss on train set:  -102.40155048313905\n",
      "Iter:  2091  Loss on train set:  -102.37916443887761\n",
      "Iter:  2092  Loss on train set:  -102.34121947107548\n",
      "Iter:  2093  Loss on train set:  -102.293391690532\n",
      "Iter:  2094  Loss on train set:  -102.23810512130143\n",
      "Iter:  2095  Loss on train set:  -102.22693690081687\n",
      "Iter:  2096  Loss on train set:  -102.23610581845166\n",
      "Iter:  2097  Loss on train set:  -102.2510785234658\n",
      "Iter:  2098  Loss on train set:  -102.27522873454504\n",
      "Iter:  2099  Loss on train set:  -102.27365875823784\n",
      "Iter:  2100  Loss on train set:  -102.26063129001658\n",
      "Iter:  2101  Loss on train set:  -102.29334180293203\n",
      "Iter:  2102  Loss on train set:  -102.32885924222093\n",
      "Iter:  2103  Loss on train set:  -102.38772304482622\n",
      "Iter:  2104  Loss on train set:  -102.39684100917493\n",
      "Iter:  2105  Loss on train set:  -102.45171554597428\n",
      "Iter:  2106  Loss on train set:  -102.43975532277317\n",
      "Iter:  2107  Loss on train set:  -102.39404857052028\n",
      "Iter:  2108  Loss on train set:  -102.37412518518859\n",
      "Iter:  2109  Loss on train set:  -102.33210592180706\n",
      "Iter:  2110  Loss on train set:  -102.37957774205566\n",
      "Iter:  2111  Loss on train set:  -102.51694280417614\n",
      "Iter:  2112  Loss on train set:  -102.58412620142748\n",
      "Iter:  2113  Loss on train set:  -102.64106760698553\n",
      "Iter:  2114  Loss on train set:  -102.61413147957396\n",
      "Iter:  2115  Loss on train set:  -102.52910153373779\n",
      "Iter:  2116  Loss on train set:  -102.40736316834824\n",
      "Iter:  2117  Loss on train set:  -102.26920074175385\n",
      "Iter:  2118  Loss on train set:  -102.13614883863511\n",
      "Iter:  2119  Loss on train set:  -102.17299373476732\n",
      "Iter:  2120  Loss on train set:  -102.20314797635935\n",
      "Iter:  2121  Loss on train set:  -102.23261011533927\n",
      "Iter:  2122  Loss on train set:  -102.2514369167793\n",
      "Iter:  2123  Loss on train set:  -102.25686465715758\n",
      "Iter:  2124  Loss on train set:  -102.2446864137182\n",
      "Iter:  2125  Loss on train set:  -102.21515036454254\n",
      "Iter:  2126  Loss on train set:  -102.23279279461524\n",
      "Iter:  2127  Loss on train set:  -102.25500355348944\n",
      "Iter:  2128  Loss on train set:  -102.33614297323713\n",
      "Iter:  2129  Loss on train set:  -102.45889143067555\n",
      "Iter:  2130  Loss on train set:  -102.6070720937385\n",
      "Iter:  2131  Loss on train set:  -102.6998275088015\n",
      "Iter:  2132  Loss on train set:  -102.74728565487152\n",
      "Iter:  2133  Loss on train set:  -102.7522524329777\n",
      "Iter:  2134  Loss on train set:  -102.73135560854422\n",
      "Iter:  2135  Loss on train set:  -102.68566508493409\n",
      "Iter:  2136  Loss on train set:  -102.62451539703166\n",
      "Iter:  2137  Loss on train set:  -102.62110144050718\n",
      "Iter:  2138  Loss on train set:  -102.62883389615499\n",
      "Iter:  2139  Loss on train set:  -102.64082144818937\n",
      "Iter:  2140  Loss on train set:  -102.66630219544268\n",
      "Iter:  2141  Loss on train set:  -102.67983436135229\n",
      "Iter:  2142  Loss on train set:  -102.69581295903969\n",
      "Iter:  2143  Loss on train set:  -102.7236116037002\n",
      "Iter:  2144  Loss on train set:  -102.75281859861462\n",
      "Iter:  2145  Loss on train set:  -102.77437002973895\n",
      "Iter:  2146  Loss on train set:  -102.7833729646386\n",
      "Iter:  2147  Loss on train set:  -102.78304484323375\n",
      "Iter:  2148  Loss on train set:  -102.79962608542161\n",
      "Iter:  2149  Loss on train set:  -102.81350094696163\n",
      "Iter:  2150  Loss on train set:  -102.83656164320503\n",
      "Iter:  2151  Loss on train set:  -102.84105133144926\n",
      "Iter:  2152  Loss on train set:  -102.83182929342472\n",
      "Iter:  2153  Loss on train set:  -102.81743911715652\n",
      "Iter:  2154  Loss on train set:  -102.80020897694561\n",
      "Iter:  2155  Loss on train set:  -102.78759143196058\n",
      "Iter:  2156  Loss on train set:  -102.80493689361046\n",
      "Iter:  2157  Loss on train set:  -102.8020213999026\n",
      "Iter:  2158  Loss on train set:  -102.78627559185321\n",
      "Iter:  2159  Loss on train set:  -102.75677553559197\n",
      "Iter:  2160  Loss on train set:  -102.7177788251271\n",
      "Iter:  2161  Loss on train set:  -102.67735178401044\n",
      "Iter:  2162  Loss on train set:  -102.62767833105917\n",
      "Iter:  2163  Loss on train set:  -102.58199168433363\n",
      "Iter:  2164  Loss on train set:  -102.52938536546382\n",
      "Iter:  2165  Loss on train set:  -102.49664340314703\n",
      "Iter:  2166  Loss on train set:  -102.48541863741731\n",
      "Iter:  2167  Loss on train set:  -102.50187323238808\n",
      "Iter:  2168  Loss on train set:  -102.50598150091766\n",
      "Iter:  2169  Loss on train set:  -102.52020553066927\n",
      "Iter:  2170  Loss on train set:  -102.51538692251422\n",
      "Iter:  2171  Loss on train set:  -102.50286491330927\n",
      "Iter:  2172  Loss on train set:  -102.4630480279221\n",
      "Iter:  2173  Loss on train set:  -102.459479436328\n",
      "Iter:  2174  Loss on train set:  -102.44303996187352\n",
      "Iter:  2175  Loss on train set:  -102.40976073875034\n",
      "Iter:  2176  Loss on train set:  -102.36483981369268\n",
      "Iter:  2177  Loss on train set:  -102.36168347696314\n",
      "Iter:  2178  Loss on train set:  -102.32423817324266\n",
      "Iter:  2179  Loss on train set:  -102.29132532474149\n",
      "Iter:  2180  Loss on train set:  -102.24488011536269\n",
      "Iter:  2181  Loss on train set:  -102.19772004979336\n",
      "Iter:  2182  Loss on train set:  -102.13555467073387\n",
      "Iter:  2183  Loss on train set:  -102.06240952447578\n",
      "Iter:  2184  Loss on train set:  -102.03119429214506\n",
      "Iter:  2185  Loss on train set:  -102.06090916220687\n",
      "Iter:  2186  Loss on train set:  -102.05760133441092\n",
      "Iter:  2187  Loss on train set:  -102.03206711014649\n",
      "Iter:  2188  Loss on train set:  -102.01594693539903\n",
      "Iter:  2189  Loss on train set:  -101.98578873470404\n",
      "Iter:  2190  Loss on train set:  -101.94355773469609\n",
      "Iter:  2191  Loss on train set:  -101.89443835995154\n",
      "Iter:  2192  Loss on train set:  -101.84011502060278\n",
      "Iter:  2193  Loss on train set:  -101.81028314329522\n",
      "Iter:  2194  Loss on train set:  -101.80976123223692\n",
      "Iter:  2195  Loss on train set:  -101.78973556141482\n",
      "Iter:  2196  Loss on train set:  -101.76711958678824\n",
      "Iter:  2197  Loss on train set:  -101.78384917154064\n",
      "Iter:  2198  Loss on train set:  -101.80297993855875\n",
      "Iter:  2199  Loss on train set:  -101.85302013245034\n",
      "Iter:  2200  Loss on train set:  -101.84151747267012\n",
      "     fun: -101.78472436786124\n",
      "     jac: array([-0.76767618,  0.76767618, -0.76767618,  0.76767618,  0.76767618,\n",
      "        0.76767618,  0.76767618,  0.76767618,  0.76767618, -0.76767618,\n",
      "       -0.76767618, -0.76767618, -0.76767618,  0.76767618, -0.76767618,\n",
      "       -0.76767618,  0.76767618, -0.76767618,  0.76767618,  0.76767618,\n",
      "       -0.76767618,  0.76767618,  0.76767618, -0.76767618, -0.76767618,\n",
      "        0.76767618, -0.76767618, -0.76767618,  0.76767618, -0.76767618,\n",
      "        0.76767618,  0.76767618])\n",
      "    nfev: 200\n",
      "     nit: 200\n",
      " success: True\n",
      "       x: array([-0.76355364,  0.25534741, -0.31039295, -0.38960158, -0.26001479,\n",
      "        0.30986864,  0.15283885, -0.26024707,  1.16085927,  0.67304373,\n",
      "       -1.67189075,  0.06965924,  0.46003   , -0.14072924,  0.43630515,\n",
      "       -0.24618704, -0.0291819 ,  0.03372821,  0.08981091,  0.18660227,\n",
      "       -0.02659012, -0.12293141,  0.39411078, -0.50345591, -0.17901058,\n",
      "        0.26170454,  1.36499092, -1.63184581,  0.0772492 , -0.39094983,\n",
      "        0.36557288, -0.39691886])\n",
      "Unfrozen params:  32  Frozen params:  64\n",
      "Iter:  2201  Loss on train set:  -101.78472436786124\n",
      "Iter:  2202  Loss on train set:  -101.92107056255941\n",
      "Iter:  2203  Loss on train set:  -101.86597010558552\n",
      "Iter:  2204  Loss on train set:  -101.7298548065589\n",
      "Iter:  2205  Loss on train set:  -101.58774701019543\n",
      "Iter:  2206  Loss on train set:  -101.41636797157773\n",
      "Iter:  2207  Loss on train set:  -101.23671627028543\n",
      "Iter:  2208  Loss on train set:  -101.34653247341352\n",
      "Iter:  2209  Loss on train set:  -101.42459303091486\n",
      "Iter:  2210  Loss on train set:  -101.4657131923712\n",
      "Iter:  2211  Loss on train set:  -101.46602626649349\n",
      "Iter:  2212  Loss on train set:  -101.40180394163578\n",
      "Iter:  2213  Loss on train set:  -101.27825552655354\n",
      "Iter:  2214  Loss on train set:  -101.12883482330076\n",
      "Iter:  2215  Loss on train set:  -101.07491873760712\n",
      "Iter:  2216  Loss on train set:  -101.03984134467375\n",
      "Iter:  2217  Loss on train set:  -100.9503057149658\n",
      "Iter:  2218  Loss on train set:  -100.98747565625126\n",
      "Iter:  2219  Loss on train set:  -100.99822152172547\n",
      "Iter:  2220  Loss on train set:  -100.95042437028434\n",
      "Iter:  2221  Loss on train set:  -100.85808274202103\n",
      "Iter:  2222  Loss on train set:  -100.75991008818148\n",
      "Iter:  2223  Loss on train set:  -100.7729670227368\n",
      "Iter:  2224  Loss on train set:  -100.76862704795153\n",
      "Iter:  2225  Loss on train set:  -100.73427583461546\n",
      "Iter:  2226  Loss on train set:  -100.74442119017344\n",
      "Iter:  2227  Loss on train set:  -100.8007082761088\n",
      "Iter:  2228  Loss on train set:  -100.94867870350893\n",
      "Iter:  2229  Loss on train set:  -101.01917949685253\n",
      "Iter:  2230  Loss on train set:  -101.10874949913227\n",
      "Iter:  2231  Loss on train set:  -101.12911839187885\n",
      "Iter:  2232  Loss on train set:  -101.12228232947909\n",
      "Iter:  2233  Loss on train set:  -101.12728318768951\n",
      "Iter:  2234  Loss on train set:  -101.0673519651376\n",
      "Iter:  2235  Loss on train set:  -101.01888827742216\n",
      "Iter:  2236  Loss on train set:  -100.97950959270463\n",
      "Iter:  2237  Loss on train set:  -100.93658946117682\n",
      "Iter:  2238  Loss on train set:  -100.88652746464899\n",
      "Iter:  2239  Loss on train set:  -100.81681134658912\n",
      "Iter:  2240  Loss on train set:  -100.73254878126482\n",
      "Iter:  2241  Loss on train set:  -100.63993953737453\n",
      "Iter:  2242  Loss on train set:  -100.54643564972334\n",
      "Iter:  2243  Loss on train set:  -100.44749955621445\n",
      "Iter:  2244  Loss on train set:  -100.35840272232338\n",
      "Iter:  2245  Loss on train set:  -100.28154853242681\n",
      "Iter:  2246  Loss on train set:  -100.28422358788957\n",
      "Iter:  2247  Loss on train set:  -100.27908702653087\n",
      "Iter:  2248  Loss on train set:  -100.30102414410499\n",
      "Iter:  2249  Loss on train set:  -100.37822852253733\n",
      "Iter:  2250  Loss on train set:  -100.42801694709968\n",
      "Iter:  2251  Loss on train set:  -100.46833416982095\n",
      "Iter:  2252  Loss on train set:  -100.49922997419634\n",
      "Iter:  2253  Loss on train set:  -100.50987268342655\n",
      "Iter:  2254  Loss on train set:  -100.50520303704954\n",
      "Iter:  2255  Loss on train set:  -100.57680703880293\n",
      "Iter:  2256  Loss on train set:  -100.64427486769846\n",
      "Iter:  2257  Loss on train set:  -100.68541059805578\n",
      "Iter:  2258  Loss on train set:  -100.71520428846829\n",
      "Iter:  2259  Loss on train set:  -100.73291697672387\n",
      "Iter:  2260  Loss on train set:  -100.77006731284153\n",
      "Iter:  2261  Loss on train set:  -100.8645494683608\n",
      "Iter:  2262  Loss on train set:  -100.93943728393485\n",
      "Iter:  2263  Loss on train set:  -101.0118653431735\n",
      "Iter:  2264  Loss on train set:  -101.0806478784305\n",
      "Iter:  2265  Loss on train set:  -101.141246484518\n",
      "Iter:  2266  Loss on train set:  -101.18330871038657\n",
      "Iter:  2267  Loss on train set:  -101.2121510178358\n",
      "Iter:  2268  Loss on train set:  -101.22966678334792\n",
      "Iter:  2269  Loss on train set:  -101.2387700024751\n",
      "Iter:  2270  Loss on train set:  -101.32180454309916\n",
      "Iter:  2271  Loss on train set:  -101.39345518875398\n",
      "Iter:  2272  Loss on train set:  -101.44336278287761\n",
      "Iter:  2273  Loss on train set:  -101.47619964763226\n",
      "Iter:  2274  Loss on train set:  -101.50335977929598\n",
      "Iter:  2275  Loss on train set:  -101.53191491316767\n",
      "Iter:  2276  Loss on train set:  -101.5474315751922\n",
      "Iter:  2277  Loss on train set:  -101.55165698289224\n",
      "Iter:  2278  Loss on train set:  -101.54810379437254\n",
      "Iter:  2279  Loss on train set:  -101.53715334737652\n",
      "Iter:  2280  Loss on train set:  -101.51806219779239\n",
      "Iter:  2281  Loss on train set:  -101.49591976313884\n",
      "Iter:  2282  Loss on train set:  -101.46657630614533\n",
      "Iter:  2283  Loss on train set:  -101.45480571570894\n",
      "Iter:  2284  Loss on train set:  -101.43452876264863\n",
      "Iter:  2285  Loss on train set:  -101.43206036202312\n",
      "Iter:  2286  Loss on train set:  -101.4224719494347\n",
      "Iter:  2287  Loss on train set:  -101.40561267238247\n",
      "Iter:  2288  Loss on train set:  -101.45280989436024\n",
      "Iter:  2289  Loss on train set:  -101.50306779791563\n",
      "Iter:  2290  Loss on train set:  -101.53461251017919\n",
      "Iter:  2291  Loss on train set:  -101.55455595917606\n",
      "Iter:  2292  Loss on train set:  -101.59480968861926\n",
      "Iter:  2293  Loss on train set:  -101.63874391046652\n",
      "Iter:  2294  Loss on train set:  -101.67001530873677\n",
      "Iter:  2295  Loss on train set:  -101.701258949451\n",
      "Iter:  2296  Loss on train set:  -101.7236578821262\n",
      "Iter:  2297  Loss on train set:  -101.80098674509588\n",
      "Iter:  2298  Loss on train set:  -101.87980606359503\n",
      "Iter:  2299  Loss on train set:  -101.95624089717532\n",
      "Iter:  2300  Loss on train set:  -102.00843773387534\n",
      "Iter:  2301  Loss on train set:  -102.04347854386396\n",
      "Iter:  2302  Loss on train set:  -102.07218573952984\n",
      "Iter:  2303  Loss on train set:  -102.08758449442496\n",
      "Iter:  2304  Loss on train set:  -102.09027918467908\n",
      "Iter:  2305  Loss on train set:  -102.08364093773706\n",
      "Iter:  2306  Loss on train set:  -102.09253371364682\n",
      "Iter:  2307  Loss on train set:  -102.09717824968206\n",
      "Iter:  2308  Loss on train set:  -102.09730642264608\n",
      "Iter:  2309  Loss on train set:  -102.09866678431958\n",
      "Iter:  2310  Loss on train set:  -102.0997377697298\n",
      "Iter:  2311  Loss on train set:  -102.09940556184682\n",
      "Iter:  2312  Loss on train set:  -102.10168895102812\n",
      "Iter:  2313  Loss on train set:  -102.1058249527022\n",
      "Iter:  2314  Loss on train set:  -102.1142908951157\n",
      "Iter:  2315  Loss on train set:  -102.11963028156964\n",
      "Iter:  2316  Loss on train set:  -102.12260243752996\n",
      "Iter:  2317  Loss on train set:  -102.12733935431876\n",
      "Iter:  2318  Loss on train set:  -102.1351033677851\n",
      "Iter:  2319  Loss on train set:  -102.16474974996036\n",
      "Iter:  2320  Loss on train set:  -102.18993806075983\n",
      "Iter:  2321  Loss on train set:  -102.20884826108065\n",
      "Iter:  2322  Loss on train set:  -102.22734645923177\n",
      "Iter:  2323  Loss on train set:  -102.24304243612855\n",
      "Iter:  2324  Loss on train set:  -102.25618583786053\n",
      "Iter:  2325  Loss on train set:  -102.26561579343743\n",
      "Iter:  2326  Loss on train set:  -102.27157220729235\n",
      "Iter:  2327  Loss on train set:  -102.2942783518896\n",
      "Iter:  2328  Loss on train set:  -102.33802503043356\n",
      "Iter:  2329  Loss on train set:  -102.36429114753221\n",
      "Iter:  2330  Loss on train set:  -102.37734604993905\n",
      "Iter:  2331  Loss on train set:  -102.39642788047095\n",
      "Iter:  2332  Loss on train set:  -102.40889030987469\n",
      "Iter:  2333  Loss on train set:  -102.41598022770546\n",
      "Iter:  2334  Loss on train set:  -102.42714037407396\n",
      "Iter:  2335  Loss on train set:  -102.43888146050706\n",
      "Iter:  2336  Loss on train set:  -102.44733995063828\n",
      "Iter:  2337  Loss on train set:  -102.45284491890175\n",
      "Iter:  2338  Loss on train set:  -102.45595029759608\n",
      "Iter:  2339  Loss on train set:  -102.4591481283214\n",
      "Iter:  2340  Loss on train set:  -102.46074620516794\n",
      "Iter:  2341  Loss on train set:  -102.461771069859\n",
      "Iter:  2342  Loss on train set:  -102.46121354217424\n",
      "Iter:  2343  Loss on train set:  -102.4728213027297\n",
      "Iter:  2344  Loss on train set:  -102.48412547277655\n",
      "Iter:  2345  Loss on train set:  -102.49234410664204\n",
      "Iter:  2346  Loss on train set:  -102.4995152538455\n",
      "Iter:  2347  Loss on train set:  -102.50681078384038\n",
      "Iter:  2348  Loss on train set:  -102.5124429233049\n",
      "Iter:  2349  Loss on train set:  -102.5165998971986\n",
      "Iter:  2350  Loss on train set:  -102.52034107490898\n",
      "Iter:  2351  Loss on train set:  -102.52307789907044\n",
      "Iter:  2352  Loss on train set:  -102.52485227479312\n",
      "Iter:  2353  Loss on train set:  -102.5265797799257\n",
      "Iter:  2354  Loss on train set:  -102.52793194491761\n",
      "Iter:  2355  Loss on train set:  -102.52894444307688\n",
      "Iter:  2356  Loss on train set:  -102.53001475449722\n",
      "Iter:  2357  Loss on train set:  -102.53116612172467\n",
      "Iter:  2358  Loss on train set:  -102.53200183784911\n",
      "Iter:  2359  Loss on train set:  -102.53482125644933\n",
      "Iter:  2360  Loss on train set:  -102.53877724077188\n",
      "Iter:  2361  Loss on train set:  -102.54322646011758\n",
      "Iter:  2362  Loss on train set:  -102.54632154724428\n",
      "Iter:  2363  Loss on train set:  -102.5522298859478\n",
      "Iter:  2364  Loss on train set:  -102.5558907591415\n",
      "Iter:  2365  Loss on train set:  -102.56093198328783\n",
      "Iter:  2366  Loss on train set:  -102.56350382317805\n",
      "Iter:  2367  Loss on train set:  -102.56318715448015\n",
      "Iter:  2368  Loss on train set:  -102.56155183064801\n",
      "Iter:  2369  Loss on train set:  -102.5584120024187\n",
      "Iter:  2370  Loss on train set:  -102.55387954141952\n",
      "Iter:  2371  Loss on train set:  -102.5493421390687\n",
      "Iter:  2372  Loss on train set:  -102.55495623041371\n",
      "Iter:  2373  Loss on train set:  -102.56023521484755\n",
      "Iter:  2374  Loss on train set:  -102.56353634984927\n",
      "Iter:  2375  Loss on train set:  -102.56673906517807\n",
      "Iter:  2376  Loss on train set:  -102.56899398882366\n",
      "Iter:  2377  Loss on train set:  -102.5707913359067\n",
      "Iter:  2378  Loss on train set:  -102.57695598199479\n",
      "Iter:  2379  Loss on train set:  -102.58437186422974\n",
      "Iter:  2380  Loss on train set:  -102.59038697452996\n",
      "Iter:  2381  Loss on train set:  -102.59653554088455\n",
      "Iter:  2382  Loss on train set:  -102.60194877543529\n",
      "Iter:  2383  Loss on train set:  -102.60635267243691\n",
      "Iter:  2384  Loss on train set:  -102.6106056194028\n",
      "Iter:  2385  Loss on train set:  -102.61375690001408\n",
      "Iter:  2386  Loss on train set:  -102.61607058130565\n",
      "Iter:  2387  Loss on train set:  -102.61774713944789\n",
      "Iter:  2388  Loss on train set:  -102.62022551997808\n",
      "Iter:  2389  Loss on train set:  -102.62233305981182\n",
      "Iter:  2390  Loss on train set:  -102.62534629587685\n",
      "Iter:  2391  Loss on train set:  -102.629806003852\n",
      "Iter:  2392  Loss on train set:  -102.63438406350552\n",
      "Iter:  2393  Loss on train set:  -102.63736191670647\n",
      "Iter:  2394  Loss on train set:  -102.63924169663949\n",
      "Iter:  2395  Loss on train set:  -102.64072188159695\n",
      "Iter:  2396  Loss on train set:  -102.64578697173626\n",
      "Iter:  2397  Loss on train set:  -102.65087148335004\n",
      "Iter:  2398  Loss on train set:  -102.65772564230197\n",
      "Iter:  2399  Loss on train set:  -102.66725889074505\n",
      "Iter:  2400  Loss on train set:  -102.67529514509319\n",
      "     fun: -102.68128586564488\n",
      "     jac: array([ 0.69576039,  0.69576039, -0.69576039,  0.69576039, -0.69576039,\n",
      "        0.69576039, -0.69576039, -0.69576039, -0.69576039,  0.69576039,\n",
      "        0.69576039, -0.69576039, -0.69576039, -0.69576039, -0.69576039,\n",
      "        0.69576039, -0.69576039,  0.69576039,  0.69576039,  0.69576039,\n",
      "       -0.69576039,  0.69576039, -0.69576039,  0.69576039, -0.69576039,\n",
      "        0.69576039,  0.69576039, -0.69576039,  0.69576039, -0.69576039,\n",
      "        0.69576039,  0.69576039])\n",
      "    nfev: 200\n",
      "     nit: 200\n",
      " success: True\n",
      "       x: array([ 1.01709796,  1.1479525 , -0.26604916, -0.53075434,  1.47843535,\n",
      "        0.24444597,  0.20748557, -0.10447591,  0.8538905 , -0.25021386,\n",
      "       -2.11054335, -0.49412875, -1.42771671, -0.48886134, -0.74133143,\n",
      "        0.44671232,  0.14850553, -0.85555784, -0.21067674, -0.42573457,\n",
      "        0.09990725,  0.65437343, -0.29065845,  0.07172837, -0.57956811,\n",
      "       -0.02032736, -0.65322909,  1.07614777,  0.13416567, -1.02741982,\n",
      "        0.36799524, -0.85616877])\n",
      "Running repetition:  3\n",
      "Unfrozen params:  16  Frozen params:  80\n",
      "Iter:  2401  Loss on train set:  -102.68128586564488\n",
      "Iter:  2402  Loss on train set:  -102.52453644970595\n",
      "Iter:  2403  Loss on train set:  -102.4046542278895\n",
      "Iter:  2404  Loss on train set:  -102.2150117109241\n",
      "Iter:  2405  Loss on train set:  -102.05435883447171\n",
      "Iter:  2406  Loss on train set:  -101.94506503858554\n",
      "Iter:  2407  Loss on train set:  -101.93743972123386\n",
      "Iter:  2408  Loss on train set:  -102.07661676014936\n",
      "Iter:  2409  Loss on train set:  -102.1634239490909\n",
      "Iter:  2410  Loss on train set:  -102.45857145719752\n",
      "Iter:  2411  Loss on train set:  -102.60418290796501\n",
      "Iter:  2412  Loss on train set:  -102.72106786879336\n",
      "Iter:  2413  Loss on train set:  -102.74539248818861\n",
      "Iter:  2414  Loss on train set:  -102.70177028233061\n",
      "Iter:  2415  Loss on train set:  -102.60972392047029\n",
      "Iter:  2416  Loss on train set:  -102.48369531310861\n",
      "Iter:  2417  Loss on train set:  -102.4256948130321\n",
      "Iter:  2418  Loss on train set:  -102.33136958054222\n",
      "Iter:  2419  Loss on train set:  -102.28570714756042\n",
      "Iter:  2420  Loss on train set:  -102.21729614824416\n",
      "Iter:  2421  Loss on train set:  -102.16259532685957\n",
      "Iter:  2422  Loss on train set:  -102.09116137078625\n",
      "Iter:  2423  Loss on train set:  -102.0171920885161\n",
      "Iter:  2424  Loss on train set:  -101.94306539635956\n",
      "Iter:  2425  Loss on train set:  -102.15484777254889\n",
      "Iter:  2426  Loss on train set:  -102.32835060944707\n",
      "Iter:  2427  Loss on train set:  -102.46665518456224\n",
      "Iter:  2428  Loss on train set:  -102.57377947399827\n",
      "Iter:  2429  Loss on train set:  -102.64213094626383\n",
      "Iter:  2430  Loss on train set:  -102.67424093115115\n",
      "Iter:  2431  Loss on train set:  -102.69299706526463\n",
      "Iter:  2432  Loss on train set:  -102.67738570662644\n",
      "Iter:  2433  Loss on train set:  -102.66414955267675\n",
      "Iter:  2434  Loss on train set:  -102.64118151268904\n",
      "Iter:  2435  Loss on train set:  -102.70552104345458\n",
      "Iter:  2436  Loss on train set:  -102.7623405840232\n",
      "Iter:  2437  Loss on train set:  -102.80590835871485\n",
      "Iter:  2438  Loss on train set:  -102.84135059041749\n",
      "Iter:  2439  Loss on train set:  -102.88404182732073\n",
      "Iter:  2440  Loss on train set:  -102.95595263461936\n",
      "Iter:  2441  Loss on train set:  -103.0281018339747\n",
      "Iter:  2442  Loss on train set:  -103.0938448773123\n",
      "Iter:  2443  Loss on train set:  -103.1526718529795\n",
      "Iter:  2444  Loss on train set:  -103.28545404745859\n",
      "Iter:  2445  Loss on train set:  -103.36679451738232\n",
      "Iter:  2446  Loss on train set:  -103.402402218237\n",
      "Iter:  2447  Loss on train set:  -103.40738276058143\n",
      "Iter:  2448  Loss on train set:  -103.37053878693918\n",
      "Iter:  2449  Loss on train set:  -103.3050720981502\n",
      "Iter:  2450  Loss on train set:  -103.2242654372687\n",
      "Iter:  2451  Loss on train set:  -103.15507877575521\n",
      "Iter:  2452  Loss on train set:  -103.07469439859912\n",
      "Iter:  2453  Loss on train set:  -103.08554950060628\n",
      "Iter:  2454  Loss on train set:  -103.08982589670545\n",
      "Iter:  2455  Loss on train set:  -103.12517133040262\n",
      "Iter:  2456  Loss on train set:  -103.19095333736621\n",
      "Iter:  2457  Loss on train set:  -103.23482876823513\n",
      "Iter:  2458  Loss on train set:  -103.26172388915526\n",
      "Iter:  2459  Loss on train set:  -103.27788216010413\n",
      "Iter:  2460  Loss on train set:  -103.30557190101827\n",
      "Iter:  2461  Loss on train set:  -103.31847432990209\n",
      "Iter:  2462  Loss on train set:  -103.32599481194507\n",
      "Iter:  2463  Loss on train set:  -103.34998681985371\n",
      "Iter:  2464  Loss on train set:  -103.37032126991222\n",
      "Iter:  2465  Loss on train set:  -103.37676267282899\n",
      "Iter:  2466  Loss on train set:  -103.37127339868333\n",
      "Iter:  2467  Loss on train set:  -103.35817106000741\n",
      "Iter:  2468  Loss on train set:  -103.33954325744617\n",
      "Iter:  2469  Loss on train set:  -103.31592017053713\n",
      "Iter:  2470  Loss on train set:  -103.2880705756727\n",
      "Iter:  2471  Loss on train set:  -103.26367143000606\n",
      "Iter:  2472  Loss on train set:  -103.2456189804134\n",
      "Iter:  2473  Loss on train set:  -103.26210381412842\n",
      "Iter:  2474  Loss on train set:  -103.26813327368467\n",
      "Iter:  2475  Loss on train set:  -103.26923493704598\n",
      "Iter:  2476  Loss on train set:  -103.26623902478377\n",
      "Iter:  2477  Loss on train set:  -103.28848518108146\n",
      "Iter:  2478  Loss on train set:  -103.30603752322583\n",
      "Iter:  2479  Loss on train set:  -103.31539744396943\n",
      "Iter:  2480  Loss on train set:  -103.31931849088099\n",
      "Iter:  2481  Loss on train set:  -103.3194153518785\n",
      "Iter:  2482  Loss on train set:  -103.32013844907102\n",
      "Iter:  2483  Loss on train set:  -103.32292135737144\n",
      "Iter:  2484  Loss on train set:  -103.35982774777997\n",
      "Iter:  2485  Loss on train set:  -103.38109392330182\n",
      "Iter:  2486  Loss on train set:  -103.41296296101568\n",
      "Iter:  2487  Loss on train set:  -103.43098172304033\n",
      "Iter:  2488  Loss on train set:  -103.44399384137525\n",
      "Iter:  2489  Loss on train set:  -103.45517960867522\n",
      "Iter:  2490  Loss on train set:  -103.45306418745336\n",
      "Iter:  2491  Loss on train set:  -103.43894358696346\n",
      "Iter:  2492  Loss on train set:  -103.42299668735559\n",
      "Iter:  2493  Loss on train set:  -103.40848846569448\n",
      "Iter:  2494  Loss on train set:  -103.38457874425887\n",
      "Iter:  2495  Loss on train set:  -103.35497927436919\n",
      "Iter:  2496  Loss on train set:  -103.32156744289985\n",
      "Iter:  2497  Loss on train set:  -103.30044282612677\n",
      "Iter:  2498  Loss on train set:  -103.28863590638198\n",
      "Iter:  2499  Loss on train set:  -103.30052237104442\n",
      "Iter:  2500  Loss on train set:  -103.30584126669588\n",
      "Iter:  2501  Loss on train set:  -103.31162721670344\n",
      "Iter:  2502  Loss on train set:  -103.31337705557087\n",
      "Iter:  2503  Loss on train set:  -103.31274309455343\n",
      "Iter:  2504  Loss on train set:  -103.31540936542225\n",
      "Iter:  2505  Loss on train set:  -103.3468534864894\n",
      "Iter:  2506  Loss on train set:  -103.39120600493847\n",
      "Iter:  2507  Loss on train set:  -103.41204105520887\n",
      "Iter:  2508  Loss on train set:  -103.41772684083844\n",
      "Iter:  2509  Loss on train set:  -103.40932684346154\n",
      "Iter:  2510  Loss on train set:  -103.39945146614554\n",
      "Iter:  2511  Loss on train set:  -103.38011160279767\n",
      "Iter:  2512  Loss on train set:  -103.3562119028447\n",
      "Iter:  2513  Loss on train set:  -103.33531165890662\n",
      "Iter:  2514  Loss on train set:  -103.31167505929233\n",
      "Iter:  2515  Loss on train set:  -103.2884864636886\n",
      "Iter:  2516  Loss on train set:  -103.27553683047225\n",
      "Iter:  2517  Loss on train set:  -103.26175435911237\n",
      "Iter:  2518  Loss on train set:  -103.24753022995552\n",
      "Iter:  2519  Loss on train set:  -103.24775452924048\n",
      "Iter:  2520  Loss on train set:  -103.25162128484656\n",
      "Iter:  2521  Loss on train set:  -103.2521555884476\n",
      "Iter:  2522  Loss on train set:  -103.24919101847283\n",
      "Iter:  2523  Loss on train set:  -103.26207156393811\n",
      "Iter:  2524  Loss on train set:  -103.27114360927094\n",
      "Iter:  2525  Loss on train set:  -103.27405769230677\n",
      "Iter:  2526  Loss on train set:  -103.28841707877596\n",
      "Iter:  2527  Loss on train set:  -103.28796022961991\n",
      "Iter:  2528  Loss on train set:  -103.27549980758026\n",
      "Iter:  2529  Loss on train set:  -103.2525641559478\n",
      "Iter:  2530  Loss on train set:  -103.24873523328718\n",
      "Iter:  2531  Loss on train set:  -103.23891647147417\n",
      "Iter:  2532  Loss on train set:  -103.22778357177215\n",
      "Iter:  2533  Loss on train set:  -103.21205277290944\n",
      "Iter:  2534  Loss on train set:  -103.20449289503063\n",
      "Iter:  2535  Loss on train set:  -103.22772592816115\n",
      "Iter:  2536  Loss on train set:  -103.24324738344521\n",
      "Iter:  2537  Loss on train set:  -103.26691012599449\n",
      "Iter:  2538  Loss on train set:  -103.35046076424672\n",
      "Iter:  2539  Loss on train set:  -103.38659579300557\n",
      "Iter:  2540  Loss on train set:  -103.40856665983608\n",
      "Iter:  2541  Loss on train set:  -103.3931321178144\n",
      "Iter:  2542  Loss on train set:  -103.35823878288844\n",
      "Iter:  2543  Loss on train set:  -103.30809550724325\n",
      "Iter:  2544  Loss on train set:  -103.24864707623472\n",
      "Iter:  2545  Loss on train set:  -103.23825518186473\n",
      "Iter:  2546  Loss on train set:  -103.21566639373597\n",
      "Iter:  2547  Loss on train set:  -103.20180920089757\n",
      "Iter:  2548  Loss on train set:  -103.16594260665248\n",
      "Iter:  2549  Loss on train set:  -103.12222139240126\n",
      "Iter:  2550  Loss on train set:  -103.06874913600092\n",
      "Iter:  2551  Loss on train set:  -103.03834198197427\n",
      "Iter:  2552  Loss on train set:  -102.9951592034252\n",
      "Iter:  2553  Loss on train set:  -102.95922453192303\n",
      "Iter:  2554  Loss on train set:  -102.91602405034624\n",
      "Iter:  2555  Loss on train set:  -102.9022676043409\n",
      "Iter:  2556  Loss on train set:  -102.88967998055371\n",
      "Iter:  2557  Loss on train set:  -102.87525482867764\n",
      "Iter:  2558  Loss on train set:  -102.8692595631158\n",
      "Iter:  2559  Loss on train set:  -102.88774741771488\n",
      "Iter:  2560  Loss on train set:  -102.92167265325713\n",
      "Iter:  2561  Loss on train set:  -102.93498653105273\n",
      "Iter:  2562  Loss on train set:  -102.94476266218648\n",
      "Iter:  2563  Loss on train set:  -102.94816840070105\n",
      "Iter:  2564  Loss on train set:  -103.00413576606027\n",
      "Iter:  2565  Loss on train set:  -103.05855196311231\n",
      "Iter:  2566  Loss on train set:  -103.0871091960938\n",
      "Iter:  2567  Loss on train set:  -103.09724839001623\n",
      "Iter:  2568  Loss on train set:  -103.10272535360801\n",
      "Iter:  2569  Loss on train set:  -103.09128876846808\n",
      "Iter:  2570  Loss on train set:  -103.06524343781251\n",
      "Iter:  2571  Loss on train set:  -103.03850816681359\n",
      "Iter:  2572  Loss on train set:  -103.04062210086245\n",
      "Iter:  2573  Loss on train set:  -103.03959574173469\n",
      "Iter:  2574  Loss on train set:  -103.06250122184218\n",
      "Iter:  2575  Loss on train set:  -103.06861105424541\n",
      "Iter:  2576  Loss on train set:  -103.07244070668978\n",
      "Iter:  2577  Loss on train set:  -103.06866788764535\n",
      "Iter:  2578  Loss on train set:  -103.06279476073423\n",
      "Iter:  2579  Loss on train set:  -103.06875143340812\n",
      "Iter:  2580  Loss on train set:  -103.07463365873693\n",
      "Iter:  2581  Loss on train set:  -103.08528220611251\n",
      "Iter:  2582  Loss on train set:  -103.08437597469349\n",
      "Iter:  2583  Loss on train set:  -103.12155041029199\n",
      "Iter:  2584  Loss on train set:  -103.1354936129353\n",
      "Iter:  2585  Loss on train set:  -103.15595388102012\n",
      "Iter:  2586  Loss on train set:  -103.15893456768583\n",
      "Iter:  2587  Loss on train set:  -103.13922676134622\n",
      "Iter:  2588  Loss on train set:  -103.14266641579876\n",
      "Iter:  2589  Loss on train set:  -103.19216101468787\n",
      "Iter:  2590  Loss on train set:  -103.20928497892588\n",
      "Iter:  2591  Loss on train set:  -103.20528008664657\n",
      "Iter:  2592  Loss on train set:  -103.20281772739101\n",
      "Iter:  2593  Loss on train set:  -103.18472092620799\n",
      "Iter:  2594  Loss on train set:  -103.19993643942244\n",
      "Iter:  2595  Loss on train set:  -103.21747410267038\n",
      "Iter:  2596  Loss on train set:  -103.20989153801419\n",
      "Iter:  2597  Loss on train set:  -103.18420878009375\n",
      "Iter:  2598  Loss on train set:  -103.17955997476729\n",
      "Iter:  2599  Loss on train set:  -103.16725780425568\n",
      "Iter:  2600  Loss on train set:  -103.14889658757829\n",
      "     fun: -103.15033224262757\n",
      "     jac: array([-6.39061009, -6.39061009, -6.39061009,  6.39061009,  6.39061009,\n",
      "       -6.39061009, -6.39061009, -6.39061009, -6.39061009, -6.39061009,\n",
      "        6.39061009,  6.39061009, -6.39061009, -6.39061009,  6.39061009,\n",
      "       -6.39061009])\n",
      "    nfev: 200\n",
      "     nit: 200\n",
      " success: True\n",
      "       x: array([-0.46841498, -2.17833509, -0.84805879, -0.91692112,  0.58674427,\n",
      "       -1.30890182, -1.4520901 , -0.66163552,  0.40509364, -1.76681066,\n",
      "       -0.446058  , -1.56830833, -0.73767418, -0.65321297,  1.41270788,\n",
      "        0.60541485])\n",
      "Unfrozen params:  16  Frozen params:  80\n",
      "Iter:  2601  Loss on train set:  -103.15033224262757\n",
      "Iter:  2602  Loss on train set:  -103.05759713789979\n",
      "Iter:  2603  Loss on train set:  -103.06797364340258\n",
      "Iter:  2604  Loss on train set:  -103.06633617700186\n",
      "Iter:  2605  Loss on train set:  -103.05670894549007\n",
      "Iter:  2606  Loss on train set:  -103.04119038004093\n",
      "Iter:  2607  Loss on train set:  -103.0126892210539\n",
      "Iter:  2608  Loss on train set:  -102.97687801663201\n",
      "Iter:  2609  Loss on train set:  -102.94744505430562\n",
      "Iter:  2610  Loss on train set:  -102.90267447720396\n",
      "Iter:  2611  Loss on train set:  -102.8615737594313\n",
      "Iter:  2612  Loss on train set:  -102.83233279654365\n",
      "Iter:  2613  Loss on train set:  -102.78860676970105\n",
      "Iter:  2614  Loss on train set:  -102.74176630986443\n",
      "Iter:  2615  Loss on train set:  -102.67227991102185\n",
      "Iter:  2616  Loss on train set:  -102.58956102159661\n",
      "Iter:  2617  Loss on train set:  -102.50516789798871\n",
      "Iter:  2618  Loss on train set:  -102.48396838106751\n",
      "Iter:  2619  Loss on train set:  -102.48025221231302\n",
      "Iter:  2620  Loss on train set:  -102.43610105130016\n",
      "Iter:  2621  Loss on train set:  -102.42847270374351\n",
      "Iter:  2622  Loss on train set:  -102.3971642923931\n",
      "Iter:  2623  Loss on train set:  -102.51417330084674\n",
      "Iter:  2624  Loss on train set:  -102.58024355932884\n",
      "Iter:  2625  Loss on train set:  -102.63464703022987\n",
      "Iter:  2626  Loss on train set:  -102.6685528594255\n",
      "Iter:  2627  Loss on train set:  -102.76637203785106\n",
      "Iter:  2628  Loss on train set:  -102.82185136054173\n",
      "Iter:  2629  Loss on train set:  -102.84703195754354\n",
      "Iter:  2630  Loss on train set:  -102.84904663907311\n",
      "Iter:  2631  Loss on train set:  -102.83520930730123\n",
      "Iter:  2632  Loss on train set:  -102.81487715521595\n",
      "Iter:  2633  Loss on train set:  -102.7942747926299\n",
      "Iter:  2634  Loss on train set:  -102.76871919513148\n",
      "Iter:  2635  Loss on train set:  -102.74309995166539\n",
      "Iter:  2636  Loss on train set:  -102.75171452104857\n",
      "Iter:  2637  Loss on train set:  -102.76896896211927\n",
      "Iter:  2638  Loss on train set:  -102.77655520007899\n",
      "Iter:  2639  Loss on train set:  -102.77606676604566\n",
      "Iter:  2640  Loss on train set:  -102.7739333523223\n",
      "Iter:  2641  Loss on train set:  -102.77722809320596\n",
      "Iter:  2642  Loss on train set:  -102.78796500848567\n",
      "Iter:  2643  Loss on train set:  -102.8559288284326\n",
      "Iter:  2644  Loss on train set:  -102.90859079381735\n",
      "Iter:  2645  Loss on train set:  -102.9542389330313\n",
      "Iter:  2646  Loss on train set:  -102.99177421212767\n",
      "Iter:  2647  Loss on train set:  -103.01507837616177\n",
      "Iter:  2648  Loss on train set:  -103.03056505822646\n",
      "Iter:  2649  Loss on train set:  -103.05206159017203\n",
      "Iter:  2650  Loss on train set:  -103.06310809098187\n",
      "Iter:  2651  Loss on train set:  -103.0959166602625\n",
      "Iter:  2652  Loss on train set:  -103.12500571613248\n",
      "Iter:  2653  Loss on train set:  -103.16298733779085\n",
      "Iter:  2654  Loss on train set:  -103.18314967372564\n",
      "Iter:  2655  Loss on train set:  -103.20655209456132\n",
      "Iter:  2656  Loss on train set:  -103.21942822304794\n",
      "Iter:  2657  Loss on train set:  -103.21713881003659\n",
      "Iter:  2658  Loss on train set:  -103.20602746316673\n",
      "Iter:  2659  Loss on train set:  -103.18233912431342\n",
      "Iter:  2660  Loss on train set:  -103.14850721612183\n",
      "Iter:  2661  Loss on train set:  -103.10768252867497\n",
      "Iter:  2662  Loss on train set:  -103.08879034119627\n",
      "Iter:  2663  Loss on train set:  -103.06491094066632\n",
      "Iter:  2664  Loss on train set:  -103.0461747248727\n",
      "Iter:  2665  Loss on train set:  -103.02657690002835\n",
      "Iter:  2666  Loss on train set:  -103.01622916031899\n",
      "Iter:  2667  Loss on train set:  -102.99658350809491\n",
      "Iter:  2668  Loss on train set:  -102.97032362403684\n",
      "Iter:  2669  Loss on train set:  -102.93492090070829\n",
      "Iter:  2670  Loss on train set:  -102.89903688955225\n",
      "Iter:  2671  Loss on train set:  -102.90507275711506\n",
      "Iter:  2672  Loss on train set:  -102.93504735713914\n",
      "Iter:  2673  Loss on train set:  -102.96354280815663\n",
      "Iter:  2674  Loss on train set:  -102.97886682899095\n",
      "Iter:  2675  Loss on train set:  -102.98897133683715\n",
      "Iter:  2676  Loss on train set:  -102.99239355455731\n",
      "Iter:  2677  Loss on train set:  -102.99727033976902\n",
      "Iter:  2678  Loss on train set:  -102.99569332655808\n",
      "Iter:  2679  Loss on train set:  -102.9993396837746\n",
      "Iter:  2680  Loss on train set:  -103.00508600199719\n",
      "Iter:  2681  Loss on train set:  -103.0030916277692\n",
      "Iter:  2682  Loss on train set:  -103.00250735033198\n",
      "Iter:  2683  Loss on train set:  -102.99699096401592\n",
      "Iter:  2684  Loss on train set:  -102.9917662906011\n",
      "Iter:  2685  Loss on train set:  -102.99212764587713\n",
      "Iter:  2686  Loss on train set:  -103.02510557349837\n",
      "Iter:  2687  Loss on train set:  -103.04990970205382\n",
      "Iter:  2688  Loss on train set:  -103.10420380550016\n",
      "Iter:  2689  Loss on train set:  -103.1523601995157\n",
      "Iter:  2690  Loss on train set:  -103.25986722360987\n",
      "Iter:  2691  Loss on train set:  -103.34777585250585\n",
      "Iter:  2692  Loss on train set:  -103.39841850648747\n",
      "Iter:  2693  Loss on train set:  -103.40371207190839\n",
      "Iter:  2694  Loss on train set:  -103.37590667710778\n",
      "Iter:  2695  Loss on train set:  -103.32557574157738\n",
      "Iter:  2696  Loss on train set:  -103.25611079322098\n",
      "Iter:  2697  Loss on train set:  -103.24089064080145\n",
      "Iter:  2698  Loss on train set:  -103.21460608515171\n",
      "Iter:  2699  Loss on train set:  -103.18515823217702\n",
      "Iter:  2700  Loss on train set:  -103.14402649336121\n",
      "Iter:  2701  Loss on train set:  -103.10584875848491\n",
      "Iter:  2702  Loss on train set:  -103.08948667310018\n",
      "Iter:  2703  Loss on train set:  -103.06650376286692\n",
      "Iter:  2704  Loss on train set:  -103.03238742776048\n",
      "Iter:  2705  Loss on train set:  -103.01836376125684\n",
      "Iter:  2706  Loss on train set:  -102.98623206787408\n",
      "Iter:  2707  Loss on train set:  -102.95122941478012\n",
      "Iter:  2708  Loss on train set:  -102.90888023459773\n",
      "Iter:  2709  Loss on train set:  -102.87387365151679\n",
      "Iter:  2710  Loss on train set:  -102.84082280390095\n",
      "Iter:  2711  Loss on train set:  -102.86456297518804\n",
      "Iter:  2712  Loss on train set:  -102.877196636599\n",
      "Iter:  2713  Loss on train set:  -102.8812085644382\n",
      "Iter:  2714  Loss on train set:  -102.87921880913528\n",
      "Iter:  2715  Loss on train set:  -102.89619378936922\n",
      "Iter:  2716  Loss on train set:  -102.94780995397001\n",
      "Iter:  2717  Loss on train set:  -103.0291153093001\n",
      "Iter:  2718  Loss on train set:  -103.0988358854877\n",
      "Iter:  2719  Loss on train set:  -103.14560696820422\n",
      "Iter:  2720  Loss on train set:  -103.18153293032373\n",
      "Iter:  2721  Loss on train set:  -103.22713913340655\n",
      "Iter:  2722  Loss on train set:  -103.24803222479365\n",
      "Iter:  2723  Loss on train set:  -103.24592191216642\n",
      "Iter:  2724  Loss on train set:  -103.2362820088865\n",
      "Iter:  2725  Loss on train set:  -103.20944524149384\n",
      "Iter:  2726  Loss on train set:  -103.18648245264951\n",
      "Iter:  2727  Loss on train set:  -103.15857673317076\n",
      "Iter:  2728  Loss on train set:  -103.12428440655157\n",
      "Iter:  2729  Loss on train set:  -103.11045310734957\n",
      "Iter:  2730  Loss on train set:  -103.09202299307636\n",
      "Iter:  2731  Loss on train set:  -103.13655572738855\n",
      "Iter:  2732  Loss on train set:  -103.16686825479454\n",
      "Iter:  2733  Loss on train set:  -103.20005233489974\n",
      "Iter:  2734  Loss on train set:  -103.23188250053803\n",
      "Iter:  2735  Loss on train set:  -103.27149048475147\n",
      "Iter:  2736  Loss on train set:  -103.29121977064247\n",
      "Iter:  2737  Loss on train set:  -103.30434613667022\n",
      "Iter:  2738  Loss on train set:  -103.32520630045892\n",
      "Iter:  2739  Loss on train set:  -103.3377351379122\n",
      "Iter:  2740  Loss on train set:  -103.34236626372757\n",
      "Iter:  2741  Loss on train set:  -103.34041940125495\n",
      "Iter:  2742  Loss on train set:  -103.34131472671922\n",
      "Iter:  2743  Loss on train set:  -103.33819920974165\n",
      "Iter:  2744  Loss on train set:  -103.33264799031453\n",
      "Iter:  2745  Loss on train set:  -103.32390500984133\n",
      "Iter:  2746  Loss on train set:  -103.31432457960287\n",
      "Iter:  2747  Loss on train set:  -103.30651560921471\n",
      "Iter:  2748  Loss on train set:  -103.29617491251102\n",
      "Iter:  2749  Loss on train set:  -103.32788957618304\n",
      "Iter:  2750  Loss on train set:  -103.35281129856888\n",
      "Iter:  2751  Loss on train set:  -103.3700910174886\n",
      "Iter:  2752  Loss on train set:  -103.38190490580494\n",
      "Iter:  2753  Loss on train set:  -103.38774660376174\n",
      "Iter:  2754  Loss on train set:  -103.40880745829583\n",
      "Iter:  2755  Loss on train set:  -103.42678218104722\n",
      "Iter:  2756  Loss on train set:  -103.42559368961778\n",
      "Iter:  2757  Loss on train set:  -103.41308010631725\n",
      "Iter:  2758  Loss on train set:  -103.39176603021038\n",
      "Iter:  2759  Loss on train set:  -103.37029603164649\n",
      "Iter:  2760  Loss on train set:  -103.35097734109678\n",
      "Iter:  2761  Loss on train set:  -103.34163748119843\n",
      "Iter:  2762  Loss on train set:  -103.36927285784645\n",
      "Iter:  2763  Loss on train set:  -103.39428154796424\n",
      "Iter:  2764  Loss on train set:  -103.41131646389043\n",
      "Iter:  2765  Loss on train set:  -103.41997315048374\n",
      "Iter:  2766  Loss on train set:  -103.42498388935061\n",
      "Iter:  2767  Loss on train set:  -103.42328041488916\n",
      "Iter:  2768  Loss on train set:  -103.43875191242758\n",
      "Iter:  2769  Loss on train set:  -103.44341063595017\n",
      "Iter:  2770  Loss on train set:  -103.44175644075325\n",
      "Iter:  2771  Loss on train set:  -103.43552293798248\n",
      "Iter:  2772  Loss on train set:  -103.42694330869553\n",
      "Iter:  2773  Loss on train set:  -103.41913636414844\n",
      "Iter:  2774  Loss on train set:  -103.40712512529906\n",
      "Iter:  2775  Loss on train set:  -103.40369427019742\n",
      "Iter:  2776  Loss on train set:  -103.40973007705907\n",
      "Iter:  2777  Loss on train set:  -103.4204222315032\n",
      "Iter:  2778  Loss on train set:  -103.43207403036368\n",
      "Iter:  2779  Loss on train set:  -103.4412012154839\n",
      "Iter:  2780  Loss on train set:  -103.44907273492863\n",
      "Iter:  2781  Loss on train set:  -103.45626226491072\n",
      "Iter:  2782  Loss on train set:  -103.46186041391815\n",
      "Iter:  2783  Loss on train set:  -103.46871647488331\n",
      "Iter:  2784  Loss on train set:  -103.47375132982204\n",
      "Iter:  2785  Loss on train set:  -103.48162942737994\n",
      "Iter:  2786  Loss on train set:  -103.48811331684536\n",
      "Iter:  2787  Loss on train set:  -103.49403997103379\n",
      "Iter:  2788  Loss on train set:  -103.5009656754471\n",
      "Iter:  2789  Loss on train set:  -103.50570046410672\n",
      "Iter:  2790  Loss on train set:  -103.50885026247609\n",
      "Iter:  2791  Loss on train set:  -103.50907634886109\n",
      "Iter:  2792  Loss on train set:  -103.50801517602994\n",
      "Iter:  2793  Loss on train set:  -103.50313498533177\n",
      "Iter:  2794  Loss on train set:  -103.4948026161924\n",
      "Iter:  2795  Loss on train set:  -103.48625479138465\n",
      "Iter:  2796  Loss on train set:  -103.47850496918109\n",
      "Iter:  2797  Loss on train set:  -103.47378299160683\n",
      "Iter:  2798  Loss on train set:  -103.46756990532661\n",
      "Iter:  2799  Loss on train set:  -103.4588600306014\n",
      "Iter:  2800  Loss on train set:  -103.45393356044256\n",
      "     fun: -103.46433825692189\n",
      "     jac: array([-3.84692549, -3.84692549, -3.84692549, -3.84692549, -3.84692549,\n",
      "        3.84692549,  3.84692549, -3.84692549,  3.84692549,  3.84692549,\n",
      "       -3.84692549,  3.84692549, -3.84692549, -3.84692549,  3.84692549,\n",
      "        3.84692549])\n",
      "    nfev: 200\n",
      "     nit: 200\n",
      " success: True\n",
      "       x: array([ 0.00520692,  0.05504463,  0.13809612, -0.09402774, -0.12998714,\n",
      "        0.04972062, -2.19997636, -0.0255792 ,  2.7623215 , -0.28802369,\n",
      "        0.62399592,  0.03123748, -0.08594859,  0.02174833,  0.15718963,\n",
      "        0.05055606])\n",
      "Unfrozen params:  32  Frozen params:  64\n",
      "Iter:  2801  Loss on train set:  -103.46433825692189\n",
      "Iter:  2802  Loss on train set:  -103.01805638487657\n",
      "Iter:  2803  Loss on train set:  -103.02724185141305\n",
      "Iter:  2804  Loss on train set:  -103.02933751173961\n",
      "Iter:  2805  Loss on train set:  -102.94702598792719\n",
      "Iter:  2806  Loss on train set:  -102.81327895010105\n",
      "Iter:  2807  Loss on train set:  -102.65272481508768\n",
      "Iter:  2808  Loss on train set:  -102.50903679211186\n",
      "Iter:  2809  Loss on train set:  -102.35448451073401\n",
      "Iter:  2810  Loss on train set:  -102.20579707731822\n",
      "Iter:  2811  Loss on train set:  -102.10397445170042\n",
      "Iter:  2812  Loss on train set:  -102.11574752185423\n",
      "Iter:  2813  Loss on train set:  -102.0679845958365\n",
      "Iter:  2814  Loss on train set:  -101.93487365421876\n",
      "Iter:  2815  Loss on train set:  -101.69576911101606\n",
      "Iter:  2816  Loss on train set:  -101.57439251546342\n",
      "Iter:  2817  Loss on train set:  -101.47295474295555\n",
      "Iter:  2818  Loss on train set:  -101.63579914234484\n",
      "Iter:  2819  Loss on train set:  -101.72261619870253\n",
      "Iter:  2820  Loss on train set:  -101.74062511044431\n",
      "Iter:  2821  Loss on train set:  -101.69349989064433\n",
      "Iter:  2822  Loss on train set:  -101.64109202966814\n",
      "Iter:  2823  Loss on train set:  -101.54304231673702\n",
      "Iter:  2824  Loss on train set:  -101.4130637957964\n",
      "Iter:  2825  Loss on train set:  -101.26393914679524\n",
      "Iter:  2826  Loss on train set:  -101.13571660409542\n",
      "Iter:  2827  Loss on train set:  -101.00824963248223\n",
      "Iter:  2828  Loss on train set:  -100.87570827226597\n",
      "Iter:  2829  Loss on train set:  -100.80019965814998\n",
      "Iter:  2830  Loss on train set:  -100.70848522477147\n",
      "Iter:  2831  Loss on train set:  -100.63187933686851\n",
      "Iter:  2832  Loss on train set:  -100.56362791648947\n",
      "Iter:  2833  Loss on train set:  -100.46458613868779\n",
      "Iter:  2834  Loss on train set:  -100.35504328278779\n",
      "Iter:  2835  Loss on train set:  -100.53124186763188\n",
      "Iter:  2836  Loss on train set:  -101.08264202216606\n",
      "Iter:  2837  Loss on train set:  -101.5049494800725\n",
      "Iter:  2838  Loss on train set:  -101.93743807525047\n",
      "Iter:  2839  Loss on train set:  -102.27981406332316\n",
      "Iter:  2840  Loss on train set:  -102.55266937367156\n",
      "Iter:  2841  Loss on train set:  -102.69314442025265\n",
      "Iter:  2842  Loss on train set:  -102.72911120697925\n",
      "Iter:  2843  Loss on train set:  -102.69735139701716\n",
      "Iter:  2844  Loss on train set:  -102.58967045465697\n",
      "Iter:  2845  Loss on train set:  -102.64735119860299\n",
      "Iter:  2846  Loss on train set:  -102.63703197472796\n",
      "Iter:  2847  Loss on train set:  -102.5884334781355\n",
      "Iter:  2848  Loss on train set:  -102.50486779436889\n",
      "Iter:  2849  Loss on train set:  -102.4823845600009\n",
      "Iter:  2850  Loss on train set:  -102.49627976576865\n",
      "Iter:  2851  Loss on train set:  -102.46327331079011\n",
      "Iter:  2852  Loss on train set:  -102.41150549719627\n",
      "Iter:  2853  Loss on train set:  -102.34414664793118\n",
      "Iter:  2854  Loss on train set:  -102.31842708152521\n",
      "Iter:  2855  Loss on train set:  -102.29908129095341\n",
      "Iter:  2856  Loss on train set:  -102.34499554051655\n",
      "Iter:  2857  Loss on train set:  -102.39859318380161\n",
      "Iter:  2858  Loss on train set:  -102.49702346380505\n",
      "Iter:  2859  Loss on train set:  -102.5606215600656\n",
      "Iter:  2860  Loss on train set:  -102.62795128735691\n",
      "Iter:  2861  Loss on train set:  -102.65898260680292\n",
      "Iter:  2862  Loss on train set:  -102.66835316181103\n",
      "Iter:  2863  Loss on train set:  -102.67124174561042\n",
      "Iter:  2864  Loss on train set:  -102.66333216511272\n",
      "Iter:  2865  Loss on train set:  -102.63490247149761\n",
      "Iter:  2866  Loss on train set:  -102.5900361121916\n",
      "Iter:  2867  Loss on train set:  -102.54539368656549\n",
      "Iter:  2868  Loss on train set:  -102.5560463472344\n",
      "Iter:  2869  Loss on train set:  -102.6327427335453\n",
      "Iter:  2870  Loss on train set:  -102.70472955032048\n",
      "Iter:  2871  Loss on train set:  -102.75792595592581\n",
      "Iter:  2872  Loss on train set:  -102.79323991349895\n",
      "Iter:  2873  Loss on train set:  -102.82647719671185\n",
      "Iter:  2874  Loss on train set:  -102.8444468616781\n",
      "Iter:  2875  Loss on train set:  -102.97864260763924\n",
      "Iter:  2876  Loss on train set:  -103.06823334488968\n",
      "Iter:  2877  Loss on train set:  -103.11856026745227\n",
      "Iter:  2878  Loss on train set:  -103.13803770818575\n",
      "Iter:  2879  Loss on train set:  -103.1430156299209\n",
      "Iter:  2880  Loss on train set:  -103.13095749632903\n",
      "Iter:  2881  Loss on train set:  -103.10585885561191\n",
      "Iter:  2882  Loss on train set:  -103.0820293942548\n",
      "Iter:  2883  Loss on train set:  -103.07123405542903\n",
      "Iter:  2884  Loss on train set:  -103.05766364128615\n",
      "Iter:  2885  Loss on train set:  -103.02985531597095\n",
      "Iter:  2886  Loss on train set:  -103.01107736177782\n",
      "Iter:  2887  Loss on train set:  -103.02988641264892\n",
      "Iter:  2888  Loss on train set:  -103.06990611277566\n",
      "Iter:  2889  Loss on train set:  -103.09736119542876\n",
      "Iter:  2890  Loss on train set:  -103.13662698460865\n",
      "Iter:  2891  Loss on train set:  -103.14309468800745\n",
      "Iter:  2892  Loss on train set:  -103.13136560361824\n",
      "Iter:  2893  Loss on train set:  -103.09794661563961\n",
      "Iter:  2894  Loss on train set:  -103.08449813391819\n",
      "Iter:  2895  Loss on train set:  -103.06017898088206\n",
      "Iter:  2896  Loss on train set:  -103.10437502486002\n",
      "Iter:  2897  Loss on train set:  -103.11344742220545\n",
      "Iter:  2898  Loss on train set:  -103.11177325627324\n",
      "Iter:  2899  Loss on train set:  -103.0997681338106\n",
      "Iter:  2900  Loss on train set:  -103.06520205104347\n",
      "Iter:  2901  Loss on train set:  -103.03201846693224\n",
      "Iter:  2902  Loss on train set:  -102.99070943417107\n",
      "Iter:  2903  Loss on train set:  -102.9464843180481\n",
      "Iter:  2904  Loss on train set:  -102.93320632407249\n",
      "Iter:  2905  Loss on train set:  -102.88380872145387\n",
      "Iter:  2906  Loss on train set:  -102.81599225425542\n",
      "Iter:  2907  Loss on train set:  -102.73898226163237\n",
      "Iter:  2908  Loss on train set:  -102.65227807366375\n",
      "Iter:  2909  Loss on train set:  -102.58069796303045\n",
      "Iter:  2910  Loss on train set:  -102.50606433871867\n",
      "Iter:  2911  Loss on train set:  -102.52432835196024\n",
      "Iter:  2912  Loss on train set:  -102.53715599127572\n",
      "Iter:  2913  Loss on train set:  -102.53473734789041\n",
      "Iter:  2914  Loss on train set:  -102.5418918864129\n",
      "Iter:  2915  Loss on train set:  -102.55374137741282\n",
      "Iter:  2916  Loss on train set:  -102.64939603115184\n",
      "Iter:  2917  Loss on train set:  -102.75679169872562\n",
      "Iter:  2918  Loss on train set:  -102.82745887579918\n",
      "Iter:  2919  Loss on train set:  -102.86695941583204\n",
      "Iter:  2920  Loss on train set:  -102.89085518748116\n",
      "Iter:  2921  Loss on train set:  -102.89987866779089\n",
      "Iter:  2922  Loss on train set:  -102.90232650205664\n",
      "Iter:  2923  Loss on train set:  -102.89302866128044\n",
      "Iter:  2924  Loss on train set:  -102.87456965313312\n",
      "Iter:  2925  Loss on train set:  -102.86473847009476\n",
      "Iter:  2926  Loss on train set:  -102.85482780255622\n",
      "Iter:  2927  Loss on train set:  -102.88092444039586\n",
      "Iter:  2928  Loss on train set:  -102.89907463470348\n",
      "Iter:  2929  Loss on train set:  -102.91230347148934\n",
      "Iter:  2930  Loss on train set:  -102.91992621499458\n",
      "Iter:  2931  Loss on train set:  -102.92367562105797\n",
      "Iter:  2932  Loss on train set:  -102.95486034889656\n",
      "Iter:  2933  Loss on train set:  -102.98115206431068\n",
      "Iter:  2934  Loss on train set:  -102.99471478012026\n",
      "Iter:  2935  Loss on train set:  -103.002324863028\n",
      "Iter:  2936  Loss on train set:  -103.05758898974781\n",
      "Iter:  2937  Loss on train set:  -103.13204232004166\n",
      "Iter:  2938  Loss on train set:  -103.17472035227942\n",
      "Iter:  2939  Loss on train set:  -103.20833739581894\n",
      "Iter:  2940  Loss on train set:  -103.2258849644853\n",
      "Iter:  2941  Loss on train set:  -103.2333220987914\n",
      "Iter:  2942  Loss on train set:  -103.24201292813086\n",
      "Iter:  2943  Loss on train set:  -103.23922211871307\n",
      "Iter:  2944  Loss on train set:  -103.23731422230273\n",
      "Iter:  2945  Loss on train set:  -103.25540015147176\n",
      "Iter:  2946  Loss on train set:  -103.25261316938715\n",
      "Iter:  2947  Loss on train set:  -103.2381747824973\n",
      "Iter:  2948  Loss on train set:  -103.21985834565336\n",
      "Iter:  2949  Loss on train set:  -103.1898105358457\n",
      "Iter:  2950  Loss on train set:  -103.22138468968237\n",
      "Iter:  2951  Loss on train set:  -103.24300816761988\n",
      "Iter:  2952  Loss on train set:  -103.27005154389872\n",
      "Iter:  2953  Loss on train set:  -103.28372669429893\n",
      "Iter:  2954  Loss on train set:  -103.28795981106444\n",
      "Iter:  2955  Loss on train set:  -103.28604813399666\n",
      "Iter:  2956  Loss on train set:  -103.28024787562234\n",
      "Iter:  2957  Loss on train set:  -103.26760223926084\n",
      "Iter:  2958  Loss on train set:  -103.25183207374255\n",
      "Iter:  2959  Loss on train set:  -103.29409044411558\n",
      "Iter:  2960  Loss on train set:  -103.34124686863284\n",
      "Iter:  2961  Loss on train set:  -103.42356464854616\n",
      "Iter:  2962  Loss on train set:  -103.47576195559898\n",
      "Iter:  2963  Loss on train set:  -103.511368166497\n",
      "Iter:  2964  Loss on train set:  -103.53013048154472\n",
      "Iter:  2965  Loss on train set:  -103.54528271480692\n",
      "Iter:  2966  Loss on train set:  -103.54644773513793\n",
      "Iter:  2967  Loss on train set:  -103.5493107466225\n",
      "Iter:  2968  Loss on train set:  -103.54887956372475\n",
      "Iter:  2969  Loss on train set:  -103.54691384750608\n",
      "Iter:  2970  Loss on train set:  -103.53197615595583\n",
      "Iter:  2971  Loss on train set:  -103.50801286255793\n",
      "Iter:  2972  Loss on train set:  -103.47561041122361\n",
      "Iter:  2973  Loss on train set:  -103.46140596408607\n",
      "Iter:  2974  Loss on train set:  -103.43628377670134\n",
      "Iter:  2975  Loss on train set:  -103.4083211594893\n",
      "Iter:  2976  Loss on train set:  -103.38243438938673\n",
      "Iter:  2977  Loss on train set:  -103.37175307719583\n",
      "Iter:  2978  Loss on train set:  -103.3482594997856\n",
      "Iter:  2979  Loss on train set:  -103.31595900631841\n",
      "Iter:  2980  Loss on train set:  -103.28332677422995\n",
      "Iter:  2981  Loss on train set:  -103.25304174430252\n",
      "Iter:  2982  Loss on train set:  -103.23332188457064\n",
      "Iter:  2983  Loss on train set:  -103.2399187097459\n",
      "Iter:  2984  Loss on train set:  -103.23462533603393\n",
      "Iter:  2985  Loss on train set:  -103.22498992815784\n",
      "Iter:  2986  Loss on train set:  -103.21501648113541\n",
      "Iter:  2987  Loss on train set:  -103.19902638730295\n",
      "Iter:  2988  Loss on train set:  -103.18343234830081\n",
      "Iter:  2989  Loss on train set:  -103.19912405082377\n",
      "Iter:  2990  Loss on train set:  -103.20580501684135\n",
      "Iter:  2991  Loss on train set:  -103.20711927309381\n",
      "Iter:  2992  Loss on train set:  -103.20723376380917\n",
      "Iter:  2993  Loss on train set:  -103.20202277279624\n",
      "Iter:  2994  Loss on train set:  -103.19373545832657\n",
      "Iter:  2995  Loss on train set:  -103.18277005902121\n",
      "Iter:  2996  Loss on train set:  -103.19552004035144\n",
      "Iter:  2997  Loss on train set:  -103.1998831903133\n",
      "Iter:  2998  Loss on train set:  -103.20206213679143\n",
      "Iter:  2999  Loss on train set:  -103.21686075513969\n",
      "Iter:  3000  Loss on train set:  -103.22998959635011\n",
      "     fun: -103.22677088760014\n",
      "     jac: array([-3.10326982, -3.10326982, -3.10326982,  3.10326982, -3.10326982,\n",
      "       -3.10326982, -3.10326982, -3.10326982,  3.10326982, -3.10326982,\n",
      "       -3.10326982, -3.10326982, -3.10326982, -3.10326982,  3.10326982,\n",
      "        3.10326982, -3.10326982,  3.10326982,  3.10326982,  3.10326982,\n",
      "        3.10326982,  3.10326982,  3.10326982, -3.10326982, -3.10326982,\n",
      "        3.10326982, -3.10326982,  3.10326982, -3.10326982,  3.10326982,\n",
      "        3.10326982,  3.10326982])\n",
      "    nfev: 200\n",
      "     nit: 200\n",
      " success: True\n",
      "       x: array([-1.83185681e-01,  2.63118856e-01,  6.45155828e-02, -5.42698492e-01,\n",
      "       -2.29468130e-03,  4.66306979e-01,  5.46614440e-01, -1.07135107e-01,\n",
      "        7.57994320e-01,  5.35414112e-01, -2.46150026e+00,  9.60575952e-02,\n",
      "        8.59379666e-01, -1.57139523e-02,  1.32359451e+00,  5.00114222e-02,\n",
      "        7.26786645e-02,  1.30913688e-01,  1.04994829e-01,  3.85877276e-01,\n",
      "       -7.16341708e-02, -2.88685829e-01,  6.80151238e-01, -6.76299759e-01,\n",
      "       -6.12000681e-02,  2.08147697e-01,  8.87699812e-01, -1.28466114e+00,\n",
      "        6.41229868e-02, -2.22484963e-01,  6.88620257e-02,  6.75148315e-02])\n",
      "Unfrozen params:  32  Frozen params:  64\n",
      "Iter:  3001  Loss on train set:  -103.22677088760014\n",
      "Iter:  3002  Loss on train set:  -103.14170388285052\n",
      "Iter:  3003  Loss on train set:  -102.91063692626915\n",
      "Iter:  3004  Loss on train set:  -102.6181929257869\n",
      "Iter:  3005  Loss on train set:  -102.42007900854945\n",
      "Iter:  3006  Loss on train set:  -102.48549531474362\n",
      "Iter:  3007  Loss on train set:  -102.47230794217549\n",
      "Iter:  3008  Loss on train set:  -102.62010681310696\n",
      "Iter:  3009  Loss on train set:  -102.68707341949091\n",
      "Iter:  3010  Loss on train set:  -102.68066222117443\n",
      "Iter:  3011  Loss on train set:  -102.66314736680835\n",
      "Iter:  3012  Loss on train set:  -102.62944970996438\n",
      "Iter:  3013  Loss on train set:  -102.61970918780034\n",
      "Iter:  3014  Loss on train set:  -102.64073391421124\n",
      "Iter:  3015  Loss on train set:  -102.69087549031856\n",
      "Iter:  3016  Loss on train set:  -102.67986619516907\n",
      "Iter:  3017  Loss on train set:  -102.62766963513299\n",
      "Iter:  3018  Loss on train set:  -102.68058720510479\n",
      "Iter:  3019  Loss on train set:  -102.71259622937635\n",
      "Iter:  3020  Loss on train set:  -102.75378472601703\n",
      "Iter:  3021  Loss on train set:  -102.76336167635196\n",
      "Iter:  3022  Loss on train set:  -102.75177815714224\n",
      "Iter:  3023  Loss on train set:  -102.78843609347341\n",
      "Iter:  3024  Loss on train set:  -102.82822973687573\n",
      "Iter:  3025  Loss on train set:  -102.87846007726304\n",
      "Iter:  3026  Loss on train set:  -102.94389355776337\n",
      "Iter:  3027  Loss on train set:  -102.98410555686871\n",
      "Iter:  3028  Loss on train set:  -103.01822305296537\n",
      "Iter:  3029  Loss on train set:  -103.0692233992984\n",
      "Iter:  3030  Loss on train set:  -103.11323774593741\n",
      "Iter:  3031  Loss on train set:  -103.14660211338287\n",
      "Iter:  3032  Loss on train set:  -103.22094375630543\n",
      "Iter:  3033  Loss on train set:  -103.26996303540858\n",
      "Iter:  3034  Loss on train set:  -103.30001579180728\n",
      "Iter:  3035  Loss on train set:  -103.31674879164096\n",
      "Iter:  3036  Loss on train set:  -103.32917634862795\n",
      "Iter:  3037  Loss on train set:  -103.33635839610665\n",
      "Iter:  3038  Loss on train set:  -103.40317170899344\n",
      "Iter:  3039  Loss on train set:  -103.4375734385994\n",
      "Iter:  3040  Loss on train set:  -103.45446511149775\n",
      "Iter:  3041  Loss on train set:  -103.46263481125872\n",
      "Iter:  3042  Loss on train set:  -103.46175426952345\n",
      "Iter:  3043  Loss on train set:  -103.45454818912732\n",
      "Iter:  3044  Loss on train set:  -103.44177486304012\n",
      "Iter:  3045  Loss on train set:  -103.42425045680567\n",
      "Iter:  3046  Loss on train set:  -103.40535788711617\n",
      "Iter:  3047  Loss on train set:  -103.40751994774408\n",
      "Iter:  3048  Loss on train set:  -103.43266549050081\n",
      "Iter:  3049  Loss on train set:  -103.44991471848263\n",
      "Iter:  3050  Loss on train set:  -103.47252254045985\n",
      "Iter:  3051  Loss on train set:  -103.47962430858672\n",
      "Iter:  3052  Loss on train set:  -103.47233171974172\n",
      "Iter:  3053  Loss on train set:  -103.47840927447515\n",
      "Iter:  3054  Loss on train set:  -103.54418182658401\n",
      "Iter:  3055  Loss on train set:  -103.59241281167728\n",
      "Iter:  3056  Loss on train set:  -103.67217653556176\n",
      "Iter:  3057  Loss on train set:  -103.72676437839075\n",
      "Iter:  3058  Loss on train set:  -103.77553458242669\n",
      "Iter:  3059  Loss on train set:  -103.80453686333219\n",
      "Iter:  3060  Loss on train set:  -103.81982943648212\n",
      "Iter:  3061  Loss on train set:  -103.82832286624689\n",
      "Iter:  3062  Loss on train set:  -103.83053212071327\n",
      "Iter:  3063  Loss on train set:  -103.82469035425083\n",
      "Iter:  3064  Loss on train set:  -103.81347613945427\n",
      "Iter:  3065  Loss on train set:  -103.80468803147217\n",
      "Iter:  3066  Loss on train set:  -103.79189467744224\n",
      "Iter:  3067  Loss on train set:  -103.79227808372788\n",
      "Iter:  3068  Loss on train set:  -103.78353049492668\n",
      "Iter:  3069  Loss on train set:  -103.78564541163696\n",
      "Iter:  3070  Loss on train set:  -103.79283794315786\n",
      "Iter:  3071  Loss on train set:  -103.77909661348163\n",
      "Iter:  3072  Loss on train set:  -103.75368955819394\n",
      "Iter:  3073  Loss on train set:  -103.73106818999108\n",
      "Iter:  3074  Loss on train set:  -103.70058861182281\n",
      "Iter:  3075  Loss on train set:  -103.66425950118865\n",
      "Iter:  3076  Loss on train set:  -103.62483865000108\n",
      "Iter:  3077  Loss on train set:  -103.58504096757781\n",
      "Iter:  3078  Loss on train set:  -103.57348163000044\n",
      "Iter:  3079  Loss on train set:  -103.55958963645125\n",
      "Iter:  3080  Loss on train set:  -103.54358946500116\n",
      "Iter:  3081  Loss on train set:  -103.53279424228386\n",
      "Iter:  3082  Loss on train set:  -103.5304669583492\n",
      "Iter:  3083  Loss on train set:  -103.54886550445163\n",
      "Iter:  3084  Loss on train set:  -103.56357173365468\n",
      "Iter:  3085  Loss on train set:  -103.58592439908325\n",
      "Iter:  3086  Loss on train set:  -103.61786605720562\n",
      "Iter:  3087  Loss on train set:  -103.63344879352465\n",
      "Iter:  3088  Loss on train set:  -103.6346853887056\n",
      "Iter:  3089  Loss on train set:  -103.65831478584542\n",
      "Iter:  3090  Loss on train set:  -103.66409699067678\n",
      "Iter:  3091  Loss on train set:  -103.65655845186221\n",
      "Iter:  3092  Loss on train set:  -103.72599028729873\n",
      "Iter:  3093  Loss on train set:  -103.7751429936784\n",
      "Iter:  3094  Loss on train set:  -103.80471929803461\n",
      "Iter:  3095  Loss on train set:  -103.82488949470515\n",
      "Iter:  3096  Loss on train set:  -103.83962094742543\n",
      "Iter:  3097  Loss on train set:  -103.85063591478774\n",
      "Iter:  3098  Loss on train set:  -103.87272910880777\n",
      "Iter:  3099  Loss on train set:  -103.89777070566016\n",
      "Iter:  3100  Loss on train set:  -103.93977355622937\n",
      "Iter:  3101  Loss on train set:  -103.95849992818728\n",
      "Iter:  3102  Loss on train set:  -103.96282687425224\n",
      "Iter:  3103  Loss on train set:  -103.95453453101595\n",
      "Iter:  3104  Loss on train set:  -103.93422916883847\n",
      "Iter:  3105  Loss on train set:  -103.90801892412256\n",
      "Iter:  3106  Loss on train set:  -103.8751599285549\n",
      "Iter:  3107  Loss on train set:  -103.87257425442698\n",
      "Iter:  3108  Loss on train set:  -103.87673859653763\n",
      "Iter:  3109  Loss on train set:  -103.872475470547\n",
      "Iter:  3110  Loss on train set:  -103.86207476079271\n",
      "Iter:  3111  Loss on train set:  -103.84713005865353\n",
      "Iter:  3112  Loss on train set:  -103.84072159702778\n",
      "Iter:  3113  Loss on train set:  -103.85522789193185\n",
      "Iter:  3114  Loss on train set:  -103.85937559699094\n",
      "Iter:  3115  Loss on train set:  -103.86541320626057\n",
      "Iter:  3116  Loss on train set:  -103.88754970121158\n",
      "Iter:  3117  Loss on train set:  -103.89536452250559\n",
      "Iter:  3118  Loss on train set:  -103.89106239313543\n",
      "Iter:  3119  Loss on train set:  -103.87501932935942\n",
      "Iter:  3120  Loss on train set:  -103.84200169643154\n",
      "Iter:  3121  Loss on train set:  -103.79706911283361\n",
      "Iter:  3122  Loss on train set:  -103.7511977681783\n",
      "Iter:  3123  Loss on train set:  -103.7142273732753\n",
      "Iter:  3124  Loss on train set:  -103.67800592205265\n",
      "Iter:  3125  Loss on train set:  -103.6390221921867\n",
      "Iter:  3126  Loss on train set:  -103.60135366204936\n",
      "Iter:  3127  Loss on train set:  -103.59831864702856\n",
      "Iter:  3128  Loss on train set:  -103.59713434098866\n",
      "Iter:  3129  Loss on train set:  -103.59516919662033\n",
      "Iter:  3130  Loss on train set:  -103.59335722880861\n",
      "Iter:  3131  Loss on train set:  -103.59320723664163\n",
      "Iter:  3132  Loss on train set:  -103.65301855953025\n",
      "Iter:  3133  Loss on train set:  -103.71993009410602\n",
      "Iter:  3134  Loss on train set:  -103.76504619867387\n",
      "Iter:  3135  Loss on train set:  -103.7931941407532\n",
      "Iter:  3136  Loss on train set:  -103.8108590030162\n",
      "Iter:  3137  Loss on train set:  -103.83262354121902\n",
      "Iter:  3138  Loss on train set:  -103.84147604806225\n",
      "Iter:  3139  Loss on train set:  -103.84158804926123\n",
      "Iter:  3140  Loss on train set:  -103.86633005027966\n",
      "Iter:  3141  Loss on train set:  -103.87404542632297\n",
      "Iter:  3142  Loss on train set:  -103.8834543213338\n",
      "Iter:  3143  Loss on train set:  -103.88934379138905\n",
      "Iter:  3144  Loss on train set:  -103.8995983486765\n",
      "Iter:  3145  Loss on train set:  -103.90971908376584\n",
      "Iter:  3146  Loss on train set:  -103.92327032844524\n",
      "Iter:  3147  Loss on train set:  -103.94125687405291\n",
      "Iter:  3148  Loss on train set:  -103.9524875983551\n",
      "Iter:  3149  Loss on train set:  -103.96222450598182\n",
      "Iter:  3150  Loss on train set:  -103.97223353159751\n",
      "Iter:  3151  Loss on train set:  -103.97592734362482\n",
      "Iter:  3152  Loss on train set:  -103.97164835367239\n",
      "Iter:  3153  Loss on train set:  -103.95942419961686\n",
      "Iter:  3154  Loss on train set:  -103.95143753908158\n",
      "Iter:  3155  Loss on train set:  -103.94359054330455\n",
      "Iter:  3156  Loss on train set:  -103.95527167280596\n",
      "Iter:  3157  Loss on train set:  -103.95275651238757\n",
      "Iter:  3158  Loss on train set:  -103.93879885107835\n",
      "Iter:  3159  Loss on train set:  -103.92355499930655\n",
      "Iter:  3160  Loss on train set:  -103.91128061255024\n",
      "Iter:  3161  Loss on train set:  -103.8991293467903\n",
      "Iter:  3162  Loss on train set:  -103.89110846694723\n",
      "Iter:  3163  Loss on train set:  -103.92750826833557\n",
      "Iter:  3164  Loss on train set:  -103.95058212668447\n",
      "Iter:  3165  Loss on train set:  -103.99231580942919\n",
      "Iter:  3166  Loss on train set:  -104.03422877646699\n",
      "Iter:  3167  Loss on train set:  -104.06657371274311\n",
      "Iter:  3168  Loss on train set:  -104.08828869203764\n",
      "Iter:  3169  Loss on train set:  -104.10157285782128\n",
      "Iter:  3170  Loss on train set:  -104.10832772062558\n",
      "Iter:  3171  Loss on train set:  -104.10931660020195\n",
      "Iter:  3172  Loss on train set:  -104.10641720984427\n",
      "Iter:  3173  Loss on train set:  -104.1024896163735\n",
      "Iter:  3174  Loss on train set:  -104.09698923523968\n",
      "Iter:  3175  Loss on train set:  -104.09141485335434\n",
      "Iter:  3176  Loss on train set:  -104.11778402911293\n",
      "Iter:  3177  Loss on train set:  -104.1328547927507\n",
      "Iter:  3178  Loss on train set:  -104.13998002929105\n",
      "Iter:  3179  Loss on train set:  -104.15543732394694\n",
      "Iter:  3180  Loss on train set:  -104.16340688935833\n",
      "Iter:  3181  Loss on train set:  -104.16274651045968\n",
      "Iter:  3182  Loss on train set:  -104.15993289156651\n",
      "Iter:  3183  Loss on train set:  -104.14983207732404\n",
      "Iter:  3184  Loss on train set:  -104.13934842856892\n",
      "Iter:  3185  Loss on train set:  -104.15032531575744\n",
      "Iter:  3186  Loss on train set:  -104.15889309163619\n",
      "Iter:  3187  Loss on train set:  -104.17056965182037\n",
      "Iter:  3188  Loss on train set:  -104.17420452735006\n",
      "Iter:  3189  Loss on train set:  -104.1781656735528\n",
      "Iter:  3190  Loss on train set:  -104.17612664686175\n",
      "Iter:  3191  Loss on train set:  -104.16782342510005\n",
      "Iter:  3192  Loss on train set:  -104.17275050366231\n",
      "Iter:  3193  Loss on train set:  -104.16656009237512\n",
      "Iter:  3194  Loss on train set:  -104.15191511562168\n",
      "Iter:  3195  Loss on train set:  -104.1403776245195\n",
      "Iter:  3196  Loss on train set:  -104.12149114241438\n",
      "Iter:  3197  Loss on train set:  -104.11728838184715\n",
      "Iter:  3198  Loss on train set:  -104.10909510860276\n",
      "Iter:  3199  Loss on train set:  -104.0970714677153\n",
      "Iter:  3200  Loss on train set:  -104.1166821080124\n",
      "     fun: -104.132440734646\n",
      "     jac: array([-2.66862276, -2.66862276, -2.66862276, -2.66862276, -2.66862276,\n",
      "        2.66862276, -2.66862276, -2.66862276, -2.66862276, -2.66862276,\n",
      "        2.66862276,  2.66862276, -2.66862276,  2.66862276, -2.66862276,\n",
      "       -2.66862276,  2.66862276,  2.66862276, -2.66862276,  2.66862276,\n",
      "        2.66862276,  2.66862276, -2.66862276,  2.66862276,  2.66862276,\n",
      "        2.66862276,  2.66862276, -2.66862276, -2.66862276, -2.66862276,\n",
      "       -2.66862276,  2.66862276])\n",
      "    nfev: 200\n",
      "     nit: 200\n",
      " success: True\n",
      "       x: array([ 1.48537796,  0.78155622, -0.19856996, -0.74513989,  1.73817048,\n",
      "       -0.4898067 ,  0.58921621,  0.01874217, -0.35750916, -0.08176435,\n",
      "       -1.51635004, -0.42781717, -1.19166845, -0.80128932, -0.62331406,\n",
      "       -0.04564506,  0.27353567, -0.79084373, -0.22014954, -0.51567603,\n",
      "        0.15242781,  0.77312395, -0.81465076,  0.63812434, -0.59918742,\n",
      "       -0.32705013, -0.61832737,  1.22719698, -0.01977072, -1.03454332,\n",
      "        0.55451731, -0.99543496])\n"
     ]
    }
   ],
   "source": [
    "def staged_meta_vqe_energy_loss(circuit_params, qubits, frozen_circuit_params,insertion_pointer, hamilt_params_training_set, n_meas_reps=1000):\n",
    "    \"\"\"\n",
    "    Compute the energy loss of the meta_vqe circuit for a staged meta-vqe, i.e. a meta-vqe trained layer-by-layer.\n",
    "    \n",
    "    Args:\n",
    "    ----------------\n",
    "        circuit_params [ndarray]: array of the parameters of the circuit\n",
    "        qubits [list]: list of qubits\n",
    "        frozen_circuit_params [ndarray]: array of the parameters of the frozen part of the circuit\n",
    "        insertion_pointer [int]: index of the insertion point of the circuit_params\n",
    "        hamilt_params_training_set [list]: list of the parameters of the hamiltonians\n",
    "        n_meas_reps [int]: number of measurement repetitions\n",
    "    \n",
    "    Returns:\n",
    "    ----------------\n",
    "        energy_loss [float]: energy loss of the meta_vqe circuit\n",
    "    \"\"\"\n",
    "    \n",
    "    energy_loss = 0\n",
    "    \n",
    "    #start_time = time.time()\n",
    "    for hamilt_params_sample in hamilt_params_training_set:\n",
    "        full_circuit_params = np.insert(frozen_circuit_params, insertion_pointer, circuit_params)\n",
    "        #energy_loss += meas_hamilt_expectation(qubits, circuit_params, hamilt_params_sample, n_meas_reps)\n",
    "        energy_loss += direct_hamilt_expectation(qubits, full_circuit_params, hamilt_params_sample)\n",
    "    \n",
    "    #print(\"Time taken for the meta_vqe loss evaluation: \", time.time() - start_time)\n",
    "    \n",
    "    return energy_loss\n",
    "\n",
    "def train_staged_meta_vqe(qubits, hamilt_params_training_set, init_circuit_params=None, params_init_mode=\"normal_random\", n_meas_reps=1000):\n",
    "    \"\"\"\n",
    "    Train the meta_vqe circuit for a staged meta-vqe, i.e. a meta-vqe trained layer-by-layer.\n",
    "    \n",
    "    Args:\n",
    "    ----------------\n",
    "        qubits [list]: list of qubits\n",
    "        hamilt_params_training_set [list]: list of the parameters of the hamiltonians\n",
    "        init_circuit_params [ndarray]: array of the parameters of the circuit\n",
    "        params_init_mode [str]: mode of initialization of the circuit parameters\n",
    "        n_meas_reps [int]: number of measurement repetitions\n",
    "        \n",
    "    Returns:\n",
    "    ----------------\n",
    "        opt_circuit_params [ndarray]: array of the parameters of the optimized circuit\n",
    "    \"\"\"\n",
    "    \n",
    "    if init_circuit_params is None:\n",
    "        init_circuit_params = initialize_circuit_params(qubits, params_init_mode)\n",
    "\n",
    "\n",
    "    num_var_encoding_layers, num_var_processing_layers = 2, 2 #Default for the paper PQC\n",
    "    params_per_processing_layer = 2 * len(qubits)\n",
    "    params_per_encoding_layer = 2 * params_per_processing_layer\n",
    "    layer_sizes =  [params_per_encoding_layer for i in range(num_var_encoding_layers)] + [params_per_processing_layer for i in range(num_var_processing_layers)]\n",
    "    print(\"Layer sizes: \", layer_sizes)\n",
    "\n",
    "    print(\"Train loss should converge to \", sum(exact_hamilt_GS_energy(qubits, hamilt_params_training_set[i]) for i in range(len(hamilt_params_training_set))))\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "\n",
    "    meta_vqe_loss_grad_fun = prepare_loss_gradient_approx(staged_meta_vqe_energy_loss, stochastic_approx=True, stoch_shift_size_c=0.01)\n",
    "\n",
    "\n",
    "    optim_circuit_params = init_circuit_params\n",
    "    for repetit in range(4):\n",
    "        print(\"Running repetition: \", repetit)\n",
    "        for lay_i in reversed(range(len(layer_sizes))):\n",
    "            \n",
    "            unfreeze_steps = 200\n",
    "            \n",
    "            insertion_pointer = sum(layer_sizes[:lay_i]) #Index of the first trainable parameter in the complete circuit parameter vector\n",
    "            frozen_circuit_params = np.concatenate([optim_circuit_params[:insertion_pointer], optim_circuit_params[insertion_pointer + layer_sizes[lay_i]:]])\n",
    "            trainable_circuit_params = optim_circuit_params[insertion_pointer:insertion_pointer + layer_sizes[lay_i]]\n",
    "            print(\"Unfrozen params: \", len(trainable_circuit_params), \" Frozen params: \", len(frozen_circuit_params))\n",
    "            \n",
    "            def callbackF(Xi):\n",
    "                global Nfeval\n",
    "                print(\"Iter: \", Nfeval, \" Loss on train set: \", staged_meta_vqe_energy_loss(Xi, qubits, frozen_circuit_params, insertion_pointer, hamilt_params_training_set, n_meas_reps=1000))\n",
    "                Nfeval += 1\n",
    "            \n",
    "            loss_args = (qubits, frozen_circuit_params, insertion_pointer, hamilt_params_training_set, n_meas_reps)\n",
    "            \n",
    "            \n",
    "            optim_result = minimize(staged_meta_vqe_energy_loss,\n",
    "                                    args=loss_args, \n",
    "                                    x0=trainable_circuit_params, \n",
    "                                    method=adam, #'L-BFGS-B', #'COBYLA', #adam_with_lr_decay,,#'COBYLA', \n",
    "                                    callback=callbackF,\n",
    "                                    jac=meta_vqe_loss_grad_fun, #None,\n",
    "                                    options={'maxiter': unfreeze_steps},\n",
    "                                    )\n",
    "            print(optim_result)\n",
    "            optim_circuit_params = np.insert(frozen_circuit_params,insertion_pointer, optim_result.x)        \n",
    "        \n",
    "    #print(optim_result.fun)\n",
    "    return optim_circuit_params\n",
    "\n",
    "def staged_meta_vqe(qubits, hamilt_params_training_set, hamilt_params_test_set):\n",
    "    \"\"\"\n",
    "    Convenience function for training and evaluating a staged meta-vqe, i.e. a meta-vqe trained layer-by-layer.\n",
    "    \n",
    "    Args:\n",
    "    ----------------\n",
    "        qubits [list]: list of qubits\n",
    "        hamilt_params_training_set [list]: list of the parameters of the hamiltonians\n",
    "        hamilt_params_test_set [list]: list of the parameters of the hamiltonians\n",
    "        \n",
    "    Returns:\n",
    "    ----------------\n",
    "        energy_expectations [list]: list of the energy expectations\n",
    "        abs_energy_errors [list]: list of the absolute energy errors\n",
    "    \"\"\"\n",
    "    \n",
    "    #Train meta-VQE:\n",
    "    opt_circuit_params = train_staged_meta_vqe(qubits, hamilt_params_training_set, params_init_mode=\"normal_random\", n_meas_reps=1000)\n",
    "    \n",
    "    #Evaluate meta-VQE on test set:\n",
    "    #energy_expectations, abs_energy_errors = evaluate_meta_vqe(qubits, opt_circuit_params, hamilt_params_test_set, n_meas_reps=1000)\n",
    "    \n",
    "    ##return energy_expectations, abs_energy_errors\n",
    "\n",
    "\n",
    "qubits = cirq.LineQubit.range(8)\n",
    "\n",
    "Nfeval = 1  \n",
    "\n",
    "train_set_size = 10\n",
    "min_hamilt_params = {\"lambda\": 0.75, \"Delta\": -1.1}\n",
    "max_hamilt_params = {\"lambda\": 0.75, \"Delta\": 1.1}\n",
    "\n",
    "train_set = construct_training_set(train_set_size=train_set_size, mode=\"equidistant\")\n",
    "test_set = construct_test_set(test_set_size=100, mode=\"uniform_random\", min_hamilt_params=min_hamilt_params, max_hamilt_params=max_hamilt_params)\n",
    "\n",
    "staged_meta_vqe(qubits, train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss should converge to  -65.56662081055886\n",
      "Starting training...\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1  Loss on train set:  29.848294994033672\n",
      "Iter:  2  Loss on train set:  20.67942723111402\n",
      "Iter:  3  Loss on train set:  19.03461001974393\n",
      "Iter:  4  Loss on train set:  19.034599333402895\n",
      "Iter:  5  Loss on train set:  21.241273359923063\n",
      "Iter:  6  Loss on train set:  19.034597154249738\n",
      "Iter:  7  Loss on train set:  19.03459618306855\n",
      "Iter:  8  Loss on train set:  19.03643008354366\n",
      "Iter:  9  Loss on train set:  19.034594614883904\n",
      "Iter:  10  Loss on train set:  8.356113272396618\n",
      "Iter:  11  Loss on train set:  1.666650185429389\n",
      "Iter:  12  Loss on train set:  -1.0119488578395919\n",
      "Iter:  13  Loss on train set:  -2.3579173912451346\n",
      "Iter:  14  Loss on train set:  -4.329720804531213\n",
      "Iter:  15  Loss on train set:  10.724485917052926\n",
      "Iter:  16  Loss on train set:  -4.325915153598629\n",
      "Iter:  17  Loss on train set:  -16.115202673701855\n",
      "Iter:  18  Loss on train set:  -28.4491817847101\n",
      "Iter:  19  Loss on train set:  -33.21124851403562\n",
      "Iter:  20  Loss on train set:  -28.83240464624062\n",
      "Iter:  21  Loss on train set:  -33.62505665335563\n",
      "Iter:  22  Loss on train set:  -32.26246380224717\n",
      "Iter:  23  Loss on train set:  -31.47396447272233\n",
      "Iter:  24  Loss on train set:  -30.91067738616118\n",
      "Iter:  25  Loss on train set:  -32.58808390735276\n",
      "Iter:  26  Loss on train set:  -25.180181661259933\n",
      "Iter:  27  Loss on train set:  -33.6149472966834\n",
      "Iter:  28  Loss on train set:  -21.082066368625934\n",
      "Iter:  29  Loss on train set:  -36.952919391806816\n",
      "Iter:  30  Loss on train set:  -26.286113408528013\n",
      "Iter:  31  Loss on train set:  -37.7436875059765\n",
      "Iter:  32  Loss on train set:  -33.828081968066954\n",
      "Iter:  33  Loss on train set:  -38.65894335845794\n",
      "Iter:  34  Loss on train set:  -38.74106656011831\n",
      "Iter:  35  Loss on train set:  -37.66393556249624\n",
      "Iter:  36  Loss on train set:  -38.782326440521004\n",
      "Iter:  37  Loss on train set:  -37.57553488816771\n",
      "Iter:  38  Loss on train set:  -38.63791822478443\n",
      "Iter:  39  Loss on train set:  -36.98935397289749\n",
      "Iter:  40  Loss on train set:  -38.19396452960149\n",
      "Iter:  41  Loss on train set:  -38.44968049440675\n",
      "Iter:  42  Loss on train set:  -38.484376069748976\n",
      "Iter:  43  Loss on train set:  -38.618488498107304\n",
      "Iter:  44  Loss on train set:  -38.85060195430629\n",
      "Iter:  45  Loss on train set:  -38.91777602400329\n",
      "Iter:  46  Loss on train set:  -38.871575024061826\n",
      "Iter:  47  Loss on train set:  -39.00164756411096\n",
      "Iter:  48  Loss on train set:  -38.80750804175442\n",
      "Iter:  49  Loss on train set:  -39.01566371627217\n",
      "Iter:  50  Loss on train set:  -39.077223814559716\n",
      "Iter:  51  Loss on train set:  -39.01566371627217\n",
      "     fun: -39.01566371627217\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 3.42921953e-01,  3.18848897e+00,  3.93401361e+00,  1.11020616e+00,\n",
      "       -1.12978045e+00,  1.15665861e+00,  1.36885571e+00,  1.29086177e-03,\n",
      "        9.21309259e-01])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  52  Loss on train set:  -34.48793064031567\n",
      "Iter:  53  Loss on train set:  -39.15146736256149\n",
      "Iter:  54  Loss on train set:  -39.22290789064992\n",
      "Iter:  55  Loss on train set:  -37.143018372144155\n",
      "Iter:  56  Loss on train set:  -35.27015426760535\n",
      "Iter:  57  Loss on train set:  -24.745621442239607\n",
      "Iter:  58  Loss on train set:  -24.663650507559947\n",
      "Iter:  59  Loss on train set:  -35.273326335987626\n",
      "Iter:  60  Loss on train set:  -37.08684393479913\n",
      "Iter:  61  Loss on train set:  -27.39172405601239\n",
      "Iter:  62  Loss on train set:  -37.805223467728084\n",
      "Iter:  63  Loss on train set:  -38.48325974365437\n",
      "Iter:  64  Loss on train set:  -38.37852596392495\n",
      "Iter:  65  Loss on train set:  -39.26807529423103\n",
      "Iter:  66  Loss on train set:  -38.48882465533058\n",
      "Iter:  67  Loss on train set:  -39.31020925024282\n",
      "Iter:  68  Loss on train set:  -38.73600861179296\n",
      "Iter:  69  Loss on train set:  -39.24038404047503\n",
      "Iter:  70  Loss on train set:  -36.80836947991742\n",
      "Iter:  71  Loss on train set:  -39.062352735405405\n",
      "Iter:  72  Loss on train set:  -38.246839942182525\n",
      "Iter:  73  Loss on train set:  -38.96517439154814\n",
      "Iter:  74  Loss on train set:  -39.50375946814863\n",
      "Iter:  75  Loss on train set:  -39.2325293175914\n",
      "Iter:  76  Loss on train set:  -39.58457121961449\n",
      "Iter:  77  Loss on train set:  -39.58142605997997\n",
      "Iter:  78  Loss on train set:  -39.456439884766816\n",
      "Iter:  79  Loss on train set:  -39.56934804934477\n",
      "Iter:  80  Loss on train set:  -39.58866156984498\n",
      "Iter:  81  Loss on train set:  -39.59890369838956\n",
      "Iter:  82  Loss on train set:  -39.59164908173287\n",
      "Iter:  83  Loss on train set:  -39.57630635010347\n",
      "Iter:  84  Loss on train set:  -39.84175318692708\n",
      "Iter:  85  Loss on train set:  -39.747663880150434\n",
      "Iter:  86  Loss on train set:  -39.73109186064595\n",
      "Iter:  87  Loss on train set:  -39.84780202600445\n",
      "Iter:  88  Loss on train set:  -39.83112251352421\n",
      "Iter:  89  Loss on train set:  -39.781840731092394\n",
      "Iter:  90  Loss on train set:  -39.833844209988555\n",
      "Iter:  91  Loss on train set:  -39.87998230013062\n",
      "Iter:  92  Loss on train set:  -39.884848862673316\n",
      "Iter:  93  Loss on train set:  -39.89213347357701\n",
      "Iter:  94  Loss on train set:  -39.94317894939134\n",
      "Iter:  95  Loss on train set:  -40.08787342966356\n",
      "Iter:  96  Loss on train set:  -40.20304668476097\n",
      "Iter:  97  Loss on train set:  -40.3176670969396\n",
      "Iter:  98  Loss on train set:  -40.40853393835535\n",
      "Iter:  99  Loss on train set:  -40.459335210447875\n",
      "Iter:  100  Loss on train set:  -40.420697191748054\n",
      "Iter:  101  Loss on train set:  -40.468549620303605\n",
      "Iter:  102  Loss on train set:  -40.459335210447875\n",
      "     fun: -40.459335210447875\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.17868384,  0.89404572,  1.36444012,  0.02670229, -0.19893095,\n",
      "       -0.08255338, -0.22125497,  0.17121202,  0.06770489])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  103  Loss on train set:  -40.445907974587016\n",
      "Iter:  104  Loss on train set:  -36.46970534202188\n",
      "Iter:  105  Loss on train set:  -40.48560115462903\n",
      "Iter:  106  Loss on train set:  -35.533288544919834\n",
      "Iter:  107  Loss on train set:  -32.73730664798739\n",
      "Iter:  108  Loss on train set:  -35.2254135515628\n",
      "Iter:  109  Loss on train set:  -29.236949255907778\n",
      "Iter:  110  Loss on train set:  -35.27431692340643\n",
      "Iter:  111  Loss on train set:  -35.26246471679172\n",
      "Iter:  112  Loss on train set:  -39.83393587612456\n",
      "Iter:  113  Loss on train set:  -40.60463218683392\n",
      "Iter:  114  Loss on train set:  -41.64997233512596\n",
      "Iter:  115  Loss on train set:  -38.68855753267314\n",
      "Iter:  116  Loss on train set:  -41.642143520561916\n",
      "Iter:  117  Loss on train set:  -41.45221777386834\n",
      "Iter:  118  Loss on train set:  -41.262303613874536\n",
      "Iter:  119  Loss on train set:  -39.407896206850744\n",
      "Iter:  120  Loss on train set:  -40.45969107120544\n",
      "Iter:  121  Loss on train set:  -41.24415542753581\n",
      "Iter:  122  Loss on train set:  -41.63849248798135\n",
      "Iter:  123  Loss on train set:  -38.94567347733821\n",
      "Iter:  124  Loss on train set:  -41.125895968933946\n",
      "Iter:  125  Loss on train set:  -41.86384371833035\n",
      "Iter:  126  Loss on train set:  -41.91828757218608\n",
      "Iter:  127  Loss on train set:  -42.48060893616552\n",
      "Iter:  128  Loss on train set:  -42.95613367637497\n",
      "Iter:  129  Loss on train set:  -43.38916721488138\n",
      "Iter:  130  Loss on train set:  -43.573803644209086\n",
      "Iter:  131  Loss on train set:  -43.44646375773257\n",
      "Iter:  132  Loss on train set:  -42.92675426449855\n",
      "Iter:  133  Loss on train set:  -44.631770403520306\n",
      "Iter:  134  Loss on train set:  -45.02779692475083\n",
      "Iter:  135  Loss on train set:  -45.11596378681973\n",
      "Iter:  136  Loss on train set:  -44.96654791079168\n",
      "Iter:  137  Loss on train set:  -45.88823156361746\n",
      "Iter:  138  Loss on train set:  -45.945925199953855\n",
      "Iter:  139  Loss on train set:  -45.90896360984527\n",
      "Iter:  140  Loss on train set:  -46.336045104401286\n",
      "Iter:  141  Loss on train set:  -46.7668019125943\n",
      "Iter:  142  Loss on train set:  -46.32950747338813\n",
      "Iter:  143  Loss on train set:  -46.5887995908682\n",
      "Iter:  144  Loss on train set:  -47.11675846439962\n",
      "Iter:  145  Loss on train set:  -47.15414069083175\n",
      "Iter:  146  Loss on train set:  -47.19993630160138\n",
      "Iter:  147  Loss on train set:  -47.343660666863244\n",
      "Iter:  148  Loss on train set:  -47.457619392200954\n",
      "Iter:  149  Loss on train set:  -47.36002079233193\n",
      "Iter:  150  Loss on train set:  -47.413978820136535\n",
      "Iter:  151  Loss on train set:  -47.46554090684225\n",
      "Iter:  152  Loss on train set:  -47.37062445784519\n",
      "Iter:  153  Loss on train set:  -47.46554090684225\n",
      "     fun: -47.46554090684225\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.08686967, -1.15978758,  2.40179152,  0.16315165, -0.19219053,\n",
      "       -0.62524372, -1.39140662, -0.56938303, -0.66373788])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  154  Loss on train set:  -41.9617972974298\n",
      "Iter:  155  Loss on train set:  -47.465537448118305\n",
      "Iter:  156  Loss on train set:  -35.51300593170161\n",
      "Iter:  157  Loss on train set:  -42.013447630353795\n",
      "Iter:  158  Loss on train set:  -47.46554587724884\n",
      "Iter:  159  Loss on train set:  -47.242942568554085\n",
      "Iter:  160  Loss on train set:  -39.389948357660096\n",
      "Iter:  161  Loss on train set:  -40.04384225430879\n",
      "Iter:  162  Loss on train set:  -36.961599298578975\n",
      "Iter:  163  Loss on train set:  -42.24685782288154\n",
      "Iter:  164  Loss on train set:  -46.389346996940915\n",
      "Iter:  165  Loss on train set:  -47.46553881728004\n",
      "Iter:  166  Loss on train set:  -43.75338906625003\n",
      "Iter:  167  Loss on train set:  -46.25109691823076\n",
      "Iter:  168  Loss on train set:  -47.74648685111495\n",
      "Iter:  169  Loss on train set:  -47.82735864300331\n",
      "Iter:  170  Loss on train set:  -46.50849529220914\n",
      "Iter:  171  Loss on train set:  -47.83031824771194\n",
      "Iter:  172  Loss on train set:  -47.62208363565743\n",
      "Iter:  173  Loss on train set:  -47.83031715990222\n",
      "Iter:  174  Loss on train set:  -47.08287588989312\n",
      "Iter:  175  Loss on train set:  -47.830870868796666\n",
      "Iter:  176  Loss on train set:  -47.7967116665875\n",
      "Iter:  177  Loss on train set:  -47.85749914873027\n",
      "Iter:  178  Loss on train set:  -47.8860307928018\n",
      "Iter:  179  Loss on train set:  -47.68492316927304\n",
      "Iter:  180  Loss on train set:  -47.58242386943907\n",
      "Iter:  181  Loss on train set:  -47.734839890866276\n",
      "Iter:  182  Loss on train set:  -47.792768676845405\n",
      "Iter:  183  Loss on train set:  -47.89585550443123\n",
      "Iter:  184  Loss on train set:  -47.75823062424135\n",
      "Iter:  185  Loss on train set:  -47.93587514013886\n",
      "Iter:  186  Loss on train set:  -47.931284504817825\n",
      "Iter:  187  Loss on train set:  -47.839176029546174\n",
      "Iter:  188  Loss on train set:  -47.903580938440335\n",
      "Iter:  189  Loss on train set:  -47.97092432580811\n",
      "Iter:  190  Loss on train set:  -47.89332889575813\n",
      "Iter:  191  Loss on train set:  -48.00624513739069\n",
      "Iter:  192  Loss on train set:  -48.022243360533736\n",
      "Iter:  193  Loss on train set:  -47.997842211335104\n",
      "Iter:  194  Loss on train set:  -47.99669438831661\n",
      "Iter:  195  Loss on train set:  -48.01617048579762\n",
      "Iter:  196  Loss on train set:  -48.0163708254356\n",
      "Iter:  197  Loss on train set:  -48.02024602056939\n",
      "Iter:  198  Loss on train set:  -48.00812493434751\n",
      "Iter:  199  Loss on train set:  -48.03386946186937\n",
      "Iter:  200  Loss on train set:  -47.99680963604128\n",
      "Iter:  201  Loss on train set:  -48.05198796919528\n",
      "Iter:  202  Loss on train set:  -48.06039313984465\n",
      "Iter:  203  Loss on train set:  -48.067738263308165\n",
      "Iter:  204  Loss on train set:  -48.06039313984465\n",
      "     fun: -48.06039313984465\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.17320996,  0.20440248, -0.16794791,  0.30128773,  2.02674101,\n",
      "       -0.12352843, -0.30067432,  0.00295328, -0.20519659])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  205  Loss on train set:  -46.32974727451016\n",
      "Iter:  206  Loss on train set:  -44.286911889656224\n",
      "Iter:  207  Loss on train set:  -35.211402327504366\n",
      "Iter:  208  Loss on train set:  -44.00042955397075\n",
      "Iter:  209  Loss on train set:  -31.986230136823508\n",
      "Iter:  210  Loss on train set:  -47.35232314090204\n",
      "Iter:  211  Loss on train set:  -46.7854908903064\n",
      "Iter:  212  Loss on train set:  -48.060394059259075\n",
      "Iter:  213  Loss on train set:  -47.928429913658135\n",
      "Iter:  214  Loss on train set:  -37.560132974961064\n",
      "Iter:  215  Loss on train set:  -45.48490245550635\n",
      "Iter:  216  Loss on train set:  -48.05810208743844\n",
      "Iter:  217  Loss on train set:  -46.36624444432859\n",
      "Iter:  218  Loss on train set:  -48.02872139852957\n",
      "Iter:  219  Loss on train set:  -43.290288036949896\n",
      "Iter:  220  Loss on train set:  -48.186421784874724\n",
      "Iter:  221  Loss on train set:  -46.60813458428271\n",
      "Iter:  222  Loss on train set:  -47.94051115116787\n",
      "Iter:  223  Loss on train set:  -46.26645024840024\n",
      "Iter:  224  Loss on train set:  -46.68138863952544\n",
      "Iter:  225  Loss on train set:  -47.06896644019305\n",
      "Iter:  226  Loss on train set:  -48.18642583611029\n",
      "Iter:  227  Loss on train set:  -47.57484802147259\n",
      "Iter:  228  Loss on train set:  -48.20148410930455\n",
      "Iter:  229  Loss on train set:  -48.44482278578652\n",
      "Iter:  230  Loss on train set:  -47.154238882752416\n",
      "Iter:  231  Loss on train set:  -48.62043620211926\n",
      "Iter:  232  Loss on train set:  -48.794913292123546\n",
      "Iter:  233  Loss on train set:  -48.7856061733423\n",
      "Iter:  234  Loss on train set:  -48.827949160629665\n",
      "Iter:  235  Loss on train set:  -48.858474410845666\n",
      "Iter:  236  Loss on train set:  -48.465566485792884\n",
      "Iter:  237  Loss on train set:  -48.88480143349969\n",
      "Iter:  238  Loss on train set:  -48.981617132177796\n",
      "Iter:  239  Loss on train set:  -48.98162057575761\n",
      "Iter:  240  Loss on train set:  -48.98733539818278\n",
      "Iter:  241  Loss on train set:  -48.94433578354165\n",
      "Iter:  242  Loss on train set:  -48.81241152630952\n",
      "Iter:  243  Loss on train set:  -48.96284646379292\n",
      "Iter:  244  Loss on train set:  -48.893969889515674\n",
      "Iter:  245  Loss on train set:  -48.98845932881657\n",
      "Iter:  246  Loss on train set:  -48.98871370381433\n",
      "Iter:  247  Loss on train set:  -48.96558175877199\n",
      "Iter:  248  Loss on train set:  -48.988712468679594\n",
      "Iter:  249  Loss on train set:  -48.940279011902874\n",
      "Iter:  250  Loss on train set:  -48.987291443222404\n",
      "Iter:  251  Loss on train set:  -49.01413763764863\n",
      "Iter:  252  Loss on train set:  -49.04406091130131\n",
      "Iter:  253  Loss on train set:  -49.04570488737458\n",
      "Iter:  254  Loss on train set:  -49.03045837501012\n",
      "Iter:  255  Loss on train set:  -49.04570488737458\n",
      "     fun: -49.04570488737458\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.0704456 ,  0.39580894,  3.03815123,  0.10703581, -0.04671119,\n",
      "       -0.09918832,  0.07087879,  1.39190376,  2.16225168])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  256  Loss on train set:  -49.09127298314358\n",
      "Iter:  257  Loss on train set:  -49.09127410584462\n",
      "Iter:  258  Loss on train set:  -40.20303924964162\n",
      "Iter:  259  Loss on train set:  -39.88748235646712\n",
      "Iter:  260  Loss on train set:  -34.73642457443277\n",
      "Iter:  261  Loss on train set:  -48.873973752999426\n",
      "Iter:  262  Loss on train set:  -46.17641586292429\n",
      "Iter:  263  Loss on train set:  -47.98562415131114\n",
      "Iter:  264  Loss on train set:  -49.09126863554775\n",
      "Iter:  265  Loss on train set:  -40.55801120404875\n",
      "Iter:  266  Loss on train set:  -47.607154576640504\n",
      "Iter:  267  Loss on train set:  -49.08071773311329\n",
      "Iter:  268  Loss on train set:  -47.05273598531787\n",
      "Iter:  269  Loss on train set:  -48.192740326781646\n",
      "Iter:  270  Loss on train set:  -49.09126881089692\n",
      "Iter:  271  Loss on train set:  -48.29605621183415\n",
      "Iter:  272  Loss on train set:  -49.11054377382115\n",
      "Iter:  273  Loss on train set:  -48.869380732369265\n",
      "Iter:  274  Loss on train set:  -49.110544374210484\n",
      "Iter:  275  Loss on train set:  -49.01042315523355\n",
      "Iter:  276  Loss on train set:  -49.2633996464361\n",
      "Iter:  277  Loss on train set:  -49.15368696769046\n",
      "Iter:  278  Loss on train set:  -48.89852267711449\n",
      "Iter:  279  Loss on train set:  -49.26666822397844\n",
      "Iter:  280  Loss on train set:  -49.284683299785605\n",
      "Iter:  281  Loss on train set:  -49.28809138949042\n",
      "Iter:  282  Loss on train set:  -49.215043066641144\n",
      "Iter:  283  Loss on train set:  -49.28811248132339\n",
      "Iter:  284  Loss on train set:  -49.55215909610271\n",
      "Iter:  285  Loss on train set:  -49.423335833398234\n",
      "Iter:  286  Loss on train set:  -49.56268204739102\n",
      "Iter:  287  Loss on train set:  -49.52465898181467\n",
      "Iter:  288  Loss on train set:  -49.56166456003849\n",
      "Iter:  289  Loss on train set:  -49.58606741705343\n",
      "Iter:  290  Loss on train set:  -49.594102896354926\n",
      "Iter:  291  Loss on train set:  -49.59344599544006\n",
      "Iter:  292  Loss on train set:  -49.5895246906792\n",
      "Iter:  293  Loss on train set:  -49.58688707750122\n",
      "Iter:  294  Loss on train set:  -49.57408189902743\n",
      "Iter:  295  Loss on train set:  -49.57368249420968\n",
      "Iter:  296  Loss on train set:  -49.57552413516375\n",
      "Iter:  297  Loss on train set:  -49.595044581540385\n",
      "Iter:  298  Loss on train set:  -49.58640540933777\n",
      "Iter:  299  Loss on train set:  -49.58956445292179\n",
      "Iter:  300  Loss on train set:  -49.57733728316652\n",
      "Iter:  301  Loss on train set:  -49.59271093512959\n",
      "Iter:  302  Loss on train set:  -49.59232302412666\n",
      "Iter:  303  Loss on train set:  -49.58906313864982\n",
      "Iter:  304  Loss on train set:  -49.59536111744993\n",
      "Iter:  305  Loss on train set:  -49.59703434470677\n",
      "Iter:  306  Loss on train set:  -49.59536111744993\n",
      "     fun: -49.59536111744993\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 3.09991739,  2.07912761, -1.26868004, -0.22100596, -0.06322137,\n",
      "        0.7804787 , -0.00665249, -0.14825164,  1.89641504])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  307  Loss on train set:  -39.76047809046095\n",
      "Iter:  308  Loss on train set:  -47.845489137880456\n",
      "Iter:  309  Loss on train set:  -44.43419797656048\n",
      "Iter:  310  Loss on train set:  -37.70441091278454\n",
      "Iter:  311  Loss on train set:  -41.549234160645256\n",
      "Iter:  312  Loss on train set:  -44.17217442968813\n",
      "Iter:  313  Loss on train set:  -36.766682724920365\n",
      "Iter:  314  Loss on train set:  -46.50808117642721\n",
      "Iter:  315  Loss on train set:  -41.354699179877166\n",
      "Iter:  316  Loss on train set:  -39.32371549874297\n",
      "Iter:  317  Loss on train set:  -46.919023409125685\n",
      "Iter:  318  Loss on train set:  -48.75136511152872\n",
      "Iter:  319  Loss on train set:  -49.58231691341972\n",
      "Iter:  320  Loss on train set:  -49.20751736599085\n",
      "Iter:  321  Loss on train set:  -49.37280360349807\n",
      "Iter:  322  Loss on train set:  -48.84442633184465\n",
      "Iter:  323  Loss on train set:  -49.506973924626465\n",
      "Iter:  324  Loss on train set:  -48.89468449618387\n",
      "Iter:  325  Loss on train set:  -49.459190177714504\n",
      "Iter:  326  Loss on train set:  -49.21520670970025\n",
      "Iter:  327  Loss on train set:  -49.54051570809113\n",
      "Iter:  328  Loss on train set:  -49.59732391204531\n",
      "Iter:  329  Loss on train set:  -49.59306036974766\n",
      "Iter:  330  Loss on train set:  -49.55363079105824\n",
      "Iter:  331  Loss on train set:  -49.601630950106724\n",
      "Iter:  332  Loss on train set:  -49.54097577002702\n",
      "Iter:  333  Loss on train set:  -49.58294064469797\n",
      "Iter:  334  Loss on train set:  -49.54536110099305\n",
      "Iter:  335  Loss on train set:  -49.56715803841853\n",
      "Iter:  336  Loss on train set:  -49.64823519064865\n",
      "Iter:  337  Loss on train set:  -49.622951091270025\n",
      "Iter:  338  Loss on train set:  -49.64425948108987\n",
      "Iter:  339  Loss on train set:  -49.60776135896423\n",
      "Iter:  340  Loss on train set:  -49.640818706906714\n",
      "Iter:  341  Loss on train set:  -49.606159448619806\n",
      "Iter:  342  Loss on train set:  -49.64219382452819\n",
      "Iter:  343  Loss on train set:  -49.64432932032164\n",
      "Iter:  344  Loss on train set:  -49.64328900732807\n",
      "Iter:  345  Loss on train set:  -49.644263516779716\n",
      "Iter:  346  Loss on train set:  -49.63748297244824\n",
      "Iter:  347  Loss on train set:  -49.64435593316257\n",
      "Iter:  348  Loss on train set:  -49.646829893011855\n",
      "Iter:  349  Loss on train set:  -49.64938845717277\n",
      "Iter:  350  Loss on train set:  -49.64910961670539\n",
      "Iter:  351  Loss on train set:  -49.649012890298096\n",
      "Iter:  352  Loss on train set:  -49.64990755659463\n",
      "Iter:  353  Loss on train set:  -49.65051216668465\n",
      "Iter:  354  Loss on train set:  -49.65077365064623\n",
      "Iter:  355  Loss on train set:  -49.650719155312295\n",
      "Iter:  356  Loss on train set:  -49.65098466784176\n",
      "Iter:  357  Loss on train set:  -49.65077365064623\n",
      "     fun: -49.65077365064623\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.61071333,  0.05113985,  0.05188736, -0.26276772, -1.2576993 ,\n",
      "        0.33492395, -0.06948736,  0.00634419,  0.02378482])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  358  Loss on train set:  -49.65076801600606\n",
      "Iter:  359  Loss on train set:  -48.13413031494291\n",
      "Iter:  360  Loss on train set:  -45.63607493915104\n",
      "Iter:  361  Loss on train set:  -49.65077425503486\n",
      "Iter:  362  Loss on train set:  -38.740824734211245\n",
      "Iter:  363  Loss on train set:  -40.47509746574164\n",
      "Iter:  364  Loss on train set:  -39.884313801997834\n",
      "Iter:  365  Loss on train set:  -49.65077317306891\n",
      "Iter:  366  Loss on train set:  -49.58544494600144\n",
      "Iter:  367  Loss on train set:  -43.47064971578575\n",
      "Iter:  368  Loss on train set:  -48.25348276318877\n",
      "Iter:  369  Loss on train set:  -49.650772878798485\n",
      "Iter:  370  Loss on train set:  -48.00572972134024\n",
      "Iter:  371  Loss on train set:  -49.57094622614116\n",
      "Iter:  372  Loss on train set:  -48.92975726318398\n",
      "Iter:  373  Loss on train set:  -48.86030073660561\n",
      "Iter:  374  Loss on train set:  -49.65077494123508\n",
      "Iter:  375  Loss on train set:  -48.661235728982376\n",
      "Iter:  376  Loss on train set:  -49.650771197740326\n",
      "Iter:  377  Loss on train set:  -49.227003306485976\n",
      "Iter:  378  Loss on train set:  -49.650063393902045\n",
      "Iter:  379  Loss on train set:  -49.27514639622055\n",
      "Iter:  380  Loss on train set:  -49.678148457058604\n",
      "Iter:  381  Loss on train set:  -49.68117895495165\n",
      "Iter:  382  Loss on train set:  -49.555910922335144\n",
      "Iter:  383  Loss on train set:  -49.681178612892005\n",
      "Iter:  384  Loss on train set:  -49.408601739665286\n",
      "Iter:  385  Loss on train set:  -49.640302983830765\n",
      "Iter:  386  Loss on train set:  -49.72386906136464\n",
      "Iter:  387  Loss on train set:  -49.27296487772556\n",
      "Iter:  388  Loss on train set:  -49.69876938553437\n",
      "Iter:  389  Loss on train set:  -49.725338063154226\n",
      "Iter:  390  Loss on train set:  -49.72439772310639\n",
      "Iter:  391  Loss on train set:  -49.62522563442754\n",
      "Iter:  392  Loss on train set:  -49.72533707216065\n",
      "Iter:  393  Loss on train set:  -49.726620372934676\n",
      "Iter:  394  Loss on train set:  -49.71919725668755\n",
      "Iter:  395  Loss on train set:  -49.7141939995671\n",
      "Iter:  396  Loss on train set:  -49.7266203944255\n",
      "Iter:  397  Loss on train set:  -49.7027733210372\n",
      "Iter:  398  Loss on train set:  -49.75764435985142\n",
      "Iter:  399  Loss on train set:  -49.79133021352015\n",
      "Iter:  400  Loss on train set:  -49.75652616644554\n",
      "Iter:  401  Loss on train set:  -49.76998386284346\n",
      "Iter:  402  Loss on train set:  -49.78199202678007\n",
      "Iter:  403  Loss on train set:  -49.789638972389724\n",
      "Iter:  404  Loss on train set:  -49.79037292129526\n",
      "Iter:  405  Loss on train set:  -49.77725325777243\n",
      "Iter:  406  Loss on train set:  -49.789257586748754\n",
      "Iter:  407  Loss on train set:  -49.799389401899305\n",
      "Iter:  408  Loss on train set:  -49.79133021352015\n",
      "     fun: -49.79133021352015\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 2.04787737e+00, -7.15435273e-02,  2.77630924e-02,  2.56601355e+00,\n",
      "       -2.74128707e-01, -6.99760345e-01, -1.87245688e-01,  1.45361316e-03,\n",
      "       -8.60316678e-02])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  409  Loss on train set:  -49.20424665277926\n",
      "Iter:  410  Loss on train set:  -40.91687579580087\n",
      "Iter:  411  Loss on train set:  -41.86847042152412\n",
      "Iter:  412  Loss on train set:  -40.0435238901687\n",
      "Iter:  413  Loss on train set:  -40.874350660992505\n",
      "Iter:  414  Loss on train set:  -47.53837557508055\n",
      "Iter:  415  Loss on train set:  -49.791329384787076\n",
      "Iter:  416  Loss on train set:  -49.7913278928172\n",
      "Iter:  417  Loss on train set:  -49.79132744175453\n",
      "Iter:  418  Loss on train set:  -43.07737014815153\n",
      "Iter:  419  Loss on train set:  -47.92494069003905\n",
      "Iter:  420  Loss on train set:  -48.97619673328077\n",
      "Iter:  421  Loss on train set:  -49.810209420373376\n",
      "Iter:  422  Loss on train set:  -49.52646281040077\n",
      "Iter:  423  Loss on train set:  -49.59045473170701\n",
      "Iter:  424  Loss on train set:  -49.13867658295448\n",
      "Iter:  425  Loss on train set:  -49.81021945746649\n",
      "Iter:  426  Loss on train set:  -49.638712998496835\n",
      "Iter:  427  Loss on train set:  -49.81021416058508\n",
      "Iter:  428  Loss on train set:  -49.23096420468545\n",
      "Iter:  429  Loss on train set:  -49.810211113790416\n",
      "Iter:  430  Loss on train set:  -49.19179138810396\n",
      "Iter:  431  Loss on train set:  -49.83772432309255\n",
      "Iter:  432  Loss on train set:  -49.52276547206373\n",
      "Iter:  433  Loss on train set:  -49.818357590365046\n",
      "Iter:  434  Loss on train set:  -49.41320300720651\n",
      "Iter:  435  Loss on train set:  -49.68219572847117\n",
      "Iter:  436  Loss on train set:  -49.823716242554234\n",
      "Iter:  437  Loss on train set:  -49.71898155999256\n",
      "Iter:  438  Loss on train set:  -49.80583348025816\n",
      "Iter:  439  Loss on train set:  -49.83799685106126\n",
      "Iter:  440  Loss on train set:  -49.80427047929406\n",
      "Iter:  441  Loss on train set:  -49.827184994520174\n",
      "Iter:  442  Loss on train set:  -49.84871687151149\n",
      "Iter:  443  Loss on train set:  -49.83619236242703\n",
      "Iter:  444  Loss on train set:  -49.84141283320468\n",
      "Iter:  445  Loss on train set:  -49.829321353846865\n",
      "Iter:  446  Loss on train set:  -49.84883131423678\n",
      "Iter:  447  Loss on train set:  -49.84044943978361\n",
      "Iter:  448  Loss on train set:  -49.847546206906756\n",
      "Iter:  449  Loss on train set:  -49.85207433313716\n",
      "Iter:  450  Loss on train set:  -49.85475174480562\n",
      "Iter:  451  Loss on train set:  -49.85257213203911\n",
      "Iter:  452  Loss on train set:  -49.858488191351675\n",
      "Iter:  453  Loss on train set:  -49.852283934084284\n",
      "Iter:  454  Loss on train set:  -49.85544217025016\n",
      "Iter:  455  Loss on train set:  -49.85937096512326\n",
      "Iter:  456  Loss on train set:  -49.85480258795932\n",
      "Iter:  457  Loss on train set:  -49.85117687717194\n",
      "Iter:  458  Loss on train set:  -49.85882200196281\n",
      "Iter:  459  Loss on train set:  -49.85937096512326\n",
      "     fun: -49.85937096512326\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 2.96212469,  0.04921742, -0.09409964, -0.71575223,  0.0724556 ,\n",
      "        0.05808987, -0.03807946,  2.60105583,  1.37003818])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  460  Loss on train set:  -40.77345556509904\n",
      "Iter:  461  Loss on train set:  -36.91487904454062\n",
      "Iter:  462  Loss on train set:  -39.326896752604576\n",
      "Iter:  463  Loss on train set:  -42.87234998039827\n",
      "Iter:  464  Loss on train set:  -49.85936423637699\n",
      "Iter:  465  Loss on train set:  -44.84061853374913\n",
      "Iter:  466  Loss on train set:  -49.67107739104288\n",
      "Iter:  467  Loss on train set:  -47.62227741328759\n",
      "Iter:  468  Loss on train set:  -49.35233165681072\n",
      "Iter:  469  Loss on train set:  -38.01221817147994\n",
      "Iter:  470  Loss on train set:  -46.68834706615236\n",
      "Iter:  471  Loss on train set:  -49.19656323639833\n",
      "Iter:  472  Loss on train set:  -49.64674173764538\n",
      "Iter:  473  Loss on train set:  -49.184787155641786\n",
      "Iter:  474  Loss on train set:  -49.85936877183927\n",
      "Iter:  475  Loss on train set:  -49.29134546012337\n",
      "Iter:  476  Loss on train set:  -49.8760405020772\n",
      "Iter:  477  Loss on train set:  -49.56348559130015\n",
      "Iter:  478  Loss on train set:  -49.85780254752562\n",
      "Iter:  479  Loss on train set:  -49.709623196167236\n",
      "Iter:  480  Loss on train set:  -49.804477691271934\n",
      "Iter:  481  Loss on train set:  -49.529363625409545\n",
      "Iter:  482  Loss on train set:  -49.71332434342326\n",
      "Iter:  483  Loss on train set:  -49.87766962239178\n",
      "Iter:  484  Loss on train set:  -49.66669017143117\n",
      "Iter:  485  Loss on train set:  -49.873043665905726\n",
      "Iter:  486  Loss on train set:  -49.867699052904754\n",
      "Iter:  487  Loss on train set:  -49.87507840390637\n",
      "Iter:  488  Loss on train set:  -49.879881264520336\n",
      "Iter:  489  Loss on train set:  -49.835126300027625\n",
      "Iter:  490  Loss on train set:  -49.8644619745503\n",
      "Iter:  491  Loss on train set:  -49.86822914999451\n",
      "Iter:  492  Loss on train set:  -49.88061427436765\n",
      "Iter:  493  Loss on train set:  -49.84324016167052\n",
      "Iter:  494  Loss on train set:  -49.8897635091537\n",
      "Iter:  495  Loss on train set:  -49.85766284905839\n",
      "Iter:  496  Loss on train set:  -49.88935921206647\n",
      "Iter:  497  Loss on train set:  -49.882145550200676\n",
      "Iter:  498  Loss on train set:  -49.889493860143105\n",
      "Iter:  499  Loss on train set:  -49.88647359301291\n",
      "Iter:  500  Loss on train set:  -49.8873622513498\n",
      "Iter:  501  Loss on train set:  -49.88107252080973\n",
      "Iter:  502  Loss on train set:  -49.8913783602607\n",
      "Iter:  503  Loss on train set:  -49.89577148831644\n",
      "Iter:  504  Loss on train set:  -49.89809006590649\n",
      "Iter:  505  Loss on train set:  -49.895865805556134\n",
      "Iter:  506  Loss on train set:  -49.90188475172769\n",
      "Iter:  507  Loss on train set:  -49.89890282838087\n",
      "Iter:  508  Loss on train set:  -49.90164270819161\n",
      "Iter:  509  Loss on train set:  -49.90426431602537\n",
      "Iter:  510  Loss on train set:  -49.90188475172769\n",
      "     fun: -49.90188475172769\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.04340713, -0.07449026, -0.20474232, -0.26228027,  0.03327444,\n",
      "       -0.14021296,  0.61581848,  0.07786985, -0.09033769])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  511  Loss on train set:  -48.863863152105246\n",
      "Iter:  512  Loss on train set:  -37.703480342677324\n",
      "Iter:  513  Loss on train set:  -39.92858753985913\n",
      "Iter:  514  Loss on train set:  -49.90188044427571\n",
      "Iter:  515  Loss on train set:  -49.046442054809745\n",
      "Iter:  516  Loss on train set:  -45.34242560380183\n",
      "Iter:  517  Loss on train set:  -47.22307288947551\n",
      "Iter:  518  Loss on train set:  -39.154762363173774\n",
      "Iter:  519  Loss on train set:  -37.20022074456937\n",
      "Iter:  520  Loss on train set:  -36.64473411900323\n",
      "Iter:  521  Loss on train set:  -46.266169669975014\n",
      "Iter:  522  Loss on train set:  -48.79781113054062\n",
      "Iter:  523  Loss on train set:  -49.89980043899031\n",
      "Iter:  524  Loss on train set:  -49.38096588495595\n",
      "Iter:  525  Loss on train set:  -49.901885428088875\n",
      "Iter:  526  Loss on train set:  -49.52309112181652\n",
      "Iter:  527  Loss on train set:  -49.9776077470655\n",
      "Iter:  528  Loss on train set:  -50.30854375611447\n",
      "Iter:  529  Loss on train set:  -50.02395989981467\n",
      "Iter:  530  Loss on train set:  -49.721210354784475\n",
      "Iter:  531  Loss on train set:  -49.353200518532155\n",
      "Iter:  532  Loss on train set:  -50.034756783185664\n",
      "Iter:  533  Loss on train set:  -50.30198558672494\n",
      "Iter:  534  Loss on train set:  -50.23576549431981\n",
      "Iter:  535  Loss on train set:  -50.27646835887883\n",
      "Iter:  536  Loss on train set:  -50.05742530379767\n",
      "Iter:  537  Loss on train set:  -50.308542179237335\n",
      "Iter:  538  Loss on train set:  -50.24162971367146\n",
      "Iter:  539  Loss on train set:  -50.27446953619029\n",
      "Iter:  540  Loss on train set:  -50.11731667879736\n",
      "Iter:  541  Loss on train set:  -50.30376864438949\n",
      "Iter:  542  Loss on train set:  -50.32167996940729\n",
      "Iter:  543  Loss on train set:  -50.31950863421632\n",
      "Iter:  544  Loss on train set:  -50.293955628657066\n",
      "Iter:  545  Loss on train set:  -50.30458140614141\n",
      "Iter:  546  Loss on train set:  -50.34782094903875\n",
      "Iter:  547  Loss on train set:  -50.342711812865446\n",
      "Iter:  548  Loss on train set:  -50.34484104809801\n",
      "Iter:  549  Loss on train set:  -50.34528460592287\n",
      "Iter:  550  Loss on train set:  -50.344070767598254\n",
      "Iter:  551  Loss on train set:  -50.35698585762989\n",
      "Iter:  552  Loss on train set:  -50.35892277895768\n",
      "Iter:  553  Loss on train set:  -50.35552277176431\n",
      "Iter:  554  Loss on train set:  -50.36712514952885\n",
      "Iter:  555  Loss on train set:  -50.351003226100836\n",
      "Iter:  556  Loss on train set:  -50.37010230089444\n",
      "Iter:  557  Loss on train set:  -50.371069765783616\n",
      "Iter:  558  Loss on train set:  -50.37327574410869\n",
      "Iter:  559  Loss on train set:  -50.37377093495992\n",
      "Iter:  560  Loss on train set:  -50.37518518396232\n",
      "Iter:  561  Loss on train set:  -50.37377093495992\n",
      "     fun: -50.37377093495992\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.08815116, -0.05118708, -0.2295392 ,  2.50071782, -0.06582126,\n",
      "        0.04054582,  0.30637347,  3.9407599 , -0.0807441 ])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  562  Loss on train set:  -50.373770270165004\n",
      "Iter:  563  Loss on train set:  -45.76094270343685\n",
      "Iter:  564  Loss on train set:  -49.48084893840345\n",
      "Iter:  565  Loss on train set:  -37.67453044441069\n",
      "Iter:  566  Loss on train set:  -39.92072316667614\n",
      "Iter:  567  Loss on train set:  -39.13256834263282\n",
      "Iter:  568  Loss on train set:  -50.373770010606\n",
      "Iter:  569  Loss on train set:  -50.373771107085275\n",
      "Iter:  570  Loss on train set:  -41.22640609195243\n",
      "Iter:  571  Loss on train set:  -38.62332319941961\n",
      "Iter:  572  Loss on train set:  -46.836341058078794\n",
      "Iter:  573  Loss on train set:  -50.37376934892847\n",
      "Iter:  574  Loss on train set:  -49.486086724733354\n",
      "Iter:  575  Loss on train set:  -50.31063701830922\n",
      "Iter:  576  Loss on train set:  -48.38147623647687\n",
      "Iter:  577  Loss on train set:  -50.373763943734346\n",
      "Iter:  578  Loss on train set:  -46.9890940697125\n",
      "Iter:  579  Loss on train set:  -49.82828822897601\n",
      "Iter:  580  Loss on train set:  -50.37377248893513\n",
      "Iter:  581  Loss on train set:  -49.35723756654605\n",
      "Iter:  582  Loss on train set:  -50.04632092079711\n",
      "Iter:  583  Loss on train set:  -50.39849039770692\n",
      "Iter:  584  Loss on train set:  -50.57734025484611\n",
      "Iter:  585  Loss on train set:  -50.51836108639501\n",
      "Iter:  586  Loss on train set:  -50.48439781899191\n",
      "Iter:  587  Loss on train set:  -50.56897108619367\n",
      "Iter:  588  Loss on train set:  -50.29422727970051\n",
      "Iter:  589  Loss on train set:  -50.57733707340478\n",
      "Iter:  590  Loss on train set:  -50.449490429707815\n",
      "Iter:  591  Loss on train set:  -50.57733803148171\n",
      "Iter:  592  Loss on train set:  -50.39284330394581\n",
      "Iter:  593  Loss on train set:  -50.537577836728516\n",
      "Iter:  594  Loss on train set:  -50.57333542034092\n",
      "Iter:  595  Loss on train set:  -50.502443178266525\n",
      "Iter:  596  Loss on train set:  -50.55932232585602\n",
      "Iter:  597  Loss on train set:  -50.581886859268806\n",
      "Iter:  598  Loss on train set:  -50.572474942246465\n",
      "Iter:  599  Loss on train set:  -50.6010302790059\n",
      "Iter:  600  Loss on train set:  -50.604014694192486\n",
      "Iter:  601  Loss on train set:  -50.60344357919484\n",
      "Iter:  602  Loss on train set:  -50.59437923080108\n",
      "Iter:  603  Loss on train set:  -50.60402391950669\n",
      "Iter:  604  Loss on train set:  -50.596451831864385\n",
      "Iter:  605  Loss on train set:  -50.604017767923764\n",
      "Iter:  606  Loss on train set:  -50.602221744054\n",
      "Iter:  607  Loss on train set:  -50.60736523532644\n",
      "Iter:  608  Loss on train set:  -50.60286559446651\n",
      "Iter:  609  Loss on train set:  -50.606455623504075\n",
      "Iter:  610  Loss on train set:  -50.60629837492177\n",
      "Iter:  611  Loss on train set:  -50.60690352023356\n",
      "Iter:  612  Loss on train set:  -50.60736523532644\n",
      "     fun: -50.60736523532644\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.03807619,  0.23253551, -0.05958504, -0.04669738, -0.09734415,\n",
      "       -0.24936474,  0.0176395 ,  3.61722249, -0.09691204])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  613  Loss on train set:  -41.878198487266495\n",
      "Iter:  614  Loss on train set:  -50.60736252651302\n",
      "Iter:  615  Loss on train set:  -38.13923995499873\n",
      "Iter:  616  Loss on train set:  -39.86337692032466\n",
      "Iter:  617  Loss on train set:  -46.68786604571767\n",
      "Iter:  618  Loss on train set:  -37.537958167166394\n",
      "Iter:  619  Loss on train set:  -46.30648740061714\n",
      "Iter:  620  Loss on train set:  -42.62583247795992\n",
      "Iter:  621  Loss on train set:  -41.532087315917046\n",
      "Iter:  622  Loss on train set:  -36.85882303190396\n",
      "Iter:  623  Loss on train set:  -47.2402113406343\n",
      "Iter:  624  Loss on train set:  -49.98580794527393\n",
      "Iter:  625  Loss on train set:  -50.60736387952719\n",
      "Iter:  626  Loss on train set:  -49.95922838960553\n",
      "Iter:  627  Loss on train set:  -50.622747100424185\n",
      "Iter:  628  Loss on train set:  -49.72582480247401\n",
      "Iter:  629  Loss on train set:  -50.44108496219607\n",
      "Iter:  630  Loss on train set:  -50.07726003192643\n",
      "Iter:  631  Loss on train set:  -50.38125760918853\n",
      "Iter:  632  Loss on train set:  -49.83054615624062\n",
      "Iter:  633  Loss on train set:  -50.63413023202645\n",
      "Iter:  634  Loss on train set:  -50.63449148056713\n",
      "Iter:  635  Loss on train set:  -50.55074475502483\n",
      "Iter:  636  Loss on train set:  -50.57711651206948\n",
      "Iter:  637  Loss on train set:  -50.384933627567364\n",
      "Iter:  638  Loss on train set:  -50.77418454380795\n",
      "Iter:  639  Loss on train set:  -50.57828588247796\n",
      "Iter:  640  Loss on train set:  -50.711619779713445\n",
      "Iter:  641  Loss on train set:  -50.68560515827718\n",
      "Iter:  642  Loss on train set:  -50.74500659520588\n",
      "Iter:  643  Loss on train set:  -50.77419075662471\n",
      "Iter:  644  Loss on train set:  -50.80174529616582\n",
      "Iter:  645  Loss on train set:  -50.798794316626186\n",
      "Iter:  646  Loss on train set:  -50.742235532921306\n",
      "Iter:  647  Loss on train set:  -50.75732599215927\n",
      "Iter:  648  Loss on train set:  -50.78887826604251\n",
      "Iter:  649  Loss on train set:  -50.78489170555161\n",
      "Iter:  650  Loss on train set:  -50.79576275219328\n",
      "Iter:  651  Loss on train set:  -50.80415991829559\n",
      "Iter:  652  Loss on train set:  -50.80415864984321\n",
      "Iter:  653  Loss on train set:  -50.79472231479966\n",
      "Iter:  654  Loss on train set:  -50.80224722187903\n",
      "Iter:  655  Loss on train set:  -50.79971231138748\n",
      "Iter:  656  Loss on train set:  -50.80101413784815\n",
      "Iter:  657  Loss on train set:  -50.81193054953382\n",
      "Iter:  658  Loss on train set:  -50.79710156101115\n",
      "Iter:  659  Loss on train set:  -50.805216469197724\n",
      "Iter:  660  Loss on train set:  -50.80748738667518\n",
      "Iter:  661  Loss on train set:  -50.809205988390616\n",
      "Iter:  662  Loss on train set:  -50.811221731516106\n",
      "Iter:  663  Loss on train set:  -50.81193054953382\n",
      "     fun: -50.81193054953382\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-1.31738383, -0.00308793,  0.02106526, -0.70492913,  0.22910307,\n",
      "        0.00446972,  0.08919374, -0.36049467,  0.03165298])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  664  Loss on train set:  -45.52651028511089\n",
      "Iter:  665  Loss on train set:  -41.938450388762334\n",
      "Iter:  666  Loss on train set:  -38.018652494980216\n",
      "Iter:  667  Loss on train set:  -38.855542054529174\n",
      "Iter:  668  Loss on train set:  -50.81192653049466\n",
      "Iter:  669  Loss on train set:  -46.122783840207816\n",
      "Iter:  670  Loss on train set:  -45.439641837533124\n",
      "Iter:  671  Loss on train set:  -44.60039620930028\n",
      "Iter:  672  Loss on train set:  -38.0312955955702\n",
      "Iter:  673  Loss on train set:  -35.98679909504439\n",
      "Iter:  674  Loss on train set:  -47.197328226588134\n",
      "Iter:  675  Loss on train set:  -50.434652298568786\n",
      "Iter:  676  Loss on train set:  -50.80693407993161\n",
      "Iter:  677  Loss on train set:  -50.4409322187213\n",
      "Iter:  678  Loss on train set:  -50.81193120575721\n",
      "Iter:  679  Loss on train set:  -49.990953520986885\n",
      "Iter:  680  Loss on train set:  -50.689785830339176\n",
      "Iter:  681  Loss on train set:  -50.564748260642936\n",
      "Iter:  682  Loss on train set:  -50.69999061694167\n",
      "Iter:  683  Loss on train set:  -49.479262770180284\n",
      "Iter:  684  Loss on train set:  -50.744721282887255\n",
      "Iter:  685  Loss on train set:  -50.83342871834327\n",
      "Iter:  686  Loss on train set:  -50.79957950959032\n",
      "Iter:  687  Loss on train set:  -50.64618990947517\n",
      "Iter:  688  Loss on train set:  -50.75158532983014\n",
      "Iter:  689  Loss on train set:  -50.80719237784026\n",
      "Iter:  690  Loss on train set:  -50.84264859268191\n",
      "Iter:  691  Loss on train set:  -50.84109467718012\n",
      "Iter:  692  Loss on train set:  -50.756280060474595\n",
      "Iter:  693  Loss on train set:  -50.84265978430089\n",
      "Iter:  694  Loss on train set:  -50.84976737903546\n",
      "Iter:  695  Loss on train set:  -50.83780629083205\n",
      "Iter:  696  Loss on train set:  -50.89038807039204\n",
      "Iter:  697  Loss on train set:  -50.85476724191723\n",
      "Iter:  698  Loss on train set:  -50.89089434440084\n",
      "Iter:  699  Loss on train set:  -50.82642827792484\n",
      "Iter:  700  Loss on train set:  -50.876351448345744\n",
      "Iter:  701  Loss on train set:  -50.883608720056\n",
      "Iter:  702  Loss on train set:  -50.88856214740484\n",
      "Iter:  703  Loss on train set:  -50.894832173111794\n",
      "Iter:  704  Loss on train set:  -50.89485048464557\n",
      "Iter:  705  Loss on train set:  -50.88327267087212\n",
      "Iter:  706  Loss on train set:  -50.894961846039315\n",
      "Iter:  707  Loss on train set:  -50.897190634747254\n",
      "Iter:  708  Loss on train set:  -50.89287710372744\n",
      "Iter:  709  Loss on train set:  -50.87917564794905\n",
      "Iter:  710  Loss on train set:  -50.89274939977778\n",
      "Iter:  711  Loss on train set:  -50.89533418904258\n",
      "Iter:  712  Loss on train set:  -50.89579331378086\n",
      "Iter:  713  Loss on train set:  -50.896550366466776\n",
      "Iter:  714  Loss on train set:  -50.897190634747254\n",
      "     fun: -50.897190634747254\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.3302823 , -0.7091721 , -0.11273273, -0.29809191,  3.44529157,\n",
      "       -0.02843775, -0.10604601, -0.0617101 ,  3.03657087])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  715  Loss on train set:  -41.476614581108635\n",
      "Iter:  716  Loss on train set:  -39.25160976689681\n",
      "Iter:  717  Loss on train set:  -45.07190197695621\n",
      "Iter:  718  Loss on train set:  -50.89719322623798\n",
      "Iter:  719  Loss on train set:  -50.500422356032686\n",
      "Iter:  720  Loss on train set:  -47.12206295552929\n",
      "Iter:  721  Loss on train set:  -41.76899722583242\n",
      "Iter:  722  Loss on train set:  -45.12779841444927\n",
      "Iter:  723  Loss on train set:  -45.19958751469347\n",
      "Iter:  724  Loss on train set:  -43.054324909302686\n",
      "Iter:  725  Loss on train set:  -48.053465613152966\n",
      "Iter:  726  Loss on train set:  -50.44093036646132\n",
      "Iter:  727  Loss on train set:  -48.25779104728574\n",
      "Iter:  728  Loss on train set:  -50.59524192847295\n",
      "Iter:  729  Loss on train set:  -50.89719059882115\n",
      "Iter:  730  Loss on train set:  -50.19673365823339\n",
      "Iter:  731  Loss on train set:  -50.90734757666658\n",
      "Iter:  732  Loss on train set:  -50.62160894992762\n",
      "Iter:  733  Loss on train set:  -50.79345102403462\n",
      "Iter:  734  Loss on train set:  -50.42349339842294\n",
      "Iter:  735  Loss on train set:  -50.669341784697274\n",
      "Iter:  736  Loss on train set:  -50.903643074823414\n",
      "Iter:  737  Loss on train set:  -50.78476653021754\n",
      "Iter:  738  Loss on train set:  -50.87379598785268\n",
      "Iter:  739  Loss on train set:  -50.72962570873905\n",
      "Iter:  740  Loss on train set:  -50.89359985089996\n",
      "Iter:  741  Loss on train set:  -50.90734745141306\n",
      "Iter:  742  Loss on train set:  -50.89278317211984\n",
      "Iter:  743  Loss on train set:  -50.906382629552944\n",
      "Iter:  744  Loss on train set:  -50.90799623592308\n",
      "Iter:  745  Loss on train set:  -50.90503279269082\n",
      "Iter:  746  Loss on train set:  -50.89972687869247\n",
      "Iter:  747  Loss on train set:  -50.89950948254579\n",
      "Iter:  748  Loss on train set:  -50.90700882188876\n",
      "Iter:  749  Loss on train set:  -50.906026901117684\n",
      "Iter:  750  Loss on train set:  -50.90929109716827\n",
      "Iter:  751  Loss on train set:  -50.90657425192989\n",
      "Iter:  752  Loss on train set:  -50.91348883450278\n",
      "Iter:  753  Loss on train set:  -50.914615302392185\n",
      "Iter:  754  Loss on train set:  -50.91461683564516\n",
      "Iter:  755  Loss on train set:  -50.91927384486678\n",
      "Iter:  756  Loss on train set:  -50.920270396644156\n",
      "Iter:  757  Loss on train set:  -50.922282620188085\n",
      "Iter:  758  Loss on train set:  -50.923207830162866\n",
      "Iter:  759  Loss on train set:  -50.92279017751929\n",
      "Iter:  760  Loss on train set:  -50.92589543286469\n",
      "Iter:  761  Loss on train set:  -50.923260683523075\n",
      "Iter:  762  Loss on train set:  -50.923926598484584\n",
      "Iter:  763  Loss on train set:  -50.927265584772414\n",
      "Iter:  764  Loss on train set:  -50.928330505392694\n",
      "Iter:  765  Loss on train set:  -50.927265584772414\n",
      "     fun: -50.927265584772414\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.02783951,  3.04892962, -0.02510569,  0.98757313, -0.2354989 ,\n",
      "        0.20105526,  0.00323845,  0.33716044, -0.09862427])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  766  Loss on train set:  -50.5945785279016\n",
      "Iter:  767  Loss on train set:  -38.19602468250994\n",
      "Iter:  768  Loss on train set:  -40.5865887431239\n",
      "Iter:  769  Loss on train set:  -36.92002721952844\n",
      "Iter:  770  Loss on train set:  -39.062295012253585\n",
      "Iter:  771  Loss on train set:  -39.76183737429161\n",
      "Iter:  772  Loss on train set:  -45.09763104434857\n",
      "Iter:  773  Loss on train set:  -45.628820948541275\n",
      "Iter:  774  Loss on train set:  -48.3455322373166\n",
      "Iter:  775  Loss on train set:  -34.57553558773026\n",
      "Iter:  776  Loss on train set:  -46.96150893401542\n",
      "Iter:  777  Loss on train set:  -50.418432720499254\n",
      "Iter:  778  Loss on train set:  -50.925752778090214\n",
      "Iter:  779  Loss on train set:  -50.4028129048771\n",
      "Iter:  780  Loss on train set:  -50.628860303724394\n",
      "Iter:  781  Loss on train set:  -49.57190697185746\n",
      "Iter:  782  Loss on train set:  -50.81158220496532\n",
      "Iter:  783  Loss on train set:  -50.31706798112388\n",
      "Iter:  784  Loss on train set:  -50.92051099795411\n",
      "Iter:  785  Loss on train set:  -50.55534574931093\n",
      "Iter:  786  Loss on train set:  -50.7451293614871\n",
      "Iter:  787  Loss on train set:  -50.90536438078827\n",
      "Iter:  788  Loss on train set:  -50.92288152351849\n",
      "Iter:  789  Loss on train set:  -50.92003501545997\n",
      "Iter:  790  Loss on train set:  -50.948537326692374\n",
      "Iter:  791  Loss on train set:  -50.88750477982127\n",
      "Iter:  792  Loss on train set:  -50.96361569911125\n",
      "Iter:  793  Loss on train set:  -50.964872018227524\n",
      "Iter:  794  Loss on train set:  -50.93534733876919\n",
      "Iter:  795  Loss on train set:  -50.95923528311497\n",
      "Iter:  796  Loss on train set:  -50.960314774224884\n",
      "Iter:  797  Loss on train set:  -50.96405904310699\n",
      "Iter:  798  Loss on train set:  -50.962722502719835\n",
      "Iter:  799  Loss on train set:  -50.977715634750886\n",
      "Iter:  800  Loss on train set:  -50.95473910873074\n",
      "Iter:  801  Loss on train set:  -50.98209152740867\n",
      "Iter:  802  Loss on train set:  -50.995141094913436\n",
      "Iter:  803  Loss on train set:  -51.00470143938905\n",
      "Iter:  804  Loss on train set:  -50.9773625103959\n",
      "Iter:  805  Loss on train set:  -50.99898522466422\n",
      "Iter:  806  Loss on train set:  -51.00040032474133\n",
      "Iter:  807  Loss on train set:  -50.99587826430369\n",
      "Iter:  808  Loss on train set:  -51.009916482076484\n",
      "Iter:  809  Loss on train set:  -51.01214215426371\n",
      "Iter:  810  Loss on train set:  -51.01793543747317\n",
      "Iter:  811  Loss on train set:  -51.01672321400486\n",
      "Iter:  812  Loss on train set:  -51.013568185437045\n",
      "Iter:  813  Loss on train set:  -51.011940823004174\n",
      "Iter:  814  Loss on train set:  -51.01424654884781\n",
      "Iter:  815  Loss on train set:  -51.01328649766566\n",
      "Iter:  816  Loss on train set:  -51.01793543747317\n",
      "     fun: -51.01793543747317\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.23621634,  0.02589977, -0.68056746, -1.49003271,  3.05684142,\n",
      "       -0.3585508 , -0.05384082,  0.054655  ,  0.02832913])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  817  Loss on train set:  -41.69098189318679\n",
      "Iter:  818  Loss on train set:  -38.087279717519735\n",
      "Iter:  819  Loss on train set:  -42.09217038175836\n",
      "Iter:  820  Loss on train set:  -45.140525043902\n",
      "Iter:  821  Loss on train set:  -41.69249233762126\n",
      "Iter:  822  Loss on train set:  -42.05177240090766\n",
      "Iter:  823  Loss on train set:  -39.004072525671745\n",
      "Iter:  824  Loss on train set:  -45.05897655196302\n",
      "Iter:  825  Loss on train set:  -51.017934879423855\n",
      "Iter:  826  Loss on train set:  -37.57391673462405\n",
      "Iter:  827  Loss on train set:  -47.056219792436856\n",
      "Iter:  828  Loss on train set:  -50.623085831515446\n",
      "Iter:  829  Loss on train set:  -50.756407715469685\n",
      "Iter:  830  Loss on train set:  -50.579090013078044\n",
      "Iter:  831  Loss on train set:  -50.89007393129035\n",
      "Iter:  832  Loss on train set:  -50.270517754141\n",
      "Iter:  833  Loss on train set:  -50.87116820841774\n",
      "Iter:  834  Loss on train set:  -50.30563380169836\n",
      "Iter:  835  Loss on train set:  -51.01793225081909\n",
      "Iter:  836  Loss on train set:  -50.55311303290763\n",
      "Iter:  837  Loss on train set:  -50.78517382390343\n",
      "Iter:  838  Loss on train set:  -50.98761807492347\n",
      "Iter:  839  Loss on train set:  -51.004485880500596\n",
      "Iter:  840  Loss on train set:  -50.97696281165114\n",
      "Iter:  841  Loss on train set:  -51.01353908434477\n",
      "Iter:  842  Loss on train set:  -50.95467471201884\n",
      "Iter:  843  Loss on train set:  -51.00763997277488\n",
      "Iter:  844  Loss on train set:  -51.01793635820861\n",
      "Iter:  845  Loss on train set:  -51.0151805543685\n",
      "Iter:  846  Loss on train set:  -51.02056200827879\n",
      "Iter:  847  Loss on train set:  -50.99427993537207\n",
      "Iter:  848  Loss on train set:  -51.02052639614432\n",
      "Iter:  849  Loss on train set:  -51.00788672025698\n",
      "Iter:  850  Loss on train set:  -51.013235688692156\n",
      "Iter:  851  Loss on train set:  -51.018566396825406\n",
      "Iter:  852  Loss on train set:  -51.02198290649492\n",
      "Iter:  853  Loss on train set:  -51.020965156454224\n",
      "Iter:  854  Loss on train set:  -51.02339739721002\n",
      "Iter:  855  Loss on train set:  -51.025253594543486\n",
      "Iter:  856  Loss on train set:  -51.0208411089547\n",
      "Iter:  857  Loss on train set:  -51.026629050621196\n",
      "Iter:  858  Loss on train set:  -51.021380634382986\n",
      "Iter:  859  Loss on train set:  -51.02662594116679\n",
      "Iter:  860  Loss on train set:  -51.01989157074791\n",
      "Iter:  861  Loss on train set:  -51.02678634306611\n",
      "Iter:  862  Loss on train set:  -51.026564399302025\n",
      "Iter:  863  Loss on train set:  -51.027352149747685\n",
      "Iter:  864  Loss on train set:  -51.02832136426371\n",
      "Iter:  865  Loss on train set:  -51.02862590828962\n",
      "Iter:  866  Loss on train set:  -51.02889817461036\n",
      "Iter:  867  Loss on train set:  -51.02862590828962\n",
      "     fun: -51.02862590828962\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.66649018, -1.49359853, -0.67728912,  0.31547044,  0.15655907,\n",
      "        0.00487002,  3.04654876, -0.04722309,  1.38565949])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  868  Loss on train set:  -37.844693905652164\n",
      "Iter:  869  Loss on train set:  -40.7560990758591\n",
      "Iter:  870  Loss on train set:  -39.70080300603303\n",
      "Iter:  871  Loss on train set:  -51.02862810548776\n",
      "Iter:  872  Loss on train set:  -49.14619669693599\n",
      "Iter:  873  Loss on train set:  -46.40202968050621\n",
      "Iter:  874  Loss on train set:  -42.30908950513543\n",
      "Iter:  875  Loss on train set:  -46.895447027559044\n",
      "Iter:  876  Loss on train set:  -45.30213210334231\n",
      "Iter:  877  Loss on train set:  -44.1045324946699\n",
      "Iter:  878  Loss on train set:  -46.76260022804126\n",
      "Iter:  879  Loss on train set:  -50.12532868653484\n",
      "Iter:  880  Loss on train set:  -49.13868148096692\n",
      "Iter:  881  Loss on train set:  -50.977179835677155\n",
      "Iter:  882  Loss on train set:  -51.02862307833181\n",
      "Iter:  883  Loss on train set:  -50.79255670815692\n",
      "Iter:  884  Loss on train set:  -50.95673147571856\n",
      "Iter:  885  Loss on train set:  -50.50352215137048\n",
      "Iter:  886  Loss on train set:  -50.92874006322534\n",
      "Iter:  887  Loss on train set:  -50.790383522846255\n",
      "Iter:  888  Loss on train set:  -50.97197397968167\n",
      "Iter:  889  Loss on train set:  -50.62607064971236\n",
      "Iter:  890  Loss on train set:  -51.03554863324744\n",
      "Iter:  891  Loss on train set:  -51.025899429611854\n",
      "Iter:  892  Loss on train set:  -51.00995709364676\n",
      "Iter:  893  Loss on train set:  -50.981711054539126\n",
      "Iter:  894  Loss on train set:  -51.0650274087339\n",
      "Iter:  895  Loss on train set:  -51.0613183527733\n",
      "Iter:  896  Loss on train set:  -51.113992546368635\n",
      "Iter:  897  Loss on train set:  -51.10780179248255\n",
      "Iter:  898  Loss on train set:  -51.11398803017563\n",
      "Iter:  899  Loss on train set:  -51.13614182891344\n",
      "Iter:  900  Loss on train set:  -51.12456000061701\n",
      "Iter:  901  Loss on train set:  -51.12862182568299\n",
      "Iter:  902  Loss on train set:  -51.11961891090128\n",
      "Iter:  903  Loss on train set:  -51.127956970688714\n",
      "Iter:  904  Loss on train set:  -51.12345787384364\n",
      "Iter:  905  Loss on train set:  -51.132774418777636\n",
      "Iter:  906  Loss on train set:  -51.14667441722833\n",
      "Iter:  907  Loss on train set:  -51.138604366261426\n",
      "Iter:  908  Loss on train set:  -51.13019825571308\n",
      "Iter:  909  Loss on train set:  -51.146674838936974\n",
      "Iter:  910  Loss on train set:  -51.14186241287879\n",
      "Iter:  911  Loss on train set:  -51.14572864042386\n",
      "Iter:  912  Loss on train set:  -51.15718891868025\n",
      "Iter:  913  Loss on train set:  -51.15053692203449\n",
      "Iter:  914  Loss on train set:  -51.15907602523422\n",
      "Iter:  915  Loss on train set:  -51.14614152459566\n",
      "Iter:  916  Loss on train set:  -51.1674188126733\n",
      "Iter:  917  Loss on train set:  -51.171593571758905\n",
      "Iter:  918  Loss on train set:  -51.1674188126733\n",
      "     fun: -51.1674188126733\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 3.81755884, -0.75499095, -0.21651896,  1.99938254, -0.14662157,\n",
      "       -0.04517086, -1.35811695,  0.0060644 ,  0.32432207])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  919  Loss on train set:  -45.50740717701877\n",
      "Iter:  920  Loss on train set:  -46.77488135315807\n",
      "Iter:  921  Loss on train set:  -51.16741993747951\n",
      "Iter:  922  Loss on train set:  -51.16741599449741\n",
      "Iter:  923  Loss on train set:  -45.9270485691726\n",
      "Iter:  924  Loss on train set:  -46.091789756786255\n",
      "Iter:  925  Loss on train set:  -39.07446394253542\n",
      "Iter:  926  Loss on train set:  -43.67601561533138\n",
      "Iter:  927  Loss on train set:  -43.02292116680103\n",
      "Iter:  928  Loss on train set:  -41.78349445959958\n",
      "Iter:  929  Loss on train set:  -49.53118278978554\n",
      "Iter:  930  Loss on train set:  -49.89319643403951\n",
      "Iter:  931  Loss on train set:  -50.608398216595745\n",
      "Iter:  932  Loss on train set:  -51.16741861971872\n",
      "Iter:  933  Loss on train set:  -50.7847810676313\n",
      "Iter:  934  Loss on train set:  -51.16741738229835\n",
      "Iter:  935  Loss on train set:  -50.863081947547805\n",
      "Iter:  936  Loss on train set:  -51.104884657602376\n",
      "Iter:  937  Loss on train set:  -50.32217806055388\n",
      "Iter:  938  Loss on train set:  -50.66846634358908\n",
      "Iter:  939  Loss on train set:  -51.05576651857706\n",
      "Iter:  940  Loss on train set:  -51.0892919109679\n",
      "Iter:  941  Loss on train set:  -51.153211106310216\n",
      "Iter:  942  Loss on train set:  -51.16717256294832\n",
      "Iter:  943  Loss on train set:  -51.131043094657095\n",
      "Iter:  944  Loss on train set:  -51.17515055019517\n",
      "Iter:  945  Loss on train set:  -51.14443453256139\n",
      "Iter:  946  Loss on train set:  -51.1728853723699\n",
      "Iter:  947  Loss on train set:  -51.17515586872407\n",
      "Iter:  948  Loss on train set:  -51.18494859088976\n",
      "Iter:  949  Loss on train set:  -51.184950456479726\n",
      "Iter:  950  Loss on train set:  -51.17578757745296\n",
      "Iter:  951  Loss on train set:  -51.1783063547909\n",
      "Iter:  952  Loss on train set:  -51.18305560080879\n",
      "Iter:  953  Loss on train set:  -51.18141218795104\n",
      "Iter:  954  Loss on train set:  -51.181047194066295\n",
      "Iter:  955  Loss on train set:  -51.18359661639877\n",
      "Iter:  956  Loss on train set:  -51.188270511256384\n",
      "Iter:  957  Loss on train set:  -51.18826751556726\n",
      "Iter:  958  Loss on train set:  -51.188644307212584\n",
      "Iter:  959  Loss on train set:  -51.18757366078283\n",
      "Iter:  960  Loss on train set:  -51.19071566735257\n",
      "Iter:  961  Loss on train set:  -51.19442844762892\n",
      "Iter:  962  Loss on train set:  -51.1940391918678\n",
      "Iter:  963  Loss on train set:  -51.194436237047114\n",
      "Iter:  964  Loss on train set:  -51.19445129400701\n",
      "Iter:  965  Loss on train set:  -51.1939687756181\n",
      "Iter:  966  Loss on train set:  -51.19159464400207\n",
      "Iter:  967  Loss on train set:  -51.194459810083146\n",
      "Iter:  968  Loss on train set:  -51.192847897809614\n",
      "Iter:  969  Loss on train set:  -51.194459810083146\n",
      "     fun: -51.194459810083146\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.03589495, -0.01850865,  2.88862466,  1.40912257,  0.06531341,\n",
      "       -0.02092151, -0.01657832,  0.20532413, -1.32621887])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  970  Loss on train set:  -46.453578706251825\n",
      "Iter:  971  Loss on train set:  -39.70379762253696\n",
      "Iter:  972  Loss on train set:  -39.84565457376185\n",
      "Iter:  973  Loss on train set:  -41.966434359874214\n",
      "Iter:  974  Loss on train set:  -39.297790466347266\n",
      "Iter:  975  Loss on train set:  -51.19445829376891\n",
      "Iter:  976  Loss on train set:  -51.19445650460827\n",
      "Iter:  977  Loss on train set:  -51.19445753870625\n",
      "Iter:  978  Loss on train set:  -51.194453537635354\n",
      "Iter:  979  Loss on train set:  -34.36992320217634\n",
      "Iter:  980  Loss on train set:  -46.870777465190784\n",
      "Iter:  981  Loss on train set:  -50.63580235218761\n",
      "Iter:  982  Loss on train set:  -51.132402089931986\n",
      "Iter:  983  Loss on train set:  -50.44206903371972\n",
      "Iter:  984  Loss on train set:  -51.01798528965026\n",
      "Iter:  985  Loss on train set:  -50.31489680415027\n",
      "Iter:  986  Loss on train set:  -51.19446071604836\n",
      "Iter:  987  Loss on train set:  -50.04700295536866\n",
      "Iter:  988  Loss on train set:  -51.19445256401079\n",
      "Iter:  989  Loss on train set:  -50.75006494566351\n",
      "Iter:  990  Loss on train set:  -51.194453512679516\n",
      "Iter:  991  Loss on train set:  -50.326194901974375\n",
      "Iter:  992  Loss on train set:  -51.19444834340911\n",
      "Iter:  993  Loss on train set:  -50.47786153632563\n",
      "Iter:  994  Loss on train set:  -51.100350517462374\n",
      "Iter:  995  Loss on train set:  -51.14904487935192\n",
      "Iter:  996  Loss on train set:  -51.18699695582527\n",
      "Iter:  997  Loss on train set:  -51.154970584335295\n",
      "Iter:  998  Loss on train set:  -51.19466782259978\n",
      "Iter:  999  Loss on train set:  -51.102859686858956\n",
      "Iter:  1000  Loss on train set:  -51.197999869429296\n",
      "Iter:  1001  Loss on train set:  -51.16854879967077\n",
      "Iter:  1002  Loss on train set:  -51.197189980731146\n",
      "Iter:  1003  Loss on train set:  -51.17538112781852\n",
      "Iter:  1004  Loss on train set:  -51.19950939650329\n",
      "Iter:  1005  Loss on train set:  -51.15922798731504\n",
      "Iter:  1006  Loss on train set:  -51.198520226200564\n",
      "Iter:  1007  Loss on train set:  -51.17870128126508\n",
      "Iter:  1008  Loss on train set:  -51.09232807435985\n",
      "Iter:  1009  Loss on train set:  -51.196560173498675\n",
      "Iter:  1010  Loss on train set:  -51.19451955423068\n",
      "Iter:  1011  Loss on train set:  -51.197349519205936\n",
      "Iter:  1012  Loss on train set:  -51.19774694485912\n",
      "Iter:  1013  Loss on train set:  -51.188565385193\n",
      "Iter:  1014  Loss on train set:  -51.20008604041252\n",
      "Iter:  1015  Loss on train set:  -51.203240352677874\n",
      "Iter:  1016  Loss on train set:  -51.20110823716459\n",
      "Iter:  1017  Loss on train set:  -51.203472000442396\n",
      "Iter:  1018  Loss on train set:  -51.20139238720693\n",
      "Iter:  1019  Loss on train set:  -51.2036487166452\n",
      "Iter:  1020  Loss on train set:  -51.203472000442396\n",
      "     fun: -51.203472000442396\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 7.77248315e-02, -2.19148614e-01, -3.74891070e-01,  4.05851315e-03,\n",
      "        3.03924167e+00,  1.94428096e+00, -9.27203242e-03,  3.45526310e+00,\n",
      "        9.72104766e-04])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1021  Loss on train set:  -38.1853185006477\n",
      "Iter:  1022  Loss on train set:  -51.203472187074915\n",
      "Iter:  1023  Loss on train set:  -50.29466595717983\n",
      "Iter:  1024  Loss on train set:  -43.42675322492704\n",
      "Iter:  1025  Loss on train set:  -45.505284663200825\n",
      "Iter:  1026  Loss on train set:  -42.57884041849768\n",
      "Iter:  1027  Loss on train set:  -39.414910813697524\n",
      "Iter:  1028  Loss on train set:  -49.541774015761895\n",
      "Iter:  1029  Loss on train set:  -38.39732178577019\n",
      "Iter:  1030  Loss on train set:  -45.28807889634611\n",
      "Iter:  1031  Loss on train set:  -46.606521422404484\n",
      "Iter:  1032  Loss on train set:  -50.900731166313825\n",
      "Iter:  1033  Loss on train set:  -51.20347517802817\n",
      "Iter:  1034  Loss on train set:  -50.81709500447134\n",
      "Iter:  1035  Loss on train set:  -51.18420802529968\n",
      "Iter:  1036  Loss on train set:  -50.82861518894101\n",
      "Iter:  1037  Loss on train set:  -50.90524242478374\n",
      "Iter:  1038  Loss on train set:  -50.875668909263545\n",
      "Iter:  1039  Loss on train set:  -51.19758811921941\n",
      "Iter:  1040  Loss on train set:  -50.0022309558172\n",
      "Iter:  1041  Loss on train set:  -50.85323749989151\n",
      "Iter:  1042  Loss on train set:  -51.191818441000265\n",
      "Iter:  1043  Loss on train set:  -51.16469638862968\n",
      "Iter:  1044  Loss on train set:  -51.20035379442707\n",
      "Iter:  1045  Loss on train set:  -51.178540470649224\n",
      "Iter:  1046  Loss on train set:  -51.18928741094438\n",
      "Iter:  1047  Loss on train set:  -51.227484531215715\n",
      "Iter:  1048  Loss on train set:  -51.2187397262348\n",
      "Iter:  1049  Loss on train set:  -51.20954749971226\n",
      "Iter:  1050  Loss on train set:  -51.229847675872556\n",
      "Iter:  1051  Loss on train set:  -51.23425319363946\n",
      "Iter:  1052  Loss on train set:  -51.23132493881968\n",
      "Iter:  1053  Loss on train set:  -51.22668921931282\n",
      "Iter:  1054  Loss on train set:  -51.23425438411674\n",
      "Iter:  1055  Loss on train set:  -51.22842204336373\n",
      "Iter:  1056  Loss on train set:  -51.235867263839694\n",
      "Iter:  1057  Loss on train set:  -51.231703502636535\n",
      "Iter:  1058  Loss on train set:  -51.224521879080086\n",
      "Iter:  1059  Loss on train set:  -51.236886158282694\n",
      "Iter:  1060  Loss on train set:  -51.24130529846174\n",
      "Iter:  1061  Loss on train set:  -51.24309028916233\n",
      "Iter:  1062  Loss on train set:  -51.244058814180974\n",
      "Iter:  1063  Loss on train set:  -51.24559373662949\n",
      "Iter:  1064  Loss on train set:  -51.24549290157014\n",
      "Iter:  1065  Loss on train set:  -51.24217314588964\n",
      "Iter:  1066  Loss on train set:  -51.245584448433135\n",
      "Iter:  1067  Loss on train set:  -51.24048643696125\n",
      "Iter:  1068  Loss on train set:  -51.2440331043773\n",
      "Iter:  1069  Loss on train set:  -51.24366580629227\n",
      "Iter:  1070  Loss on train set:  -51.24637518058397\n",
      "Iter:  1071  Loss on train set:  -51.24559373662949\n",
      "     fun: -51.24559373662949\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.11254102,  4.02925282, -0.05492643, -0.03496067,  0.31165641,\n",
      "       -1.34213613,  3.02127257, -0.15385188, -1.49464604])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1072  Loss on train set:  -45.5868959022955\n",
      "Iter:  1073  Loss on train set:  -51.24559827678266\n",
      "Iter:  1074  Loss on train set:  -39.50920855350025\n",
      "Iter:  1075  Loss on train set:  -51.24559667189692\n",
      "Iter:  1076  Loss on train set:  -42.347815376795495\n",
      "Iter:  1077  Loss on train set:  -49.94206719615374\n",
      "Iter:  1078  Loss on train set:  -45.45780769972784\n",
      "Iter:  1079  Loss on train set:  -50.37013091517892\n",
      "Iter:  1080  Loss on train set:  -51.24559850832499\n",
      "Iter:  1081  Loss on train set:  -42.68576920048165\n",
      "Iter:  1082  Loss on train set:  -49.55366423551654\n",
      "Iter:  1083  Loss on train set:  -51.24559605429371\n",
      "Iter:  1084  Loss on train set:  -48.2314159755541\n",
      "Iter:  1085  Loss on train set:  -51.24559953665037\n",
      "Iter:  1086  Loss on train set:  -48.93150477737185\n",
      "Iter:  1087  Loss on train set:  -51.14822787933386\n",
      "Iter:  1088  Loss on train set:  -49.77417114378983\n",
      "Iter:  1089  Loss on train set:  -51.11657107302777\n",
      "Iter:  1090  Loss on train set:  -48.914346596014795\n",
      "Iter:  1091  Loss on train set:  -50.78012926315544\n",
      "Iter:  1092  Loss on train set:  -51.24559828143297\n",
      "Iter:  1093  Loss on train set:  -50.511355041871276\n",
      "Iter:  1094  Loss on train set:  -51.09342145949972\n",
      "Iter:  1095  Loss on train set:  -51.221933165698964\n",
      "Iter:  1096  Loss on train set:  -51.12766704240689\n",
      "Iter:  1097  Loss on train set:  -51.245593448651775\n",
      "Iter:  1098  Loss on train set:  -51.0541441541035\n",
      "Iter:  1099  Loss on train set:  -51.22285934964086\n",
      "Iter:  1100  Loss on train set:  -51.24559644330024\n",
      "Iter:  1101  Loss on train set:  -51.24088834118294\n",
      "Iter:  1102  Loss on train set:  -51.225996342431415\n",
      "Iter:  1103  Loss on train set:  -51.2386362671229\n",
      "Iter:  1104  Loss on train set:  -51.244206100028144\n",
      "Iter:  1105  Loss on train set:  -51.238814594057544\n",
      "Iter:  1106  Loss on train set:  -51.24559558666197\n",
      "Iter:  1107  Loss on train set:  -51.241719163735304\n",
      "Iter:  1108  Loss on train set:  -51.24587643601061\n",
      "Iter:  1109  Loss on train set:  -51.24752384613425\n",
      "Iter:  1110  Loss on train set:  -51.24535004969132\n",
      "Iter:  1111  Loss on train set:  -51.24752686260695\n",
      "Iter:  1112  Loss on train set:  -51.246026188424196\n",
      "Iter:  1113  Loss on train set:  -51.24673653410444\n",
      "Iter:  1114  Loss on train set:  -51.24960160140547\n",
      "Iter:  1115  Loss on train set:  -51.25030090918463\n",
      "Iter:  1116  Loss on train set:  -51.25030554139449\n",
      "Iter:  1117  Loss on train set:  -51.24986935406653\n",
      "Iter:  1118  Loss on train set:  -51.25030573913516\n",
      "Iter:  1119  Loss on train set:  -51.253162556302144\n",
      "Iter:  1120  Loss on train set:  -51.253347189303845\n",
      "Iter:  1121  Loss on train set:  -51.25228397033524\n",
      "Iter:  1122  Loss on train set:  -51.253347189303845\n",
      "     fun: -51.253347189303845\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.32508827,  0.96979034,  3.02826886, -0.24114102,  0.01278501,\n",
      "       -0.0868654 ,  0.32310667, -0.04495883,  4.46306663])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1123  Loss on train set:  -51.25335132996324\n",
      "Iter:  1124  Loss on train set:  -51.08076922918891\n",
      "Iter:  1125  Loss on train set:  -51.25334637593072\n",
      "Iter:  1126  Loss on train set:  -49.952478348154365\n",
      "Iter:  1127  Loss on train set:  -39.462727672178374\n",
      "Iter:  1128  Loss on train set:  -41.788221210042224\n",
      "Iter:  1129  Loss on train set:  -41.3607054224016\n",
      "Iter:  1130  Loss on train set:  -39.450084458790954\n",
      "Iter:  1131  Loss on train set:  -37.7920280421786\n",
      "Iter:  1132  Loss on train set:  -40.22562401461337\n",
      "Iter:  1133  Loss on train set:  -48.12399645495046\n",
      "Iter:  1134  Loss on train set:  -50.58931239274846\n",
      "Iter:  1135  Loss on train set:  -51.253349387133106\n",
      "Iter:  1136  Loss on train set:  -50.22157604607294\n",
      "Iter:  1137  Loss on train set:  -51.240652924618686\n",
      "Iter:  1138  Loss on train set:  -50.386360191987634\n",
      "Iter:  1139  Loss on train set:  -51.25334805826544\n",
      "Iter:  1140  Loss on train set:  -50.651345449455796\n",
      "Iter:  1141  Loss on train set:  -51.23587284534018\n",
      "Iter:  1142  Loss on train set:  -50.372124332115014\n",
      "Iter:  1143  Loss on train set:  -51.15181888721884\n",
      "Iter:  1144  Loss on train set:  -51.246574142548155\n",
      "Iter:  1145  Loss on train set:  -51.21612370683547\n",
      "Iter:  1146  Loss on train set:  -51.157326036110085\n",
      "Iter:  1147  Loss on train set:  -51.23087745857098\n",
      "Iter:  1148  Loss on train set:  -51.242261143584585\n",
      "Iter:  1149  Loss on train set:  -51.26675202632009\n",
      "Iter:  1150  Loss on train set:  -51.26394465510563\n",
      "Iter:  1151  Loss on train set:  -51.246498439142634\n",
      "Iter:  1152  Loss on train set:  -51.2690096216407\n",
      "Iter:  1153  Loss on train set:  -51.28661036612903\n",
      "Iter:  1154  Loss on train set:  -51.27769551856106\n",
      "Iter:  1155  Loss on train set:  -51.28661270387798\n",
      "Iter:  1156  Loss on train set:  -51.27365157023659\n",
      "Iter:  1157  Loss on train set:  -51.28662044722736\n",
      "Iter:  1158  Loss on train set:  -51.278216231653786\n",
      "Iter:  1159  Loss on train set:  -51.26898430870301\n",
      "Iter:  1160  Loss on train set:  -51.28347107667906\n",
      "Iter:  1161  Loss on train set:  -51.288891161130714\n",
      "Iter:  1162  Loss on train set:  -51.28859817658379\n",
      "Iter:  1163  Loss on train set:  -51.29516350582293\n",
      "Iter:  1164  Loss on train set:  -51.303845348358465\n",
      "Iter:  1165  Loss on train set:  -51.311036608622686\n",
      "Iter:  1166  Loss on train set:  -51.31559288560564\n",
      "Iter:  1167  Loss on train set:  -51.32327753306791\n",
      "Iter:  1168  Loss on train set:  -51.3192464761519\n",
      "Iter:  1169  Loss on train set:  -51.32328303423235\n",
      "Iter:  1170  Loss on train set:  -51.32662958427406\n",
      "Iter:  1171  Loss on train set:  -51.326630925162014\n",
      "Iter:  1172  Loss on train set:  -51.32880722845256\n",
      "Iter:  1173  Loss on train set:  -51.326630925162014\n",
      "     fun: -51.326630925162014\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 2.38565660e+00, -2.16597195e-01,  4.48649355e+00, -7.07806928e-02,\n",
      "       -3.03184894e-03, -2.69500124e-01, -7.03770951e-01,  3.01661881e+00,\n",
      "        4.43418913e-02])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1174  Loss on train set:  -45.33693265614092\n",
      "Iter:  1175  Loss on train set:  -38.47662755780863\n",
      "Iter:  1176  Loss on train set:  -39.54021140325966\n",
      "Iter:  1177  Loss on train set:  -39.79045671056869\n",
      "Iter:  1178  Loss on train set:  -45.75523372437007\n",
      "Iter:  1179  Loss on train set:  -51.3266312094732\n",
      "Iter:  1180  Loss on train set:  -51.11902151176142\n",
      "Iter:  1181  Loss on train set:  -40.229852449635395\n",
      "Iter:  1182  Loss on train set:  -43.892812353674174\n",
      "Iter:  1183  Loss on train set:  -39.91123371314131\n",
      "Iter:  1184  Loss on train set:  -47.69731658335636\n",
      "Iter:  1185  Loss on train set:  -50.741828143023\n",
      "Iter:  1186  Loss on train set:  -48.44676310229449\n",
      "Iter:  1187  Loss on train set:  -50.87747162947061\n",
      "Iter:  1188  Loss on train set:  -48.94927684644525\n",
      "Iter:  1189  Loss on train set:  -50.6257987089122\n",
      "Iter:  1190  Loss on train set:  -51.32662943687659\n",
      "Iter:  1191  Loss on train set:  -50.43530744001321\n",
      "Iter:  1192  Loss on train set:  -51.316173740270685\n",
      "Iter:  1193  Loss on train set:  -50.79214690206825\n",
      "Iter:  1194  Loss on train set:  -51.17587254945429\n",
      "Iter:  1195  Loss on train set:  -51.26954533644897\n",
      "Iter:  1196  Loss on train set:  -51.17987039774904\n",
      "Iter:  1197  Loss on train set:  -51.27966898676766\n",
      "Iter:  1198  Loss on train set:  -51.311112143217066\n",
      "Iter:  1199  Loss on train set:  -51.28197878269683\n",
      "Iter:  1200  Loss on train set:  -51.31719058361161\n",
      "Iter:  1201  Loss on train set:  -51.26549559808246\n",
      "Iter:  1202  Loss on train set:  -51.32284107791837\n",
      "Iter:  1203  Loss on train set:  -51.32663081707378\n",
      "Iter:  1204  Loss on train set:  -51.331658667018544\n",
      "Iter:  1205  Loss on train set:  -51.33248272994948\n",
      "Iter:  1206  Loss on train set:  -51.3271415758643\n",
      "Iter:  1207  Loss on train set:  -51.330231381333824\n",
      "Iter:  1208  Loss on train set:  -51.31902365681526\n",
      "Iter:  1209  Loss on train set:  -51.33322086513202\n",
      "Iter:  1210  Loss on train set:  -51.3252637562855\n",
      "Iter:  1211  Loss on train set:  -51.331817912372664\n",
      "Iter:  1212  Loss on train set:  -51.33438527739309\n",
      "Iter:  1213  Loss on train set:  -51.33438018000284\n",
      "Iter:  1214  Loss on train set:  -51.33407830747077\n",
      "Iter:  1215  Loss on train set:  -51.33323620507791\n",
      "Iter:  1216  Loss on train set:  -51.33170692443816\n",
      "Iter:  1217  Loss on train set:  -51.33541096686737\n",
      "Iter:  1218  Loss on train set:  -51.33636495184498\n",
      "Iter:  1219  Loss on train set:  -51.334155818478585\n",
      "Iter:  1220  Loss on train set:  -51.33275624490771\n",
      "Iter:  1221  Loss on train set:  -51.335887644476315\n",
      "Iter:  1222  Loss on train set:  -51.336261663315454\n",
      "Iter:  1223  Loss on train set:  -51.33696033154998\n",
      "Iter:  1224  Loss on train set:  -51.33636495184498\n",
      "     fun: -51.33636495184498\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.30788078, -1.4592883 ,  3.01493722,  3.82151432,  0.33822966,\n",
      "        1.96978553, -0.19499751, -0.19471883, -0.38739533])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1225  Loss on train set:  -39.574172197877246\n",
      "Iter:  1226  Loss on train set:  -45.916127973110974\n",
      "Iter:  1227  Loss on train set:  -42.59936655485847\n",
      "Iter:  1228  Loss on train set:  -47.01155535053866\n",
      "Iter:  1229  Loss on train set:  -46.550625944127006\n",
      "Iter:  1230  Loss on train set:  -45.50991989310532\n",
      "Iter:  1231  Loss on train set:  -40.055380551242955\n",
      "Iter:  1232  Loss on train set:  -39.88585158452793\n",
      "Iter:  1233  Loss on train set:  -41.54175923624125\n",
      "Iter:  1234  Loss on train set:  -42.48767315796148\n",
      "Iter:  1235  Loss on train set:  -49.11957642311883\n",
      "Iter:  1236  Loss on train set:  -50.382091825942545\n",
      "Iter:  1237  Loss on train set:  -51.24846955100731\n",
      "Iter:  1238  Loss on train set:  -50.748446953921004\n",
      "Iter:  1239  Loss on train set:  -51.18608393005978\n",
      "Iter:  1240  Loss on train set:  -51.08688721360609\n",
      "Iter:  1241  Loss on train set:  -51.22560902714379\n",
      "Iter:  1242  Loss on train set:  -50.764266641376814\n",
      "Iter:  1243  Loss on train set:  -51.207108916716955\n",
      "Iter:  1244  Loss on train set:  -50.48132362201353\n",
      "Iter:  1245  Loss on train set:  -51.26477042896532\n",
      "Iter:  1246  Loss on train set:  -51.28127587043356\n",
      "Iter:  1247  Loss on train set:  -51.33064765374497\n",
      "Iter:  1248  Loss on train set:  -51.310007781566775\n",
      "Iter:  1249  Loss on train set:  -51.3369986114162\n",
      "Iter:  1250  Loss on train set:  -51.28545863400506\n",
      "Iter:  1251  Loss on train set:  -51.33632480776925\n",
      "Iter:  1252  Loss on train set:  -51.31384455258602\n",
      "Iter:  1253  Loss on train set:  -51.3340870752387\n",
      "Iter:  1254  Loss on train set:  -51.34415447555091\n",
      "Iter:  1255  Loss on train set:  -51.32913165114837\n",
      "Iter:  1256  Loss on train set:  -51.345865499751504\n",
      "Iter:  1257  Loss on train set:  -51.34182343188978\n",
      "Iter:  1258  Loss on train set:  -51.34558555868758\n",
      "Iter:  1259  Loss on train set:  -51.33526850460747\n",
      "Iter:  1260  Loss on train set:  -51.351356480155026\n",
      "Iter:  1261  Loss on train set:  -51.34467305177383\n",
      "Iter:  1262  Loss on train set:  -51.35052392165757\n",
      "Iter:  1263  Loss on train set:  -51.35335376948571\n",
      "Iter:  1264  Loss on train set:  -51.351081184394026\n",
      "Iter:  1265  Loss on train set:  -51.3532287469569\n",
      "Iter:  1266  Loss on train set:  -51.35325607768606\n",
      "Iter:  1267  Loss on train set:  -51.35106494422629\n",
      "Iter:  1268  Loss on train set:  -51.3568601336309\n",
      "Iter:  1269  Loss on train set:  -51.35231097116809\n",
      "Iter:  1270  Loss on train set:  -51.35801889823361\n",
      "Iter:  1271  Loss on train set:  -51.3578570210397\n",
      "Iter:  1272  Loss on train set:  -51.35651216731484\n",
      "Iter:  1273  Loss on train set:  -51.35866750503505\n",
      "Iter:  1274  Loss on train set:  -51.35975621057733\n",
      "Iter:  1275  Loss on train set:  -51.35866750503505\n",
      "     fun: -51.35866750503505\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.12440412, -0.03232263, -1.37090639,  0.14059396,  0.0804711 ,\n",
      "       -0.0355857 , -0.20148307,  3.82630571, -0.68405366])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1276  Loss on train set:  -51.35866970867533\n",
      "Iter:  1277  Loss on train set:  -46.354619849003804\n",
      "Iter:  1278  Loss on train set:  -37.88116372549443\n",
      "Iter:  1279  Loss on train set:  -42.8988969976723\n",
      "Iter:  1280  Loss on train set:  -42.46837100739282\n",
      "Iter:  1281  Loss on train set:  -51.358671845013916\n",
      "Iter:  1282  Loss on train set:  -50.6322949327638\n",
      "Iter:  1283  Loss on train set:  -49.43611132561677\n",
      "Iter:  1284  Loss on train set:  -46.060110106116944\n",
      "Iter:  1285  Loss on train set:  -40.71985810866898\n",
      "Iter:  1286  Loss on train set:  -50.03777454791183\n",
      "Iter:  1287  Loss on train set:  -51.35866654994951\n",
      "Iter:  1288  Loss on train set:  -47.95280158722865\n",
      "Iter:  1289  Loss on train set:  -50.87999452875721\n",
      "Iter:  1290  Loss on train set:  -49.04701406765866\n",
      "Iter:  1291  Loss on train set:  -50.973628108238536\n",
      "Iter:  1292  Loss on train set:  -51.358664517035365\n",
      "Iter:  1293  Loss on train set:  -51.20928907916668\n",
      "Iter:  1294  Loss on train set:  -51.372242997770925\n",
      "Iter:  1295  Loss on train set:  -50.75190312300142\n",
      "Iter:  1296  Loss on train set:  -51.26769315719819\n",
      "Iter:  1297  Loss on train set:  -51.339045505154196\n",
      "Iter:  1298  Loss on train set:  -51.1186951390191\n",
      "Iter:  1299  Loss on train set:  -51.37224165198148\n",
      "Iter:  1300  Loss on train set:  -51.272216467667214\n",
      "Iter:  1301  Loss on train set:  -51.37518327352387\n",
      "Iter:  1302  Loss on train set:  -51.26772622178382\n",
      "Iter:  1303  Loss on train set:  -51.3694153372785\n",
      "Iter:  1304  Loss on train set:  -51.27762035564415\n",
      "Iter:  1305  Loss on train set:  -51.360198674931375\n",
      "Iter:  1306  Loss on train set:  -51.37518564467892\n",
      "Iter:  1307  Loss on train set:  -51.36350975588337\n",
      "Iter:  1308  Loss on train set:  -51.37513851995325\n",
      "Iter:  1309  Loss on train set:  -51.31054396495389\n",
      "Iter:  1310  Loss on train set:  -51.361948114712185\n",
      "Iter:  1311  Loss on train set:  -51.376153261535805\n",
      "Iter:  1312  Loss on train set:  -51.37587953189998\n",
      "Iter:  1313  Loss on train set:  -51.37493805776333\n",
      "Iter:  1314  Loss on train set:  -51.37625373165034\n",
      "Iter:  1315  Loss on train set:  -51.37419199938554\n",
      "Iter:  1316  Loss on train set:  -51.374913657108564\n",
      "Iter:  1317  Loss on train set:  -51.383390093571165\n",
      "Iter:  1318  Loss on train set:  -51.37269384301317\n",
      "Iter:  1319  Loss on train set:  -51.38296671353263\n",
      "Iter:  1320  Loss on train set:  -51.38246217193777\n",
      "Iter:  1321  Loss on train set:  -51.38449123959904\n",
      "Iter:  1322  Loss on train set:  -51.3828894773676\n",
      "Iter:  1323  Loss on train set:  -51.384596180010284\n",
      "Iter:  1324  Loss on train set:  -51.38282009023847\n",
      "Iter:  1325  Loss on train set:  -51.388714964900316\n",
      "Iter:  1326  Loss on train set:  -51.384596180010284\n",
      "     fun: -51.384596180010284\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 2.96808768, -0.01560502, -1.4707903 , -0.01537449, -0.28989118,\n",
      "        5.00406841,  3.08996232, -0.10704896,  0.03579838])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1327  Loss on train set:  -50.475693879639884\n",
      "Iter:  1328  Loss on train set:  -45.520208644381455\n",
      "Iter:  1329  Loss on train set:  -40.9389158749059\n",
      "Iter:  1330  Loss on train set:  -46.895416753888874\n",
      "Iter:  1331  Loss on train set:  -44.245811983625444\n",
      "Iter:  1332  Loss on train set:  -46.0565310607273\n",
      "Iter:  1333  Loss on train set:  -48.88278804257034\n",
      "Iter:  1334  Loss on train set:  -39.77562882790077\n",
      "Iter:  1335  Loss on train set:  -51.38459622878015\n",
      "Iter:  1336  Loss on train set:  -41.292684996536714\n",
      "Iter:  1337  Loss on train set:  -49.82727719300105\n",
      "Iter:  1338  Loss on train set:  -51.28125589504265\n",
      "Iter:  1339  Loss on train set:  -48.473740084469476\n",
      "Iter:  1340  Loss on train set:  -51.0104361871565\n",
      "Iter:  1341  Loss on train set:  -49.56023257368908\n",
      "Iter:  1342  Loss on train set:  -51.192289858146864\n",
      "Iter:  1343  Loss on train set:  -49.990573968446576\n",
      "Iter:  1344  Loss on train set:  -48.420934523663\n",
      "Iter:  1345  Loss on train set:  -50.86370035976282\n",
      "Iter:  1346  Loss on train set:  -51.384594415854586\n",
      "Iter:  1347  Loss on train set:  -50.994813283138285\n",
      "Iter:  1348  Loss on train set:  -51.27111158025632\n",
      "Iter:  1349  Loss on train set:  -51.34280921843383\n",
      "Iter:  1350  Loss on train set:  -51.214612776689016\n",
      "Iter:  1351  Loss on train set:  -51.277451436697156\n",
      "Iter:  1352  Loss on train set:  -51.343752606983536\n",
      "Iter:  1353  Loss on train set:  -51.38019957490902\n",
      "Iter:  1354  Loss on train set:  -51.34932013929486\n",
      "Iter:  1355  Loss on train set:  -51.38566451290163\n",
      "Iter:  1356  Loss on train set:  -51.36120795427405\n",
      "Iter:  1357  Loss on train set:  -51.40039561034641\n",
      "Iter:  1358  Loss on train set:  -51.38194371950416\n",
      "Iter:  1359  Loss on train set:  -51.40130882881041\n",
      "Iter:  1360  Loss on train set:  -51.38859240560927\n",
      "Iter:  1361  Loss on train set:  -51.360430901822376\n",
      "Iter:  1362  Loss on train set:  -51.39078816085754\n",
      "Iter:  1363  Loss on train set:  -51.40235272920385\n",
      "Iter:  1364  Loss on train set:  -51.395519008297576\n",
      "Iter:  1365  Loss on train set:  -51.40451299481745\n",
      "Iter:  1366  Loss on train set:  -51.409859139764905\n",
      "Iter:  1367  Loss on train set:  -51.40816574751334\n",
      "Iter:  1368  Loss on train set:  -51.40495768469528\n",
      "Iter:  1369  Loss on train set:  -51.41235523164939\n",
      "Iter:  1370  Loss on train set:  -51.40876420778912\n",
      "Iter:  1371  Loss on train set:  -51.40929495267819\n",
      "Iter:  1372  Loss on train set:  -51.41307490340859\n",
      "Iter:  1373  Loss on train set:  -51.41324037332902\n",
      "Iter:  1374  Loss on train set:  -51.41484408357458\n",
      "Iter:  1375  Loss on train set:  -51.412752847079965\n",
      "Iter:  1376  Loss on train set:  -51.41337382490738\n",
      "Iter:  1377  Loss on train set:  -51.41484408357458\n",
      "     fun: -51.41484408357458\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 3.11485279e+00,  3.46054415e-01, -7.58315281e-01, -4.67930998e-02,\n",
      "       -4.02260301e-01,  6.41094058e-03,  2.07368902e-03,  2.69267634e-02,\n",
      "        3.02806998e+00])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1378  Loss on train set:  -41.86746946958781\n",
      "Iter:  1379  Loss on train set:  -38.829904997737366\n",
      "Iter:  1380  Loss on train set:  -50.85491019348884\n",
      "Iter:  1381  Loss on train set:  -45.73151407595982\n",
      "Iter:  1382  Loss on train set:  -51.41484231183939\n",
      "Iter:  1383  Loss on train set:  -44.02715391991382\n",
      "Iter:  1384  Loss on train set:  -51.41484424126356\n",
      "Iter:  1385  Loss on train set:  -46.64686553020463\n",
      "Iter:  1386  Loss on train set:  -49.974366857758696\n",
      "Iter:  1387  Loss on train set:  -42.83333376254093\n",
      "Iter:  1388  Loss on train set:  -49.0311847715547\n",
      "Iter:  1389  Loss on train set:  -51.468744506770314\n",
      "Iter:  1390  Loss on train set:  -47.98084721604067\n",
      "Iter:  1391  Loss on train set:  -51.46874353498832\n",
      "Iter:  1392  Loss on train set:  -49.99525535176173\n",
      "Iter:  1393  Loss on train set:  -49.63110252314297\n",
      "Iter:  1394  Loss on train set:  -50.957248983610256\n",
      "Iter:  1395  Loss on train set:  -51.468741027667775\n",
      "Iter:  1396  Loss on train set:  -51.209731160522516\n",
      "Iter:  1397  Loss on train set:  -51.42029099759504\n",
      "Iter:  1398  Loss on train set:  -50.87288952950065\n",
      "Iter:  1399  Loss on train set:  -51.37428692169574\n",
      "Iter:  1400  Loss on train set:  -51.42057189706783\n",
      "Iter:  1401  Loss on train set:  -51.24777228264281\n",
      "Iter:  1402  Loss on train set:  -51.469220248135116\n",
      "Iter:  1403  Loss on train set:  -51.460357606332906\n",
      "Iter:  1404  Loss on train set:  -51.44088001091784\n",
      "Iter:  1405  Loss on train set:  -51.47364453009597\n",
      "Iter:  1406  Loss on train set:  -51.43788487555094\n",
      "Iter:  1407  Loss on train set:  -51.47364079041116\n",
      "Iter:  1408  Loss on train set:  -51.46223578717776\n",
      "Iter:  1409  Loss on train set:  -51.473645471583\n",
      "Iter:  1410  Loss on train set:  -51.44012220619869\n",
      "Iter:  1411  Loss on train set:  -51.47010152081671\n",
      "Iter:  1412  Loss on train set:  -51.4555670174114\n",
      "Iter:  1413  Loss on train set:  -51.4746665036292\n",
      "Iter:  1414  Loss on train set:  -51.46303770336532\n",
      "Iter:  1415  Loss on train set:  -51.481114084259474\n",
      "Iter:  1416  Loss on train set:  -51.47584805148256\n",
      "Iter:  1417  Loss on train set:  -51.48488818068023\n",
      "Iter:  1418  Loss on train set:  -51.48997112929471\n",
      "Iter:  1419  Loss on train set:  -51.4914989290318\n",
      "Iter:  1420  Loss on train set:  -51.479329423775305\n",
      "Iter:  1421  Loss on train set:  -51.49150453159966\n",
      "Iter:  1422  Loss on train set:  -51.49075381076046\n",
      "Iter:  1423  Loss on train set:  -51.4953229198193\n",
      "Iter:  1424  Loss on train set:  -51.47998793416047\n",
      "Iter:  1425  Loss on train set:  -51.49740241200201\n",
      "Iter:  1426  Loss on train set:  -51.492908538286414\n",
      "Iter:  1427  Loss on train set:  -51.498041920944644\n",
      "Iter:  1428  Loss on train set:  -51.49740241200201\n",
      "     fun: -51.49740241200201\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.01837053, -0.13345793,  0.29967472, -0.02226452,  1.94047745,\n",
      "        0.21387174,  3.97848696,  0.0809132 , -0.13804907])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1429  Loss on train set:  -51.49740759153154\n",
      "Iter:  1430  Loss on train set:  -40.28161097684222\n",
      "Iter:  1431  Loss on train set:  -39.34282504585754\n",
      "Iter:  1432  Loss on train set:  -51.49741043395544\n",
      "Iter:  1433  Loss on train set:  -51.49741063718204\n",
      "Iter:  1434  Loss on train set:  -44.18949233724519\n",
      "Iter:  1435  Loss on train set:  -51.49740509347198\n",
      "Iter:  1436  Loss on train set:  -38.667042677874285\n",
      "Iter:  1437  Loss on train set:  -42.31424624799293\n",
      "Iter:  1438  Loss on train set:  -43.488517938706046\n",
      "Iter:  1439  Loss on train set:  -47.833761880120925\n",
      "Iter:  1440  Loss on train set:  -51.49740000342483\n",
      "Iter:  1441  Loss on train set:  -48.3986118762039\n",
      "Iter:  1442  Loss on train set:  -51.49740569454408\n",
      "Iter:  1443  Loss on train set:  -49.48621280026382\n",
      "Iter:  1444  Loss on train set:  -50.891481052746485\n",
      "Iter:  1445  Loss on train set:  -51.49740459881945\n",
      "Iter:  1446  Loss on train set:  -50.52264727840219\n",
      "Iter:  1447  Loss on train set:  -51.49740338464714\n",
      "Iter:  1448  Loss on train set:  -51.015435148180046\n",
      "Iter:  1449  Loss on train set:  -51.382345468961816\n",
      "Iter:  1450  Loss on train set:  -51.4286689619019\n",
      "Iter:  1451  Loss on train set:  -51.21290800564685\n",
      "Iter:  1452  Loss on train set:  -51.456133628914\n",
      "Iter:  1453  Loss on train set:  -51.49740107693849\n",
      "Iter:  1454  Loss on train set:  -51.49879676271969\n",
      "Iter:  1455  Loss on train set:  -51.498791751355235\n",
      "Iter:  1456  Loss on train set:  -51.481360532592745\n",
      "Iter:  1457  Loss on train set:  -51.48472058821504\n",
      "Iter:  1458  Loss on train set:  -51.42937232189347\n",
      "Iter:  1459  Loss on train set:  -51.49879994547482\n",
      "Iter:  1460  Loss on train set:  -51.44561337841113\n",
      "Iter:  1461  Loss on train set:  -51.49880212663196\n",
      "Iter:  1462  Loss on train set:  -51.45134154426428\n",
      "Iter:  1463  Loss on train set:  -51.42705291281435\n",
      "Iter:  1464  Loss on train set:  -51.49718815231498\n",
      "Iter:  1465  Loss on train set:  -51.498799620899156\n",
      "Iter:  1466  Loss on train set:  -51.4831066433531\n",
      "Iter:  1467  Loss on train set:  -51.48652444086916\n",
      "Iter:  1468  Loss on train set:  -51.49783810781998\n",
      "Iter:  1469  Loss on train set:  -51.50207634283661\n",
      "Iter:  1470  Loss on train set:  -51.50082165539979\n",
      "Iter:  1471  Loss on train set:  -51.502081358152076\n",
      "Iter:  1472  Loss on train set:  -51.50383729507166\n",
      "Iter:  1473  Loss on train set:  -51.50384390130056\n",
      "Iter:  1474  Loss on train set:  -51.50840428331675\n",
      "Iter:  1475  Loss on train set:  -51.505861736056076\n",
      "Iter:  1476  Loss on train set:  -51.50650263272609\n",
      "Iter:  1477  Loss on train set:  -51.50714002327223\n",
      "Iter:  1478  Loss on train set:  -51.50840355195234\n",
      "Iter:  1479  Loss on train set:  -51.50840428331675\n",
      "     fun: -51.50840428331675\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 3.96808427, -0.21749348,  2.99991269,  4.98631134,  0.9594757 ,\n",
      "        0.20529134,  4.5255613 , -1.46084597, -0.65369374])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1480  Loss on train set:  -45.50390909570688\n",
      "Iter:  1481  Loss on train set:  -37.601934158142804\n",
      "Iter:  1482  Loss on train set:  -42.343744249258165\n",
      "Iter:  1483  Loss on train set:  -51.50840110844037\n",
      "Iter:  1484  Loss on train set:  -38.23209563350643\n",
      "Iter:  1485  Loss on train set:  -49.28597391435598\n",
      "Iter:  1486  Loss on train set:  -45.985172286992935\n",
      "Iter:  1487  Loss on train set:  -40.011717399204045\n",
      "Iter:  1488  Loss on train set:  -45.44796682107527\n",
      "Iter:  1489  Loss on train set:  -43.198326459611266\n",
      "Iter:  1490  Loss on train set:  -49.27066865070162\n",
      "Iter:  1491  Loss on train set:  -50.374984613653545\n",
      "Iter:  1492  Loss on train set:  -51.36778228409108\n",
      "Iter:  1493  Loss on train set:  -51.002443181511524\n",
      "Iter:  1494  Loss on train set:  -51.50840509096731\n",
      "Iter:  1495  Loss on train set:  -50.667319257083335\n",
      "Iter:  1496  Loss on train set:  -51.46392976984934\n",
      "Iter:  1497  Loss on train set:  -51.13771010881322\n",
      "Iter:  1498  Loss on train set:  -51.424951887426715\n",
      "Iter:  1499  Loss on train set:  -50.68537509327653\n",
      "Iter:  1500  Loss on train set:  -51.36394641472159\n",
      "Iter:  1501  Loss on train set:  -51.47176208037159\n",
      "Iter:  1502  Loss on train set:  -51.24040756391835\n",
      "Iter:  1503  Loss on train set:  -51.45868183540863\n",
      "Iter:  1504  Loss on train set:  -51.504021446688085\n",
      "Iter:  1505  Loss on train set:  -51.46209208313186\n",
      "Iter:  1506  Loss on train set:  -51.4642044916939\n",
      "Iter:  1507  Loss on train set:  -51.4857673378426\n",
      "Iter:  1508  Loss on train set:  -51.508688102249735\n",
      "Iter:  1509  Loss on train set:  -51.501911095873425\n",
      "Iter:  1510  Loss on train set:  -51.508697518550555\n",
      "Iter:  1511  Loss on train set:  -51.50961677918135\n",
      "Iter:  1512  Loss on train set:  -51.5012204571954\n",
      "Iter:  1513  Loss on train set:  -51.509713208797336\n",
      "Iter:  1514  Loss on train set:  -51.50651816480875\n",
      "Iter:  1515  Loss on train set:  -51.49403298915985\n",
      "Iter:  1516  Loss on train set:  -51.50558058336996\n",
      "Iter:  1517  Loss on train set:  -51.50904783338972\n",
      "Iter:  1518  Loss on train set:  -51.50902292333996\n",
      "Iter:  1519  Loss on train set:  -51.50985920944161\n",
      "Iter:  1520  Loss on train set:  -51.509851217514175\n",
      "Iter:  1521  Loss on train set:  -51.504398924804\n",
      "Iter:  1522  Loss on train set:  -51.508386810728176\n",
      "Iter:  1523  Loss on train set:  -51.50773793954618\n",
      "Iter:  1524  Loss on train set:  -51.50759437344608\n",
      "Iter:  1525  Loss on train set:  -51.51234611441728\n",
      "Iter:  1526  Loss on train set:  -51.50876978498653\n",
      "Iter:  1527  Loss on train set:  -51.5124426846072\n",
      "Iter:  1528  Loss on train set:  -51.50956271509698\n",
      "Iter:  1529  Loss on train set:  -51.5116178873884\n",
      "Iter:  1530  Loss on train set:  -51.5124426846072\n",
      "     fun: -51.5124426846072\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.05702205,  0.0157911 , -0.02072094,  4.38817537, -1.45663721,\n",
      "        0.01323704, -0.03052502,  3.82782109,  0.2835581 ])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1531  Loss on train set:  -39.4450066408596\n",
      "Iter:  1532  Loss on train set:  -51.51243636737096\n",
      "Iter:  1533  Loss on train set:  -51.51244125020895\n",
      "Iter:  1534  Loss on train set:  -46.64463122838578\n",
      "Iter:  1535  Loss on train set:  -40.75815516320201\n",
      "Iter:  1536  Loss on train set:  -44.613972991232394\n",
      "Iter:  1537  Loss on train set:  -40.5608418265831\n",
      "Iter:  1538  Loss on train set:  -41.34166509929414\n",
      "Iter:  1539  Loss on train set:  -46.06138502951142\n",
      "Iter:  1540  Loss on train set:  -34.233063984262635\n",
      "Iter:  1541  Loss on train set:  -46.47145251456626\n",
      "Iter:  1542  Loss on train set:  -50.82562503813042\n",
      "Iter:  1543  Loss on train set:  -51.51243795997919\n",
      "Iter:  1544  Loss on train set:  -51.175901065531775\n",
      "Iter:  1545  Loss on train set:  -51.512439272083945\n",
      "Iter:  1546  Loss on train set:  -51.16449474044318\n",
      "Iter:  1547  Loss on train set:  -51.35805862664363\n",
      "Iter:  1548  Loss on train set:  -50.884791720267266\n",
      "Iter:  1549  Loss on train set:  -51.25997343462312\n",
      "Iter:  1550  Loss on train set:  -50.94271180923083\n",
      "Iter:  1551  Loss on train set:  -51.24970737205187\n",
      "Iter:  1552  Loss on train set:  -51.49755272778725\n",
      "Iter:  1553  Loss on train set:  -51.50907158969817\n",
      "Iter:  1554  Loss on train set:  -51.489811097651256\n",
      "Iter:  1555  Loss on train set:  -51.51346983124869\n",
      "Iter:  1556  Loss on train set:  -51.47112278738248\n",
      "Iter:  1557  Loss on train set:  -51.48158407612567\n",
      "Iter:  1558  Loss on train set:  -51.47783711310816\n",
      "Iter:  1559  Loss on train set:  -51.51346341581211\n",
      "Iter:  1560  Loss on train set:  -51.50559866545952\n",
      "Iter:  1561  Loss on train set:  -51.51346458542746\n",
      "Iter:  1562  Loss on train set:  -51.51215973229522\n",
      "Iter:  1563  Loss on train set:  -51.51260262726351\n",
      "Iter:  1564  Loss on train set:  -51.519824697367184\n",
      "Iter:  1565  Loss on train set:  -51.52687179075113\n",
      "Iter:  1566  Loss on train set:  -51.52589667716303\n",
      "Iter:  1567  Loss on train set:  -51.52576481227894\n",
      "Iter:  1568  Loss on train set:  -51.53141248936354\n",
      "Iter:  1569  Loss on train set:  -51.52786969725202\n",
      "Iter:  1570  Loss on train set:  -51.532441683386516\n",
      "Iter:  1571  Loss on train set:  -51.53192736683705\n",
      "Iter:  1572  Loss on train set:  -51.53425978892403\n",
      "Iter:  1573  Loss on train set:  -51.536525593525255\n",
      "Iter:  1574  Loss on train set:  -51.535812639588116\n",
      "Iter:  1575  Loss on train set:  -51.53698171064033\n",
      "Iter:  1576  Loss on train set:  -51.537404621631325\n",
      "Iter:  1577  Loss on train set:  -51.53795862635001\n",
      "Iter:  1578  Loss on train set:  -51.53663233647482\n",
      "Iter:  1579  Loss on train set:  -51.537451202776786\n",
      "Iter:  1580  Loss on train set:  -51.53873332088382\n",
      "Iter:  1581  Loss on train set:  -51.53795862635001\n",
      "     fun: -51.53795862635001\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.12705927,  3.97594135,  4.38585706, -0.02020518, -0.33521116,\n",
      "       -0.39609108, -0.24143506,  0.14908075, -0.25486759])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1582  Loss on train set:  -40.00608624419017\n",
      "Iter:  1583  Loss on train set:  -39.45443228918194\n",
      "Iter:  1584  Loss on train set:  -40.271504590928735\n",
      "Iter:  1585  Loss on train set:  -50.694259622734315\n",
      "Iter:  1586  Loss on train set:  -42.65117314041494\n",
      "Iter:  1587  Loss on train set:  -45.48066462438391\n",
      "Iter:  1588  Loss on train set:  -39.17393722897979\n",
      "Iter:  1589  Loss on train set:  -45.859715677921585\n",
      "Iter:  1590  Loss on train set:  -49.984486543210494\n",
      "Iter:  1591  Loss on train set:  -36.34749251722688\n",
      "Iter:  1592  Loss on train set:  -47.29879232794412\n",
      "Iter:  1593  Loss on train set:  -51.154784615867925\n",
      "Iter:  1594  Loss on train set:  -51.31888860284562\n",
      "Iter:  1595  Loss on train set:  -50.80626377486826\n",
      "Iter:  1596  Loss on train set:  -51.51657991581504\n",
      "Iter:  1597  Loss on train set:  -50.94081378048942\n",
      "Iter:  1598  Loss on train set:  -51.454556964500554\n",
      "Iter:  1599  Loss on train set:  -50.63246377114694\n",
      "Iter:  1600  Loss on train set:  -51.50546174104309\n",
      "Iter:  1601  Loss on train set:  -51.13181730870128\n",
      "Iter:  1602  Loss on train set:  -51.29689864908865\n",
      "Iter:  1603  Loss on train set:  -51.48651304242471\n",
      "Iter:  1604  Loss on train set:  -51.51691541023432\n",
      "Iter:  1605  Loss on train set:  -51.53101170719959\n",
      "Iter:  1606  Loss on train set:  -51.52801776683213\n",
      "Iter:  1607  Loss on train set:  -51.46328374781721\n",
      "Iter:  1608  Loss on train set:  -51.53794524208866\n",
      "Iter:  1609  Loss on train set:  -51.53697072014285\n",
      "Iter:  1610  Loss on train set:  -51.53098676379886\n",
      "Iter:  1611  Loss on train set:  -51.53717595650604\n",
      "Iter:  1612  Loss on train set:  -51.53132794358143\n",
      "Iter:  1613  Loss on train set:  -51.536949104863595\n",
      "Iter:  1614  Loss on train set:  -51.534380209878606\n",
      "Iter:  1615  Loss on train set:  -51.53963805813826\n",
      "Iter:  1616  Loss on train set:  -51.53779825953028\n",
      "Iter:  1617  Loss on train set:  -51.53889281831456\n",
      "Iter:  1618  Loss on train set:  -51.54187828900644\n",
      "Iter:  1619  Loss on train set:  -51.5456842840941\n",
      "Iter:  1620  Loss on train set:  -51.546779941176375\n",
      "Iter:  1621  Loss on train set:  -51.54552758931791\n",
      "Iter:  1622  Loss on train set:  -51.54756406673619\n",
      "Iter:  1623  Loss on train set:  -51.547365096694506\n",
      "Iter:  1624  Loss on train set:  -51.55123279768674\n",
      "Iter:  1625  Loss on train set:  -51.55220208951892\n",
      "Iter:  1626  Loss on train set:  -51.553010215829886\n",
      "Iter:  1627  Loss on train set:  -51.552740473843244\n",
      "Iter:  1628  Loss on train set:  -51.554652172576716\n",
      "Iter:  1629  Loss on train set:  -51.55716246093741\n",
      "Iter:  1630  Loss on train set:  -51.5579194728795\n",
      "Iter:  1631  Loss on train set:  -51.55792301106072\n",
      "Iter:  1632  Loss on train set:  -51.5579194728795\n",
      "     fun: -51.5579194728795\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.06290487,  2.98617663, -0.26287008, -0.0101418 , -0.30575391,\n",
      "       -0.24786692, -0.14117004,  0.3498927 , -0.13802971])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1633  Loss on train set:  -41.78090062359175\n",
      "Iter:  1634  Loss on train set:  -46.05144576506023\n",
      "Iter:  1635  Loss on train set:  -50.70545238499985\n",
      "Iter:  1636  Loss on train set:  -39.05249912292007\n",
      "Iter:  1637  Loss on train set:  -41.43278754076482\n",
      "Iter:  1638  Loss on train set:  -42.04952039739389\n",
      "Iter:  1639  Loss on train set:  -51.55792472026628\n",
      "Iter:  1640  Loss on train set:  -45.67339396416516\n",
      "Iter:  1641  Loss on train set:  -46.334453922907905\n",
      "Iter:  1642  Loss on train set:  -39.75854938368208\n",
      "Iter:  1643  Loss on train set:  -49.580862640365105\n",
      "Iter:  1644  Loss on train set:  -51.475335366103806\n",
      "Iter:  1645  Loss on train set:  -50.116326170782145\n",
      "Iter:  1646  Loss on train set:  -50.9416559417116\n",
      "Iter:  1647  Loss on train set:  -48.30461994805056\n",
      "Iter:  1648  Loss on train set:  -49.102835635154996\n",
      "Iter:  1649  Loss on train set:  -51.171910776838864\n",
      "Iter:  1650  Loss on train set:  -51.55792210801531\n",
      "Iter:  1651  Loss on train set:  -51.175920470989496\n",
      "Iter:  1652  Loss on train set:  -50.739012948744914\n",
      "Iter:  1653  Loss on train set:  -51.428298058805936\n",
      "Iter:  1654  Loss on train set:  -51.530614942795964\n",
      "Iter:  1655  Loss on train set:  -51.410302466527426\n",
      "Iter:  1656  Loss on train set:  -51.30388746948376\n",
      "Iter:  1657  Loss on train set:  -51.5204845122444\n",
      "Iter:  1658  Loss on train set:  -51.559069340600544\n",
      "Iter:  1659  Loss on train set:  -51.526541326866464\n",
      "Iter:  1660  Loss on train set:  -51.55478930121552\n",
      "Iter:  1661  Loss on train set:  -51.517045313294524\n",
      "Iter:  1662  Loss on train set:  -51.54321066990717\n",
      "Iter:  1663  Loss on train set:  -51.55908122402905\n",
      "Iter:  1664  Loss on train set:  -51.54789396513129\n",
      "Iter:  1665  Loss on train set:  -51.56150953990786\n",
      "Iter:  1666  Loss on train set:  -51.545241418490285\n",
      "Iter:  1667  Loss on train set:  -51.55864008634214\n",
      "Iter:  1668  Loss on train set:  -51.55497076725294\n",
      "Iter:  1669  Loss on train set:  -51.55611235062204\n",
      "Iter:  1670  Loss on train set:  -51.55807711681976\n",
      "Iter:  1671  Loss on train set:  -51.56155029343676\n",
      "Iter:  1672  Loss on train set:  -51.56106543725956\n",
      "Iter:  1673  Loss on train set:  -51.56129616874837\n",
      "Iter:  1674  Loss on train set:  -51.56014466568416\n",
      "Iter:  1675  Loss on train set:  -51.562174015885326\n",
      "Iter:  1676  Loss on train set:  -51.561126473185055\n",
      "Iter:  1677  Loss on train set:  -51.56236068689569\n",
      "Iter:  1678  Loss on train set:  -51.562364615246295\n",
      "Iter:  1679  Loss on train set:  -51.561705216444146\n",
      "Iter:  1680  Loss on train set:  -51.56255763541961\n",
      "Iter:  1681  Loss on train set:  -51.56234683758484\n",
      "Iter:  1682  Loss on train set:  -51.56245564175291\n",
      "Iter:  1683  Loss on train set:  -51.56255763541961\n",
      "     fun: -51.56255763541961\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.65924032, -0.03158115,  0.02955978, -0.14747282, -0.77264417,\n",
      "        0.00942555,  4.95639231,  0.2852178 , -0.01185325])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1684  Loss on train set:  -42.92330092187708\n",
      "Iter:  1685  Loss on train set:  -50.40569481904632\n",
      "Iter:  1686  Loss on train set:  -47.505070608710476\n",
      "Iter:  1687  Loss on train set:  -46.109417137037966\n",
      "Iter:  1688  Loss on train set:  -51.56255795063379\n",
      "Iter:  1689  Loss on train set:  -51.56255872954331\n",
      "Iter:  1690  Loss on train set:  -47.23110958671492\n",
      "Iter:  1691  Loss on train set:  -45.57103599336126\n",
      "Iter:  1692  Loss on train set:  -51.56255745896311\n",
      "Iter:  1693  Loss on train set:  -44.808979101185216\n",
      "Iter:  1694  Loss on train set:  -49.60987117304299\n",
      "Iter:  1695  Loss on train set:  -51.44755694586037\n",
      "Iter:  1696  Loss on train set:  -50.47112895343901\n",
      "Iter:  1697  Loss on train set:  -51.56255287680494\n",
      "Iter:  1698  Loss on train set:  -49.98659559928389\n",
      "Iter:  1699  Loss on train set:  -51.15193014832853\n",
      "Iter:  1700  Loss on train set:  -51.562557432136586\n",
      "Iter:  1701  Loss on train set:  -51.24411470644043\n",
      "Iter:  1702  Loss on train set:  -51.562558197237806\n",
      "Iter:  1703  Loss on train set:  -51.14832691618746\n",
      "Iter:  1704  Loss on train set:  -51.4601574301575\n",
      "Iter:  1705  Loss on train set:  -51.534910924329985\n",
      "Iter:  1706  Loss on train set:  -51.45939776873323\n",
      "Iter:  1707  Loss on train set:  -51.53516038337018\n",
      "Iter:  1708  Loss on train set:  -51.562556950820834\n",
      "Iter:  1709  Loss on train set:  -51.54926498276957\n",
      "Iter:  1710  Loss on train set:  -51.54987354769732\n",
      "Iter:  1711  Loss on train set:  -51.54635622961029\n",
      "Iter:  1712  Loss on train set:  -51.56005790961841\n",
      "Iter:  1713  Loss on train set:  -51.562560414751275\n",
      "Iter:  1714  Loss on train set:  -51.560694516645576\n",
      "Iter:  1715  Loss on train set:  -51.562563506618616\n",
      "Iter:  1716  Loss on train set:  -51.55911327590465\n",
      "Iter:  1717  Loss on train set:  -51.562873303502826\n",
      "Iter:  1718  Loss on train set:  -51.55479912689286\n",
      "Iter:  1719  Loss on train set:  -51.561355800841525\n",
      "Iter:  1720  Loss on train set:  -51.56396771772269\n",
      "Iter:  1721  Loss on train set:  -51.563955657162325\n",
      "Iter:  1722  Loss on train set:  -51.55889559697572\n",
      "Iter:  1723  Loss on train set:  -51.56429089351487\n",
      "Iter:  1724  Loss on train set:  -51.5663301133074\n",
      "Iter:  1725  Loss on train set:  -51.56591437521446\n",
      "Iter:  1726  Loss on train set:  -51.56632160397219\n",
      "Iter:  1727  Loss on train set:  -51.56837190053354\n",
      "Iter:  1728  Loss on train set:  -51.56836600710248\n",
      "Iter:  1729  Loss on train set:  -51.56765157918935\n",
      "Iter:  1730  Loss on train set:  -51.56835047390547\n",
      "Iter:  1731  Loss on train set:  -51.56771589608085\n",
      "Iter:  1732  Loss on train set:  -51.568373352841256\n",
      "Iter:  1733  Loss on train set:  -51.56842695643543\n",
      "Iter:  1734  Loss on train set:  -51.568373352841256\n",
      "     fun: -51.568373352841256\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.01304213, -0.024875  , -0.06527772, -0.03650105,  5.98237024,\n",
      "        2.92484568,  0.12262744,  0.27665928,  0.97505768])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1735  Loss on train set:  -47.62794623857682\n",
      "Iter:  1736  Loss on train set:  -37.62606782291206\n",
      "Iter:  1737  Loss on train set:  -44.51343477239987\n",
      "Iter:  1738  Loss on train set:  -39.24370982642802\n",
      "Iter:  1739  Loss on train set:  -46.472167351906705\n",
      "Iter:  1740  Loss on train set:  -51.56836976879785\n",
      "Iter:  1741  Loss on train set:  -50.65380680246765\n",
      "Iter:  1742  Loss on train set:  -39.53901448476359\n",
      "Iter:  1743  Loss on train set:  -40.53453684234321\n",
      "Iter:  1744  Loss on train set:  -42.140981149117565\n",
      "Iter:  1745  Loss on train set:  -49.14121083104782\n",
      "Iter:  1746  Loss on train set:  -50.54788602666701\n",
      "Iter:  1747  Loss on train set:  -51.46574304192304\n",
      "Iter:  1748  Loss on train set:  -51.14089038057487\n",
      "Iter:  1749  Loss on train set:  -51.4537421943414\n",
      "Iter:  1750  Loss on train set:  -50.857917347106245\n",
      "Iter:  1751  Loss on train set:  -51.56837253315286\n",
      "Iter:  1752  Loss on train set:  -50.7106553717391\n",
      "Iter:  1753  Loss on train set:  -51.55880527545304\n",
      "Iter:  1754  Loss on train set:  -50.82169787106892\n",
      "Iter:  1755  Loss on train set:  -51.42399544203856\n",
      "Iter:  1756  Loss on train set:  -51.50248232506048\n",
      "Iter:  1757  Loss on train set:  -51.55970908240262\n",
      "Iter:  1758  Loss on train set:  -51.50992213219577\n",
      "Iter:  1759  Loss on train set:  -51.549610627558415\n",
      "Iter:  1760  Loss on train set:  -51.54339444494654\n",
      "Iter:  1761  Loss on train set:  -51.56079539170908\n",
      "Iter:  1762  Loss on train set:  -51.568370653072\n",
      "Iter:  1763  Loss on train set:  -51.56354524411561\n",
      "Iter:  1764  Loss on train set:  -51.56701102712108\n",
      "Iter:  1765  Loss on train set:  -51.55993744485861\n",
      "Iter:  1766  Loss on train set:  -51.56469334938673\n",
      "Iter:  1767  Loss on train set:  -51.56708163293597\n",
      "Iter:  1768  Loss on train set:  -51.57092120191907\n",
      "Iter:  1769  Loss on train set:  -51.567090371363726\n",
      "Iter:  1770  Loss on train set:  -51.57118146823369\n",
      "Iter:  1771  Loss on train set:  -51.56874166701725\n",
      "Iter:  1772  Loss on train set:  -51.57052180024637\n",
      "Iter:  1773  Loss on train set:  -51.57191101530636\n",
      "Iter:  1774  Loss on train set:  -51.57310041836504\n",
      "Iter:  1775  Loss on train set:  -51.572585264690076\n",
      "Iter:  1776  Loss on train set:  -51.57310822074616\n",
      "Iter:  1777  Loss on train set:  -51.57248264057871\n",
      "Iter:  1778  Loss on train set:  -51.57292153863131\n",
      "Iter:  1779  Loss on train set:  -51.572533613084175\n",
      "Iter:  1780  Loss on train set:  -51.573092405714064\n",
      "Iter:  1781  Loss on train set:  -51.572832408375085\n",
      "Iter:  1782  Loss on train set:  -51.57298349426006\n",
      "Iter:  1783  Loss on train set:  -51.572482417284675\n",
      "Iter:  1784  Loss on train set:  -51.5733005180259\n",
      "Iter:  1785  Loss on train set:  -51.57310822074616\n",
      "     fun: -51.57310822074616\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.07318118,  0.01140589,  0.23105233, -0.14660351, -0.0112382 ,\n",
      "        0.9711442 ,  0.02772541,  2.9804277 , -0.26262168])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1786  Loss on train set:  -46.1175167807373\n",
      "Iter:  1787  Loss on train set:  -38.90045364945532\n",
      "Iter:  1788  Loss on train set:  -45.671505998607074\n",
      "Iter:  1789  Loss on train set:  -51.338057671213214\n",
      "Iter:  1790  Loss on train set:  -42.988836101095124\n",
      "Iter:  1791  Loss on train set:  -51.57310191005705\n",
      "Iter:  1792  Loss on train set:  -51.57311115963974\n",
      "Iter:  1793  Loss on train set:  -51.57311173741285\n",
      "Iter:  1794  Loss on train set:  -51.57311137742778\n",
      "Iter:  1795  Loss on train set:  -39.94804922981413\n",
      "Iter:  1796  Loss on train set:  -50.004033900653056\n",
      "Iter:  1797  Loss on train set:  -51.2659411409281\n",
      "Iter:  1798  Loss on train set:  -48.632512039013754\n",
      "Iter:  1799  Loss on train set:  -51.52439919870771\n",
      "Iter:  1800  Loss on train set:  -49.33352938876689\n",
      "Iter:  1801  Loss on train set:  -51.57311048004887\n",
      "Iter:  1802  Loss on train set:  -48.32126422980186\n",
      "Iter:  1803  Loss on train set:  -51.573108404591466\n",
      "Iter:  1804  Loss on train set:  -48.32126452316769\n",
      "Iter:  1805  Loss on train set:  -50.66172628676409\n",
      "Iter:  1806  Loss on train set:  -51.57311587448237\n",
      "Iter:  1807  Loss on train set:  -50.97660620266671\n",
      "Iter:  1808  Loss on train set:  -51.573115497617145\n",
      "Iter:  1809  Loss on train set:  -50.9502042875999\n",
      "Iter:  1810  Loss on train set:  -51.464764732796624\n",
      "Iter:  1811  Loss on train set:  -51.57608491576437\n",
      "Iter:  1812  Loss on train set:  -51.279074328029274\n",
      "Iter:  1813  Loss on train set:  -51.57608714479526\n",
      "Iter:  1814  Loss on train set:  -51.51887899275561\n",
      "Iter:  1815  Loss on train set:  -51.576088115050865\n",
      "Iter:  1816  Loss on train set:  -51.42462378392973\n",
      "Iter:  1817  Loss on train set:  -51.461650120983336\n",
      "Iter:  1818  Loss on train set:  -51.551953797688675\n",
      "Iter:  1819  Loss on train set:  -51.57608717408532\n",
      "Iter:  1820  Loss on train set:  -51.48596841330174\n",
      "Iter:  1821  Loss on train set:  -51.57608110587559\n",
      "Iter:  1822  Loss on train set:  -51.57088055512646\n",
      "Iter:  1823  Loss on train set:  -51.58782264742545\n",
      "Iter:  1824  Loss on train set:  -51.58415048933153\n",
      "Iter:  1825  Loss on train set:  -51.588348726237065\n",
      "Iter:  1826  Loss on train set:  -51.575486379169696\n",
      "Iter:  1827  Loss on train set:  -51.588339676804154\n",
      "Iter:  1828  Loss on train set:  -51.57988600557322\n",
      "Iter:  1829  Loss on train set:  -51.58834891287647\n",
      "Iter:  1830  Loss on train set:  -51.5888254114862\n",
      "Iter:  1831  Loss on train set:  -51.58989298163922\n",
      "Iter:  1832  Loss on train set:  -51.588750534404824\n",
      "Iter:  1833  Loss on train set:  -51.589566005444915\n",
      "Iter:  1834  Loss on train set:  -51.58754491789475\n",
      "Iter:  1835  Loss on train set:  -51.59121480737591\n",
      "Iter:  1836  Loss on train set:  -51.58989298163922\n",
      "     fun: -51.58989298163922\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.04469276, -1.42419384,  0.25748808, -0.10942648, -0.02691389,\n",
      "        2.98087359,  0.68720878,  2.08995625,  4.95020103])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1837  Loss on train set:  -39.51908998706899\n",
      "Iter:  1838  Loss on train set:  -51.589891106785174\n",
      "Iter:  1839  Loss on train set:  -38.203023476049644\n",
      "Iter:  1840  Loss on train set:  -50.459422590535546\n",
      "Iter:  1841  Loss on train set:  -50.689004654603416\n",
      "Iter:  1842  Loss on train set:  -49.251899839501334\n",
      "Iter:  1843  Loss on train set:  -45.80494253393453\n",
      "Iter:  1844  Loss on train set:  -50.65581086286575\n",
      "Iter:  1845  Loss on train set:  -40.646285116512516\n",
      "Iter:  1846  Loss on train set:  -44.36761841976604\n",
      "Iter:  1847  Loss on train set:  -49.63735599884953\n",
      "Iter:  1848  Loss on train set:  -50.88307547665472\n",
      "Iter:  1849  Loss on train set:  -51.58988992768148\n",
      "Iter:  1850  Loss on train set:  -50.07728327566336\n",
      "Iter:  1851  Loss on train set:  -51.567616519308004\n",
      "Iter:  1852  Loss on train set:  -51.10789871526385\n",
      "Iter:  1853  Loss on train set:  -51.57282613935199\n",
      "Iter:  1854  Loss on train set:  -51.375425370573\n",
      "Iter:  1855  Loss on train set:  -51.56959058803256\n",
      "Iter:  1856  Loss on train set:  -50.861069048145005\n",
      "Iter:  1857  Loss on train set:  -51.404441901514424\n",
      "Iter:  1858  Loss on train set:  -51.55767082521192\n",
      "Iter:  1859  Loss on train set:  -51.574367283006865\n",
      "Iter:  1860  Loss on train set:  -51.593439435892\n",
      "Iter:  1861  Loss on train set:  -51.61730318679948\n",
      "Iter:  1862  Loss on train set:  -51.666464665510475\n",
      "Iter:  1863  Loss on train set:  -51.66477397652729\n",
      "Iter:  1864  Loss on train set:  -51.663671872116836\n",
      "Iter:  1865  Loss on train set:  -51.640710357380755\n",
      "Iter:  1866  Loss on train set:  -51.66529961320473\n",
      "Iter:  1867  Loss on train set:  -51.6658981223051\n",
      "Iter:  1868  Loss on train set:  -51.66647063164621\n",
      "Iter:  1869  Loss on train set:  -51.639124153698894\n",
      "Iter:  1870  Loss on train set:  -51.66515688561776\n",
      "Iter:  1871  Loss on train set:  -51.67521911391166\n",
      "Iter:  1872  Loss on train set:  -51.65773749011677\n",
      "Iter:  1873  Loss on train set:  -51.66590020229475\n",
      "Iter:  1874  Loss on train set:  -51.6757309337937\n",
      "Iter:  1875  Loss on train set:  -51.66396578079999\n",
      "Iter:  1876  Loss on train set:  -51.69511432225145\n",
      "Iter:  1877  Loss on train set:  -51.70288039336275\n",
      "Iter:  1878  Loss on train set:  -51.7179826805784\n",
      "Iter:  1879  Loss on train set:  -51.72514635277256\n",
      "Iter:  1880  Loss on train set:  -51.7284527629351\n",
      "Iter:  1881  Loss on train set:  -51.72503676336749\n",
      "Iter:  1882  Loss on train set:  -51.731092527412\n",
      "Iter:  1883  Loss on train set:  -51.72750732694429\n",
      "Iter:  1884  Loss on train set:  -51.73107290197024\n",
      "Iter:  1885  Loss on train set:  -51.72353974414466\n",
      "Iter:  1886  Loss on train set:  -51.730991547595906\n",
      "Iter:  1887  Loss on train set:  -51.731092527412\n",
      "     fun: -51.731092527412\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.16745983,  3.04420043, -1.2679317 , -0.02740771,  0.04103211,\n",
      "       -0.0118595 , -0.06312889,  3.14152839,  0.25417818])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1888  Loss on train set:  -51.731089318449776\n",
      "Iter:  1889  Loss on train set:  -39.662759154006885\n",
      "Iter:  1890  Loss on train set:  -41.689620353118116\n",
      "Iter:  1891  Loss on train set:  -40.3367577735237\n",
      "Iter:  1892  Loss on train set:  -50.51387894555849\n",
      "Iter:  1893  Loss on train set:  -49.48579199934917\n",
      "Iter:  1894  Loss on train set:  -45.85116987012042\n",
      "Iter:  1895  Loss on train set:  -51.42893563478284\n",
      "Iter:  1896  Loss on train set:  -42.31968654370935\n",
      "Iter:  1897  Loss on train set:  -41.460500086175585\n",
      "Iter:  1898  Loss on train set:  -49.03696934912102\n",
      "Iter:  1899  Loss on train set:  -50.88452170867601\n",
      "Iter:  1900  Loss on train set:  -51.73108177799048\n",
      "Iter:  1901  Loss on train set:  -50.92248623402165\n",
      "Iter:  1902  Loss on train set:  -51.72156920345326\n",
      "Iter:  1903  Loss on train set:  -51.09137726166048\n",
      "Iter:  1904  Loss on train set:  -51.696310810749246\n",
      "Iter:  1905  Loss on train set:  -51.31003119413705\n",
      "Iter:  1906  Loss on train set:  -51.71835068362628\n",
      "Iter:  1907  Loss on train set:  -51.145872165986226\n",
      "Iter:  1908  Loss on train set:  -51.575907065520745\n",
      "Iter:  1909  Loss on train set:  -51.705775988250295\n",
      "Iter:  1910  Loss on train set:  -51.7012827241059\n",
      "Iter:  1911  Loss on train set:  -51.70318695603197\n",
      "Iter:  1912  Loss on train set:  -51.72525097156051\n",
      "Iter:  1913  Loss on train set:  -51.687786895727186\n",
      "Iter:  1914  Loss on train set:  -51.73002253699222\n",
      "Iter:  1915  Loss on train set:  -51.731090544933316\n",
      "Iter:  1916  Loss on train set:  -51.732671158880095\n",
      "Iter:  1917  Loss on train set:  -51.73082149565526\n",
      "Iter:  1918  Loss on train set:  -51.73356512082841\n",
      "Iter:  1919  Loss on train set:  -51.72726731224345\n",
      "Iter:  1920  Loss on train set:  -51.72977103498447\n",
      "Iter:  1921  Loss on train set:  -51.735596000877706\n",
      "Iter:  1922  Loss on train set:  -51.733272709150015\n",
      "Iter:  1923  Loss on train set:  -51.7330865485558\n",
      "Iter:  1924  Loss on train set:  -51.73335937696945\n",
      "Iter:  1925  Loss on train set:  -51.736986144561264\n",
      "Iter:  1926  Loss on train set:  -51.737020147697244\n",
      "Iter:  1927  Loss on train set:  -51.7364168201823\n",
      "Iter:  1928  Loss on train set:  -51.73683246466885\n",
      "Iter:  1929  Loss on train set:  -51.73784773952423\n",
      "Iter:  1930  Loss on train set:  -51.737238378013004\n",
      "Iter:  1931  Loss on train set:  -51.73679463300327\n",
      "Iter:  1932  Loss on train set:  -51.73830677153639\n",
      "Iter:  1933  Loss on train set:  -51.738361867610436\n",
      "Iter:  1934  Loss on train set:  -51.73683276465242\n",
      "Iter:  1935  Loss on train set:  -51.73860002960387\n",
      "Iter:  1936  Loss on train set:  -51.73882583699219\n",
      "Iter:  1937  Loss on train set:  -51.738910932394276\n",
      "Iter:  1938  Loss on train set:  -51.73882583699219\n",
      "     fun: -51.73882583699219\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 4.93720525e+00,  3.82037379e+00,  1.44864946e-01,  6.52544297e-02,\n",
      "       -4.88253517e-02,  1.45748102e-04,  3.33133966e-01, -1.26605684e-01,\n",
      "        1.17194530e-02])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1939  Loss on train set:  -51.738816706179115\n",
      "Iter:  1940  Loss on train set:  -51.738817721694254\n",
      "Iter:  1941  Loss on train set:  -46.834103976488976\n",
      "Iter:  1942  Loss on train set:  -51.7388143085853\n",
      "Iter:  1943  Loss on train set:  -51.170898683230334\n",
      "Iter:  1944  Loss on train set:  -44.516442474132546\n",
      "Iter:  1945  Loss on train set:  -43.09643461771744\n",
      "Iter:  1946  Loss on train set:  -49.655957765407635\n",
      "Iter:  1947  Loss on train set:  -50.072135234033226\n",
      "Iter:  1948  Loss on train set:  -45.5512874126979\n",
      "Iter:  1949  Loss on train set:  -50.30986090807118\n",
      "Iter:  1950  Loss on train set:  -51.43287858115261\n",
      "Iter:  1951  Loss on train set:  -51.73882732986086\n",
      "Iter:  1952  Loss on train set:  -51.04853708764478\n",
      "Iter:  1953  Loss on train set:  -51.73882320343034\n",
      "Iter:  1954  Loss on train set:  -51.27161339952822\n",
      "Iter:  1955  Loss on train set:  -51.738826302616246\n",
      "Iter:  1956  Loss on train set:  -51.62387065246477\n",
      "Iter:  1957  Loss on train set:  -51.7098953581646\n",
      "Iter:  1958  Loss on train set:  -51.6271476098354\n",
      "Iter:  1959  Loss on train set:  -51.77498464007556\n",
      "Iter:  1960  Loss on train set:  -51.777178001723435\n",
      "Iter:  1961  Loss on train set:  -51.517595187965355\n",
      "Iter:  1962  Loss on train set:  -51.75972660828742\n",
      "Iter:  1963  Loss on train set:  -51.78271429824169\n",
      "Iter:  1964  Loss on train set:  -51.76746887459817\n",
      "Iter:  1965  Loss on train set:  -51.66898077385371\n",
      "Iter:  1966  Loss on train set:  -51.762284424809536\n",
      "Iter:  1967  Loss on train set:  -51.78272270155923\n",
      "Iter:  1968  Loss on train set:  -51.77580117790231\n",
      "Iter:  1969  Loss on train set:  -51.782721425951046\n",
      "Iter:  1970  Loss on train set:  -51.74861144516462\n",
      "Iter:  1971  Loss on train set:  -51.78271581687941\n",
      "Iter:  1972  Loss on train set:  -51.80216823888977\n",
      "Iter:  1973  Loss on train set:  -51.81779869676553\n",
      "Iter:  1974  Loss on train set:  -51.81080475088038\n",
      "Iter:  1975  Loss on train set:  -51.81426746620711\n",
      "Iter:  1976  Loss on train set:  -51.819457034222225\n",
      "Iter:  1977  Loss on train set:  -51.817618697512145\n",
      "Iter:  1978  Loss on train set:  -51.81523870795417\n",
      "Iter:  1979  Loss on train set:  -51.82471548241113\n",
      "Iter:  1980  Loss on train set:  -51.81922907725348\n",
      "Iter:  1981  Loss on train set:  -51.82471959128239\n",
      "Iter:  1982  Loss on train set:  -51.82521921835509\n",
      "Iter:  1983  Loss on train set:  -51.82522232736498\n",
      "Iter:  1984  Loss on train set:  -51.818794235629035\n",
      "Iter:  1985  Loss on train set:  -51.82149094400212\n",
      "Iter:  1986  Loss on train set:  -51.82594421314671\n",
      "Iter:  1987  Loss on train set:  -51.82556127320292\n",
      "Iter:  1988  Loss on train set:  -51.82579298762064\n",
      "Iter:  1989  Loss on train set:  -51.82594421314671\n",
      "     fun: -51.82594421314671\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 4.91179154,  2.43249962, -0.04439528,  4.37521527,  0.2608061 ,\n",
      "        0.15172836, -0.30749132, -0.0721979 , -0.15549702])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1990  Loss on train set:  -44.97658841733323\n",
      "Iter:  1991  Loss on train set:  -44.62235380744577\n",
      "Iter:  1992  Loss on train set:  -40.710035056916695\n",
      "Iter:  1993  Loss on train set:  -40.771382692395804\n",
      "Iter:  1994  Loss on train set:  -51.28030541336493\n",
      "Iter:  1995  Loss on train set:  -51.825944562339664\n",
      "Iter:  1996  Loss on train set:  -51.825941940603826\n",
      "Iter:  1997  Loss on train set:  -50.61140839285713\n",
      "Iter:  1998  Loss on train set:  -44.38458675297442\n",
      "Iter:  1999  Loss on train set:  -42.93651854547854\n",
      "Iter:  2000  Loss on train set:  -49.824589649761364\n",
      "Iter:  2001  Loss on train set:  -51.43592534555933\n",
      "Iter:  2002  Loss on train set:  -48.55124128799919\n",
      "Iter:  2003  Loss on train set:  -51.83001849053694\n",
      "Iter:  2004  Loss on train set:  -48.73600546688039\n",
      "Iter:  2005  Loss on train set:  -51.320551585047795\n",
      "Iter:  2006  Loss on train set:  -51.830015582317884\n",
      "Iter:  2007  Loss on train set:  -51.40628287795291\n",
      "Iter:  2008  Loss on train set:  -51.83001479602871\n",
      "Iter:  2009  Loss on train set:  -51.24835395061488\n",
      "Iter:  2010  Loss on train set:  -51.801940380124265\n",
      "Iter:  2011  Loss on train set:  -51.036095978068346\n",
      "Iter:  2012  Loss on train set:  -51.606675466267845\n",
      "Iter:  2013  Loss on train set:  -51.747411766179795\n",
      "Iter:  2014  Loss on train set:  -51.79552518850531\n",
      "Iter:  2015  Loss on train set:  -51.826310286307454\n",
      "Iter:  2016  Loss on train set:  -51.82740438814902\n",
      "Iter:  2017  Loss on train set:  -51.80520218749679\n",
      "Iter:  2018  Loss on train set:  -51.80240278078825\n",
      "Iter:  2019  Loss on train set:  -51.83708192328171\n",
      "Iter:  2020  Loss on train set:  -51.83636692263508\n",
      "Iter:  2021  Loss on train set:  -51.82695482650824\n",
      "Iter:  2022  Loss on train set:  -51.837081704981955\n",
      "Iter:  2023  Loss on train set:  -51.84292008607215\n",
      "Iter:  2024  Loss on train set:  -51.84291657348278\n",
      "Iter:  2025  Loss on train set:  -51.841153164369224\n",
      "Iter:  2026  Loss on train set:  -51.833279278407126\n",
      "Iter:  2027  Loss on train set:  -51.8381327024387\n",
      "Iter:  2028  Loss on train set:  -51.84875240716667\n",
      "Iter:  2029  Loss on train set:  -51.848243098825414\n",
      "Iter:  2030  Loss on train set:  -51.84943380499444\n",
      "Iter:  2031  Loss on train set:  -51.85052680626977\n",
      "Iter:  2032  Loss on train set:  -51.85052529889423\n",
      "Iter:  2033  Loss on train set:  -51.85225240108872\n",
      "Iter:  2034  Loss on train set:  -51.84922364157171\n",
      "Iter:  2035  Loss on train set:  -51.85707114301184\n",
      "Iter:  2036  Loss on train set:  -51.85661890581424\n",
      "Iter:  2037  Loss on train set:  -51.85707061368713\n",
      "Iter:  2038  Loss on train set:  -51.85635584304633\n",
      "Iter:  2039  Loss on train set:  -51.857363623206965\n",
      "Iter:  2040  Loss on train set:  -51.85707114301184\n",
      "     fun: -51.85707114301184\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.43826875,  0.24841678, -0.76211132, -0.33340427,  0.01839724,\n",
      "        6.98237994,  4.93721037, -0.01531998, -0.01093843])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2041  Loss on train set:  -37.75809262525418\n",
      "Iter:  2042  Loss on train set:  -40.726559325241155\n",
      "Iter:  2043  Loss on train set:  -40.143231647396455\n",
      "Iter:  2044  Loss on train set:  -47.52460703937128\n",
      "Iter:  2045  Loss on train set:  -51.85706567161686\n",
      "Iter:  2046  Loss on train set:  -51.53408071179029\n",
      "Iter:  2047  Loss on train set:  -51.85706738329604\n",
      "Iter:  2048  Loss on train set:  -39.65221892132775\n",
      "Iter:  2049  Loss on train set:  -40.46285700458524\n",
      "Iter:  2050  Loss on train set:  -33.34445595190973\n",
      "Iter:  2051  Loss on train set:  -46.893886346391916\n",
      "Iter:  2052  Loss on train set:  -50.86415628706372\n",
      "Iter:  2053  Loss on train set:  -51.647415984499276\n",
      "Iter:  2054  Loss on train set:  -51.48112245215418\n",
      "Iter:  2055  Loss on train set:  -51.85706764086569\n",
      "Iter:  2056  Loss on train set:  -51.52391758592884\n",
      "Iter:  2057  Loss on train set:  -51.840631666804086\n",
      "Iter:  2058  Loss on train set:  -50.96569719843546\n",
      "Iter:  2059  Loss on train set:  -51.85707074096554\n",
      "Iter:  2060  Loss on train set:  -50.87986530271998\n",
      "Iter:  2061  Loss on train set:  -51.60796252191339\n",
      "Iter:  2062  Loss on train set:  -51.827160271434906\n",
      "Iter:  2063  Loss on train set:  -51.843997557020835\n",
      "Iter:  2064  Loss on train set:  -51.7907604689745\n",
      "Iter:  2065  Loss on train set:  -51.84639428566064\n",
      "Iter:  2066  Loss on train set:  -51.79304068354711\n",
      "Iter:  2067  Loss on train set:  -51.83601434964031\n",
      "Iter:  2068  Loss on train set:  -51.85707201779807\n",
      "Iter:  2069  Loss on train set:  -51.84828103260253\n",
      "Iter:  2070  Loss on train set:  -51.85835253544548\n",
      "Iter:  2071  Loss on train set:  -51.844210464997246\n",
      "Iter:  2072  Loss on train set:  -51.85835295819524\n",
      "Iter:  2073  Loss on train set:  -51.84844635620445\n",
      "Iter:  2074  Loss on train set:  -51.849636881484265\n",
      "Iter:  2075  Loss on train set:  -51.856700536055854\n",
      "Iter:  2076  Loss on train set:  -51.85765905524694\n",
      "Iter:  2077  Loss on train set:  -51.8496122595411\n",
      "Iter:  2078  Loss on train set:  -51.85813448571915\n",
      "Iter:  2079  Loss on train set:  -51.858864517676444\n",
      "Iter:  2080  Loss on train set:  -51.856329090829135\n",
      "Iter:  2081  Loss on train set:  -51.858854677561425\n",
      "Iter:  2082  Loss on train set:  -51.86082161574667\n",
      "Iter:  2083  Loss on train set:  -51.861517779190464\n",
      "Iter:  2084  Loss on train set:  -51.86293873939204\n",
      "Iter:  2085  Loss on train set:  -51.86265718040788\n",
      "Iter:  2086  Loss on train set:  -51.86293383174943\n",
      "Iter:  2087  Loss on train set:  -51.86247420164147\n",
      "Iter:  2088  Loss on train set:  -51.86338499042815\n",
      "Iter:  2089  Loss on train set:  -51.864515672313246\n",
      "Iter:  2090  Loss on train set:  -51.86451499909497\n",
      "Iter:  2091  Loss on train set:  -51.864515672313246\n",
      "     fun: -51.864515672313246\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.00895271, -0.33759491,  0.07704652,  0.10331435,  2.10548819,\n",
      "       -0.10703917,  4.3908373 ,  2.97678741, -0.26716737])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2092  Loss on train set:  -51.86451519304649\n",
      "Iter:  2093  Loss on train set:  -51.86450667342039\n",
      "Iter:  2094  Loss on train set:  -44.43221906458356\n",
      "Iter:  2095  Loss on train set:  -44.39955907289898\n",
      "Iter:  2096  Loss on train set:  -39.35966783877196\n",
      "Iter:  2097  Loss on train set:  -47.641518031656034\n",
      "Iter:  2098  Loss on train set:  -46.34026086312246\n",
      "Iter:  2099  Loss on train set:  -51.458261165067775\n",
      "Iter:  2100  Loss on train set:  -49.58082962315336\n",
      "Iter:  2101  Loss on train set:  -42.21980626032259\n",
      "Iter:  2102  Loss on train set:  -49.35884187369701\n",
      "Iter:  2103  Loss on train set:  -51.319231998422225\n",
      "Iter:  2104  Loss on train set:  -51.8645098504217\n",
      "Iter:  2105  Loss on train set:  -51.36431754628559\n",
      "Iter:  2106  Loss on train set:  -51.86451668016606\n",
      "Iter:  2107  Loss on train set:  -51.128953088479946\n",
      "Iter:  2108  Loss on train set:  -51.76153818406262\n",
      "Iter:  2109  Loss on train set:  -51.53359884214433\n",
      "Iter:  2110  Loss on train set:  -51.847662793656646\n",
      "Iter:  2111  Loss on train set:  -51.707762065006975\n",
      "Iter:  2112  Loss on train set:  -51.26878248735425\n",
      "Iter:  2113  Loss on train set:  -51.74403407841554\n",
      "Iter:  2114  Loss on train set:  -51.72547177235039\n",
      "Iter:  2115  Loss on train set:  -51.856726380452116\n",
      "Iter:  2116  Loss on train set:  -51.8547335117422\n",
      "Iter:  2117  Loss on train set:  -51.81884823265019\n",
      "Iter:  2118  Loss on train set:  -51.86451988465829\n",
      "Iter:  2119  Loss on train set:  -51.829192889099815\n",
      "Iter:  2120  Loss on train set:  -51.86015906022707\n",
      "Iter:  2121  Loss on train set:  -51.86560627806274\n",
      "Iter:  2122  Loss on train set:  -51.860170831706846\n",
      "Iter:  2123  Loss on train set:  -51.865608589107346\n",
      "Iter:  2124  Loss on train set:  -51.85622229095761\n",
      "Iter:  2125  Loss on train set:  -51.86419698841476\n",
      "Iter:  2126  Loss on train set:  -51.85842564878526\n",
      "Iter:  2127  Loss on train set:  -51.85852950298945\n",
      "Iter:  2128  Loss on train set:  -51.86228675430769\n",
      "Iter:  2129  Loss on train set:  -51.86561374606517\n",
      "Iter:  2130  Loss on train set:  -51.86554947166488\n",
      "Iter:  2131  Loss on train set:  -51.866345407827\n",
      "Iter:  2132  Loss on train set:  -51.86555396805897\n",
      "Iter:  2133  Loss on train set:  -51.865168975404096\n",
      "Iter:  2134  Loss on train set:  -51.863721598616365\n",
      "Iter:  2135  Loss on train set:  -51.86744032456553\n",
      "Iter:  2136  Loss on train set:  -51.86644891862156\n",
      "Iter:  2137  Loss on train set:  -51.867437630641874\n",
      "Iter:  2138  Loss on train set:  -51.86456145513118\n",
      "Iter:  2139  Loss on train set:  -51.8672524983283\n",
      "Iter:  2140  Loss on train set:  -51.86744054472376\n",
      "Iter:  2141  Loss on train set:  -51.86783979778676\n",
      "Iter:  2142  Loss on train set:  -51.86744054472376\n",
      "     fun: -51.86744054472376\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 4.42733218,  6.83609152,  0.25863554, -0.00843412, -0.14126164,\n",
      "        0.10255849,  0.34672171,  0.03549111, -0.07157832])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2143  Loss on train set:  -46.91035339665349\n",
      "Iter:  2144  Loss on train set:  -46.07752478464983\n",
      "Iter:  2145  Loss on train set:  -47.995355907413845\n",
      "Iter:  2146  Loss on train set:  -37.81584916167764\n",
      "Iter:  2147  Loss on train set:  -47.64025392652551\n",
      "Iter:  2148  Loss on train set:  -39.867632683696705\n",
      "Iter:  2149  Loss on train set:  -40.96664387869237\n",
      "Iter:  2150  Loss on train set:  -39.70111292113617\n",
      "Iter:  2151  Loss on train set:  -46.198500499768905\n",
      "Iter:  2152  Loss on train set:  -44.54303815387581\n",
      "Iter:  2153  Loss on train set:  -49.86187182043587\n",
      "Iter:  2154  Loss on train set:  -51.01062420346389\n",
      "Iter:  2155  Loss on train set:  -51.773754579844876\n",
      "Iter:  2156  Loss on train set:  -51.52744260320203\n",
      "Iter:  2157  Loss on train set:  -51.821091114053154\n",
      "Iter:  2158  Loss on train set:  -51.613475514799646\n",
      "Iter:  2159  Loss on train set:  -51.527534281385144\n",
      "Iter:  2160  Loss on train set:  -51.518020057959696\n",
      "Iter:  2161  Loss on train set:  -51.71432522313144\n",
      "Iter:  2162  Loss on train set:  -51.23288948404061\n",
      "Iter:  2163  Loss on train set:  -51.75859749333714\n",
      "Iter:  2164  Loss on train set:  -51.827497495295816\n",
      "Iter:  2165  Loss on train set:  -51.86773864616359\n",
      "Iter:  2166  Loss on train set:  -51.844904503202436\n",
      "Iter:  2167  Loss on train set:  -51.857855321507266\n",
      "Iter:  2168  Loss on train set:  -51.79412867968058\n",
      "Iter:  2169  Loss on train set:  -51.8623317863727\n",
      "Iter:  2170  Loss on train set:  -51.865935038468294\n",
      "Iter:  2171  Loss on train set:  -51.846198525105464\n",
      "Iter:  2172  Loss on train set:  -51.86315056141022\n",
      "Iter:  2173  Loss on train set:  -51.861295400393686\n",
      "Iter:  2174  Loss on train set:  -51.85640656995231\n",
      "Iter:  2175  Loss on train set:  -51.866795366267795\n",
      "Iter:  2176  Loss on train set:  -51.86700675686326\n",
      "Iter:  2177  Loss on train set:  -51.86713610149918\n",
      "Iter:  2178  Loss on train set:  -51.86716527153113\n",
      "Iter:  2179  Loss on train set:  -51.86768918029732\n",
      "Iter:  2180  Loss on train set:  -51.867695477188775\n",
      "Iter:  2181  Loss on train set:  -51.86776260670119\n",
      "Iter:  2182  Loss on train set:  -51.86823490602437\n",
      "Iter:  2183  Loss on train set:  -51.86724921815244\n",
      "Iter:  2184  Loss on train set:  -51.86687581571448\n",
      "Iter:  2185  Loss on train set:  -51.87039146559247\n",
      "Iter:  2186  Loss on train set:  -51.87071132433442\n",
      "Iter:  2187  Loss on train set:  -51.87050140930996\n",
      "Iter:  2188  Loss on train set:  -51.87055022226522\n",
      "Iter:  2189  Loss on train set:  -51.87054670623293\n",
      "Iter:  2190  Loss on train set:  -51.87019069715012\n",
      "Iter:  2191  Loss on train set:  -51.87077971647978\n",
      "Iter:  2192  Loss on train set:  -51.87064007524223\n",
      "Iter:  2193  Loss on train set:  -51.87077971647978\n",
      "     fun: -51.87077971647978\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.01678442, -0.0574858 , -0.04818094,  0.01077688,  0.11248584,\n",
      "        3.80986742, -0.76973269,  2.97268356,  0.34905101])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2194  Loss on train set:  -45.410082848910974\n",
      "Iter:  2195  Loss on train set:  -50.53476038775387\n",
      "Iter:  2196  Loss on train set:  -45.85919529203628\n",
      "Iter:  2197  Loss on train set:  -51.870781680012435\n",
      "Iter:  2198  Loss on train set:  -51.51464795864296\n",
      "Iter:  2199  Loss on train set:  -42.87260867337097\n",
      "Iter:  2200  Loss on train set:  -46.90208193925694\n",
      "Iter:  2201  Loss on train set:  -50.89026414323778\n",
      "Iter:  2202  Loss on train set:  -42.41650259734498\n",
      "Iter:  2203  Loss on train set:  -47.62844138958033\n",
      "Iter:  2204  Loss on train set:  -50.2749887472207\n",
      "Iter:  2205  Loss on train set:  -51.759004322290544\n",
      "Iter:  2206  Loss on train set:  -50.41654658450756\n",
      "Iter:  2207  Loss on train set:  -51.55334178797649\n",
      "Iter:  2208  Loss on train set:  -51.87078078445266\n",
      "Iter:  2209  Loss on train set:  -50.994439330192364\n",
      "Iter:  2210  Loss on train set:  -51.85547266135558\n",
      "Iter:  2211  Loss on train set:  -51.51961826328613\n",
      "Iter:  2212  Loss on train set:  -51.85689247899713\n",
      "Iter:  2213  Loss on train set:  -51.15060336491388\n",
      "Iter:  2214  Loss on train set:  -51.79326732643616\n",
      "Iter:  2215  Loss on train set:  -51.74872832382743\n",
      "Iter:  2216  Loss on train set:  -51.817366920949645\n",
      "Iter:  2217  Loss on train set:  -51.866253635408924\n",
      "Iter:  2218  Loss on train set:  -51.836445605475106\n",
      "Iter:  2219  Loss on train set:  -51.86782326873176\n",
      "Iter:  2220  Loss on train set:  -51.84329457866693\n",
      "Iter:  2221  Loss on train set:  -51.86197211069896\n",
      "Iter:  2222  Loss on train set:  -51.870780679361715\n",
      "Iter:  2223  Loss on train set:  -51.86763464286218\n",
      "Iter:  2224  Loss on train set:  -51.8717683517956\n",
      "Iter:  2225  Loss on train set:  -51.86503406130926\n",
      "Iter:  2226  Loss on train set:  -51.87100867515963\n",
      "Iter:  2227  Loss on train set:  -51.86263140398735\n",
      "Iter:  2228  Loss on train set:  -51.86093870254005\n",
      "Iter:  2229  Loss on train set:  -51.868885410786085\n",
      "Iter:  2230  Loss on train set:  -51.87170861108184\n",
      "Iter:  2231  Loss on train set:  -51.871718170445675\n",
      "Iter:  2232  Loss on train set:  -51.87092064302891\n",
      "Iter:  2233  Loss on train set:  -51.87163874297863\n",
      "Iter:  2234  Loss on train set:  -51.87141952971046\n",
      "Iter:  2235  Loss on train set:  -51.87218029284518\n",
      "Iter:  2236  Loss on train set:  -51.871919225424264\n",
      "Iter:  2237  Loss on train set:  -51.87218483491097\n",
      "Iter:  2238  Loss on train set:  -51.87175061482926\n",
      "Iter:  2239  Loss on train set:  -51.87226569215626\n",
      "Iter:  2240  Loss on train set:  -51.87182254188707\n",
      "Iter:  2241  Loss on train set:  -51.87188744754008\n",
      "Iter:  2242  Loss on train set:  -51.872183096682996\n",
      "Iter:  2243  Loss on train set:  -51.871939947184785\n",
      "Iter:  2244  Loss on train set:  -51.87226569215626\n",
      "     fun: -51.87226569215626\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.44475838, -0.01670472,  0.15153946,  3.10939653, -0.08972715,\n",
      "       -0.0171787 , -0.01778399,  0.0357695 ,  0.01166363])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2245  Loss on train set:  -37.73961139509636\n",
      "Iter:  2246  Loss on train set:  -47.81869009887084\n",
      "Iter:  2247  Loss on train set:  -43.38454299992279\n",
      "Iter:  2248  Loss on train set:  -50.15985726031991\n",
      "Iter:  2249  Loss on train set:  -47.20064577877646\n",
      "Iter:  2250  Loss on train set:  -46.90639400809287\n",
      "Iter:  2251  Loss on train set:  -39.16343250310022\n",
      "Iter:  2252  Loss on train set:  -39.79275020775259\n",
      "Iter:  2253  Loss on train set:  -40.45263751039863\n",
      "Iter:  2254  Loss on train set:  -42.746958123596585\n",
      "Iter:  2255  Loss on train set:  -49.22779579610448\n",
      "Iter:  2256  Loss on train set:  -51.02947929426916\n",
      "Iter:  2257  Loss on train set:  -51.798324552350365\n",
      "Iter:  2258  Loss on train set:  -51.265666062054905\n",
      "Iter:  2259  Loss on train set:  -51.83460163439881\n",
      "Iter:  2260  Loss on train set:  -51.560212253598586\n",
      "Iter:  2261  Loss on train set:  -51.716025825169176\n",
      "Iter:  2262  Loss on train set:  -50.808772035045706\n",
      "Iter:  2263  Loss on train set:  -51.6366148250986\n",
      "Iter:  2264  Loss on train set:  -51.206692190745635\n",
      "Iter:  2265  Loss on train set:  -51.60816335879984\n",
      "Iter:  2266  Loss on train set:  -51.85481309406456\n",
      "Iter:  2267  Loss on train set:  -51.86363824896957\n",
      "Iter:  2268  Loss on train set:  -51.82320453360421\n",
      "Iter:  2269  Loss on train set:  -51.87953122288675\n",
      "Iter:  2270  Loss on train set:  -51.88681000194794\n",
      "Iter:  2271  Loss on train set:  -51.883309968273785\n",
      "Iter:  2272  Loss on train set:  -51.87578457094729\n",
      "Iter:  2273  Loss on train set:  -51.86973569406152\n",
      "Iter:  2274  Loss on train set:  -51.88761340785895\n",
      "Iter:  2275  Loss on train set:  -51.88739664201818\n",
      "Iter:  2276  Loss on train set:  -51.8578740445353\n",
      "Iter:  2277  Loss on train set:  -51.932807357552115\n",
      "Iter:  2278  Loss on train set:  -51.945552672450134\n",
      "Iter:  2279  Loss on train set:  -51.925824349359615\n",
      "Iter:  2280  Loss on train set:  -51.942013893359\n",
      "Iter:  2281  Loss on train set:  -51.96862293850256\n",
      "Iter:  2282  Loss on train set:  -51.97836383138692\n",
      "Iter:  2283  Loss on train set:  -51.98632128066742\n",
      "Iter:  2284  Loss on train set:  -51.97984371767235\n",
      "Iter:  2285  Loss on train set:  -51.977241553506346\n",
      "Iter:  2286  Loss on train set:  -51.989473768877176\n",
      "Iter:  2287  Loss on train set:  -51.988942102316294\n",
      "Iter:  2288  Loss on train set:  -51.992893953032045\n",
      "Iter:  2289  Loss on train set:  -51.99476166639147\n",
      "Iter:  2290  Loss on train set:  -51.97710325786783\n",
      "Iter:  2291  Loss on train set:  -52.003021718112585\n",
      "Iter:  2292  Loss on train set:  -51.99607831508149\n",
      "Iter:  2293  Loss on train set:  -52.00780782239576\n",
      "Iter:  2294  Loss on train set:  -52.014034328220745\n",
      "Iter:  2295  Loss on train set:  -52.00780782239576\n",
      "     fun: -52.00780782239576\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.01305209, -0.02691774, -0.3051587 , -0.13159764, -0.04569953,\n",
      "       -0.03492807, -1.15519456,  2.97233918,  0.38383235])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2296  Loss on train set:  -39.069046952483845\n",
      "Iter:  2297  Loss on train set:  -43.53606953143264\n",
      "Iter:  2298  Loss on train set:  -41.797320032603594\n",
      "Iter:  2299  Loss on train set:  -45.4261696670607\n",
      "Iter:  2300  Loss on train set:  -47.20769982140639\n",
      "Iter:  2301  Loss on train set:  -42.31756175620679\n",
      "Iter:  2302  Loss on train set:  -43.16622302017497\n",
      "Iter:  2303  Loss on train set:  -47.61177615291446\n",
      "Iter:  2304  Loss on train set:  -42.11286189245711\n",
      "Iter:  2305  Loss on train set:  -46.10877169032458\n",
      "Iter:  2306  Loss on train set:  -50.42647663134937\n",
      "Iter:  2307  Loss on train set:  -51.41523627984504\n",
      "Iter:  2308  Loss on train set:  -51.778473103949466\n",
      "Iter:  2309  Loss on train set:  -51.80886865067712\n",
      "Iter:  2310  Loss on train set:  -51.88020010026628\n",
      "Iter:  2311  Loss on train set:  -51.59704953991117\n",
      "Iter:  2312  Loss on train set:  -51.7074005197021\n",
      "Iter:  2313  Loss on train set:  -51.76338625503981\n",
      "Iter:  2314  Loss on train set:  -51.91798791860819\n",
      "Iter:  2315  Loss on train set:  -50.81960649601771\n",
      "Iter:  2316  Loss on train set:  -51.91712471309926\n",
      "Iter:  2317  Loss on train set:  -51.98927000390246\n",
      "Iter:  2318  Loss on train set:  -51.99617571682936\n",
      "Iter:  2319  Loss on train set:  -51.88932431416046\n",
      "Iter:  2320  Loss on train set:  -52.014673364647265\n",
      "Iter:  2321  Loss on train set:  -52.02626672426188\n",
      "Iter:  2322  Loss on train set:  -52.01158165048139\n",
      "Iter:  2323  Loss on train set:  -51.985860013927635\n",
      "Iter:  2324  Loss on train set:  -51.9950303040056\n",
      "Iter:  2325  Loss on train set:  -52.034466121219786\n",
      "Iter:  2326  Loss on train set:  -52.01488865114423\n",
      "Iter:  2327  Loss on train set:  -51.99821272344299\n",
      "Iter:  2328  Loss on train set:  -52.01316917134056\n",
      "Iter:  2329  Loss on train set:  -52.02800672790443\n",
      "Iter:  2330  Loss on train set:  -52.036785360677996\n",
      "Iter:  2331  Loss on train set:  -52.03196572307708\n",
      "Iter:  2332  Loss on train set:  -52.03684726771149\n",
      "Iter:  2333  Loss on train set:  -52.0194268968023\n",
      "Iter:  2334  Loss on train set:  -52.046203357520774\n",
      "Iter:  2335  Loss on train set:  -52.049586293368044\n",
      "Iter:  2336  Loss on train set:  -52.04830871357201\n",
      "Iter:  2337  Loss on train set:  -52.03906957013625\n",
      "Iter:  2338  Loss on train set:  -52.0508998705511\n",
      "Iter:  2339  Loss on train set:  -52.03861210135423\n",
      "Iter:  2340  Loss on train set:  -52.04788648477102\n",
      "Iter:  2341  Loss on train set:  -52.062944497869\n",
      "Iter:  2342  Loss on train set:  -52.05931002354277\n",
      "Iter:  2343  Loss on train set:  -52.05241854707434\n",
      "Iter:  2344  Loss on train set:  -52.06667079488021\n",
      "Iter:  2345  Loss on train set:  -52.065940097356524\n",
      "Iter:  2346  Loss on train set:  -52.06667079488021\n",
      "     fun: -52.06667079488021\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-1.12525333, -0.36079474, -0.79786144, -0.50418765,  0.04090029,\n",
      "        0.0087929 , -0.00396553,  0.05562488, -0.69723693])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2347  Loss on train set:  -52.06666491228914\n",
      "Iter:  2348  Loss on train set:  -42.96209927964053\n",
      "Iter:  2349  Loss on train set:  -52.06665965471069\n",
      "Iter:  2350  Loss on train set:  -41.06775973328024\n",
      "Iter:  2351  Loss on train set:  -46.42663080197363\n",
      "Iter:  2352  Loss on train set:  -52.06666719313194\n",
      "Iter:  2353  Loss on train set:  -48.05952125669992\n",
      "Iter:  2354  Loss on train set:  -41.7251682950223\n",
      "Iter:  2355  Loss on train set:  -42.60425566015801\n",
      "Iter:  2356  Loss on train set:  -37.47949721537474\n",
      "Iter:  2357  Loss on train set:  -47.6825791557642\n",
      "Iter:  2358  Loss on train set:  -51.313755075740694\n",
      "Iter:  2359  Loss on train set:  -52.06665639188478\n",
      "Iter:  2360  Loss on train set:  -51.64401953090652\n",
      "Iter:  2361  Loss on train set:  -52.066668064754445\n",
      "Iter:  2362  Loss on train set:  -51.73568683278775\n",
      "Iter:  2363  Loss on train set:  -52.066665744718826\n",
      "Iter:  2364  Loss on train set:  -51.786582237533665\n",
      "Iter:  2365  Loss on train set:  -51.9806445388137\n",
      "Iter:  2366  Loss on train set:  -51.13180581590437\n",
      "Iter:  2367  Loss on train set:  -51.79979020882512\n",
      "Iter:  2368  Loss on train set:  -52.03963663426589\n",
      "Iter:  2369  Loss on train set:  -52.05699637851712\n",
      "Iter:  2370  Loss on train set:  -52.01850686794465\n",
      "Iter:  2371  Loss on train set:  -52.06289793762026\n",
      "Iter:  2372  Loss on train set:  -52.05925084643896\n",
      "Iter:  2373  Loss on train set:  -52.04031827159247\n",
      "Iter:  2374  Loss on train set:  -52.06666331249173\n",
      "Iter:  2375  Loss on train set:  -52.068661418584064\n",
      "Iter:  2376  Loss on train set:  -52.068659071801925\n",
      "Iter:  2377  Loss on train set:  -52.047967967695655\n",
      "Iter:  2378  Loss on train set:  -52.06866209761699\n",
      "Iter:  2379  Loss on train set:  -52.06639938899134\n",
      "Iter:  2380  Loss on train set:  -52.070394633685844\n",
      "Iter:  2381  Loss on train set:  -52.07041670971985\n",
      "Iter:  2382  Loss on train set:  -52.06513805907021\n",
      "Iter:  2383  Loss on train set:  -52.07106937330313\n",
      "Iter:  2384  Loss on train set:  -52.073992678259856\n",
      "Iter:  2385  Loss on train set:  -52.071589805191714\n",
      "Iter:  2386  Loss on train set:  -52.07279972444322\n",
      "Iter:  2387  Loss on train set:  -52.073834662151896\n",
      "Iter:  2388  Loss on train set:  -52.07400431356436\n",
      "Iter:  2389  Loss on train set:  -52.08184230638215\n",
      "Iter:  2390  Loss on train set:  -52.082842597617\n",
      "Iter:  2391  Loss on train set:  -52.08308381812529\n",
      "Iter:  2392  Loss on train set:  -52.08523322561378\n",
      "Iter:  2393  Loss on train set:  -52.08137736045626\n",
      "Iter:  2394  Loss on train set:  -52.08487185751762\n",
      "Iter:  2395  Loss on train set:  -52.07725298209904\n",
      "Iter:  2396  Loss on train set:  -52.08335666350705\n",
      "Iter:  2397  Loss on train set:  -52.08523322561378\n",
      "     fun: -52.08523322561378\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 4.94164897, -0.00944654,  3.13609688, -0.28327455, -0.03598682,\n",
      "        6.82828583,  0.07697158, -0.29600385, -0.70076206])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2398  Loss on train set:  -41.013251463496765\n",
      "Iter:  2399  Loss on train set:  -52.08522897452667\n",
      "Iter:  2400  Loss on train set:  -47.63352579191501\n",
      "Iter:  2401  Loss on train set:  -48.0396994328776\n",
      "Iter:  2402  Loss on train set:  -47.081207519977625\n",
      "Iter:  2403  Loss on train set:  -40.63399302330053\n",
      "Iter:  2404  Loss on train set:  -40.732219864271784\n",
      "Iter:  2405  Loss on train set:  -39.87159115521878\n",
      "Iter:  2406  Loss on train set:  -45.94416174998385\n",
      "Iter:  2407  Loss on train set:  -34.643335455497144\n",
      "Iter:  2408  Loss on train set:  -47.302971432568775\n",
      "Iter:  2409  Loss on train set:  -51.817633848903306\n",
      "Iter:  2410  Loss on train set:  -52.08523456674691\n",
      "Iter:  2411  Loss on train set:  -51.76873362372039\n",
      "Iter:  2412  Loss on train set:  -52.01050755302973\n",
      "Iter:  2413  Loss on train set:  -51.750588853780435\n",
      "Iter:  2414  Loss on train set:  -51.88441573824219\n",
      "Iter:  2415  Loss on train set:  -51.2453947567129\n",
      "Iter:  2416  Loss on train set:  -51.973687420509144\n",
      "Iter:  2417  Loss on train set:  -51.34065416602271\n",
      "Iter:  2418  Loss on train set:  -51.781998754903064\n",
      "Iter:  2419  Loss on train set:  -52.084913351403415\n",
      "Iter:  2420  Loss on train set:  -52.064721313162465\n",
      "Iter:  2421  Loss on train set:  -52.083309243711355\n",
      "Iter:  2422  Loss on train set:  -52.029917170329895\n",
      "Iter:  2423  Loss on train set:  -52.068895730514775\n",
      "Iter:  2424  Loss on train set:  -52.08105088267637\n",
      "Iter:  2425  Loss on train set:  -52.08522544762519\n",
      "Iter:  2426  Loss on train set:  -52.10446427785268\n",
      "Iter:  2427  Loss on train set:  -52.10821521825373\n",
      "Iter:  2428  Loss on train set:  -52.09973095499514\n",
      "Iter:  2429  Loss on train set:  -52.10866207569975\n",
      "Iter:  2430  Loss on train set:  -52.11463064972265\n",
      "Iter:  2431  Loss on train set:  -52.11697317506759\n",
      "Iter:  2432  Loss on train set:  -52.1163310612012\n",
      "Iter:  2433  Loss on train set:  -52.12126547402234\n",
      "Iter:  2434  Loss on train set:  -52.11984597527587\n",
      "Iter:  2435  Loss on train set:  -52.116661774426035\n",
      "Iter:  2436  Loss on train set:  -52.11251512958168\n",
      "Iter:  2437  Loss on train set:  -52.13035300006997\n",
      "Iter:  2438  Loss on train set:  -52.11439483918188\n",
      "Iter:  2439  Loss on train set:  -52.12972613254456\n",
      "Iter:  2440  Loss on train set:  -52.12209128257925\n",
      "Iter:  2441  Loss on train set:  -52.13018350264666\n",
      "Iter:  2442  Loss on train set:  -52.12322538494874\n",
      "Iter:  2443  Loss on train set:  -52.12630003079414\n",
      "Iter:  2444  Loss on train set:  -52.12921014538145\n",
      "Iter:  2445  Loss on train set:  -52.12749321766278\n",
      "Iter:  2446  Loss on train set:  -52.129470787613045\n",
      "Iter:  2447  Loss on train set:  -52.12818336741974\n",
      "Iter:  2448  Loss on train set:  -52.13035300006997\n",
      "     fun: -52.13035300006997\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.46563757,  0.55656514, -0.02797488,  0.07584722, -0.04414563,\n",
      "       -0.345873  ,  0.07944357, -0.15645113, -0.53631925])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2449  Loss on train set:  -51.63199673992818\n",
      "Iter:  2450  Loss on train set:  -40.91900044272296\n",
      "Iter:  2451  Loss on train set:  -42.50031432630081\n",
      "Iter:  2452  Loss on train set:  -41.07835534921348\n",
      "Iter:  2453  Loss on train set:  -37.056148564028035\n",
      "Iter:  2454  Loss on train set:  -40.41620499515889\n",
      "Iter:  2455  Loss on train set:  -50.45315688822079\n",
      "Iter:  2456  Loss on train set:  -47.694453992104854\n",
      "Iter:  2457  Loss on train set:  -43.21644418011635\n",
      "Iter:  2458  Loss on train set:  -44.419475945444205\n",
      "Iter:  2459  Loss on train set:  -50.16361060093368\n",
      "Iter:  2460  Loss on train set:  -51.41355210536972\n",
      "Iter:  2461  Loss on train set:  -52.12315606070527\n",
      "Iter:  2462  Loss on train set:  -51.38654442959458\n",
      "Iter:  2463  Loss on train set:  -51.94696780982525\n",
      "Iter:  2464  Loss on train set:  -50.951349609114416\n",
      "Iter:  2465  Loss on train set:  -52.106433114155934\n",
      "Iter:  2466  Loss on train set:  -51.552767379944775\n",
      "Iter:  2467  Loss on train set:  -52.01103417196825\n",
      "Iter:  2468  Loss on train set:  -51.600736288170644\n",
      "Iter:  2469  Loss on train set:  -51.99704520666877\n",
      "Iter:  2470  Loss on train set:  -52.08435426597645\n",
      "Iter:  2471  Loss on train set:  -52.09565447731984\n",
      "Iter:  2472  Loss on train set:  -52.101006259529534\n",
      "Iter:  2473  Loss on train set:  -52.12223833088928\n",
      "Iter:  2474  Loss on train set:  -52.110081977818254\n",
      "Iter:  2475  Loss on train set:  -52.12767881771191\n",
      "Iter:  2476  Loss on train set:  -52.12928495380207\n",
      "Iter:  2477  Loss on train set:  -52.12680290406103\n",
      "Iter:  2478  Loss on train set:  -52.130103165664146\n",
      "Iter:  2479  Loss on train set:  -52.12694564391716\n",
      "Iter:  2480  Loss on train set:  -52.130501893811505\n",
      "Iter:  2481  Loss on train set:  -52.13092865669568\n",
      "Iter:  2482  Loss on train set:  -52.130121844171306\n",
      "Iter:  2483  Loss on train set:  -52.12755829168899\n",
      "Iter:  2484  Loss on train set:  -52.129249673856634\n",
      "Iter:  2485  Loss on train set:  -52.13134044550995\n",
      "Iter:  2486  Loss on train set:  -52.132854288777594\n",
      "Iter:  2487  Loss on train set:  -52.13292328692392\n",
      "Iter:  2488  Loss on train set:  -52.12907278527007\n",
      "Iter:  2489  Loss on train set:  -52.13308714816035\n",
      "Iter:  2490  Loss on train set:  -52.133125843473785\n",
      "Iter:  2491  Loss on train set:  -52.13290242290575\n",
      "Iter:  2492  Loss on train set:  -52.13257748721509\n",
      "Iter:  2493  Loss on train set:  -52.13311370678662\n",
      "Iter:  2494  Loss on train set:  -52.13207128719306\n",
      "Iter:  2495  Loss on train set:  -52.13392794561616\n",
      "Iter:  2496  Loss on train set:  -52.13427640313507\n",
      "Iter:  2497  Loss on train set:  -52.135590665911735\n",
      "Iter:  2498  Loss on train set:  -52.135008128116326\n",
      "Iter:  2499  Loss on train set:  -52.135590665911735\n",
      "     fun: -52.135590665911735\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 3.02523084e-02,  7.33185163e-02,  8.29213464e-03, -3.51797339e-01,\n",
      "        6.73765362e-03,  3.82444956e+00, -1.62303092e-02,  8.47428143e-02,\n",
      "       -2.16143500e-03])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2500  Loss on train set:  -50.79131151743144\n",
      "Iter:  2501  Loss on train set:  -47.52317471083197\n",
      "Iter:  2502  Loss on train set:  -50.441453061237674\n",
      "Iter:  2503  Loss on train set:  -52.135592591705105\n",
      "Iter:  2504  Loss on train set:  -42.593753860645606\n",
      "Iter:  2505  Loss on train set:  -46.42544441742064\n",
      "Iter:  2506  Loss on train set:  -39.49579882392836\n",
      "Iter:  2507  Loss on train set:  -52.135587160329095\n",
      "Iter:  2508  Loss on train set:  -52.13559449163372\n",
      "Iter:  2509  Loss on train set:  -42.89760713276826\n",
      "Iter:  2510  Loss on train set:  -50.847718002020514\n",
      "Iter:  2511  Loss on train set:  -52.01367744325161\n",
      "Iter:  2512  Loss on train set:  -51.74786518274705\n",
      "Iter:  2513  Loss on train set:  -52.135596724697095\n",
      "Iter:  2514  Loss on train set:  -49.61961621586862\n",
      "Iter:  2515  Loss on train set:  -51.76684278202214\n",
      "Iter:  2516  Loss on train set:  -48.763119005285425\n",
      "Iter:  2517  Loss on train set:  -52.13559324264231\n",
      "Iter:  2518  Loss on train set:  -49.98273873113526\n",
      "Iter:  2519  Loss on train set:  -51.83033642231807\n",
      "Iter:  2520  Loss on train set:  -52.13559009317074\n",
      "Iter:  2521  Loss on train set:  -52.03515528813804\n",
      "Iter:  2522  Loss on train set:  -51.879595439992826\n",
      "Iter:  2523  Loss on train set:  -52.09963652819245\n",
      "Iter:  2524  Loss on train set:  -52.02194577492007\n",
      "Iter:  2525  Loss on train set:  -52.13529506664045\n",
      "Iter:  2526  Loss on train set:  -52.0712614833201\n",
      "Iter:  2527  Loss on train set:  -52.12873574982884\n",
      "Iter:  2528  Loss on train set:  -52.135588947143454\n",
      "Iter:  2529  Loss on train set:  -52.130464148857\n",
      "Iter:  2530  Loss on train set:  -52.13559274764666\n",
      "Iter:  2531  Loss on train set:  -52.11007431884696\n",
      "Iter:  2532  Loss on train set:  -52.13650377549076\n",
      "Iter:  2533  Loss on train set:  -52.13650772310698\n",
      "Iter:  2534  Loss on train set:  -52.12382092483627\n",
      "Iter:  2535  Loss on train set:  -52.13470284716124\n",
      "Iter:  2536  Loss on train set:  -52.12798828700957\n",
      "Iter:  2537  Loss on train set:  -52.138727352863675\n",
      "Iter:  2538  Loss on train set:  -52.13424906754353\n",
      "Iter:  2539  Loss on train set:  -52.13753046846485\n",
      "Iter:  2540  Loss on train set:  -52.132717913548504\n",
      "Iter:  2541  Loss on train set:  -52.13567567748501\n",
      "Iter:  2542  Loss on train set:  -52.138722553190085\n",
      "Iter:  2543  Loss on train set:  -52.138521166540045\n",
      "Iter:  2544  Loss on train set:  -52.138728898142034\n",
      "Iter:  2545  Loss on train set:  -52.137434226600575\n",
      "Iter:  2546  Loss on train set:  -52.13698079050168\n",
      "Iter:  2547  Loss on train set:  -52.13914398659995\n",
      "Iter:  2548  Loss on train set:  -52.13924313817931\n",
      "Iter:  2549  Loss on train set:  -52.13906979392206\n",
      "Iter:  2550  Loss on train set:  -52.13924313817931\n",
      "     fun: -52.13924313817931\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.02371344, -0.02766922, -0.03367061,  3.68253782,  0.00839505,\n",
      "       -0.02257153, -0.17069668,  3.12826822,  1.54092348])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2551  Loss on train set:  -49.896664970190045\n",
      "Iter:  2552  Loss on train set:  -39.630652610733705\n",
      "Iter:  2553  Loss on train set:  -42.25126185543797\n",
      "Iter:  2554  Loss on train set:  -39.08751203604878\n",
      "Iter:  2555  Loss on train set:  -52.1392439045892\n",
      "Iter:  2556  Loss on train set:  -42.61313443682637\n",
      "Iter:  2557  Loss on train set:  -47.080959209866506\n",
      "Iter:  2558  Loss on train set:  -52.13924690018311\n",
      "Iter:  2559  Loss on train set:  -50.59924898866102\n",
      "Iter:  2560  Loss on train set:  -42.893400408475884\n",
      "Iter:  2561  Loss on train set:  -48.632180339166524\n",
      "Iter:  2562  Loss on train set:  -51.92000640100934\n",
      "Iter:  2563  Loss on train set:  -49.68487830587727\n",
      "Iter:  2564  Loss on train set:  -52.13924171254464\n",
      "Iter:  2565  Loss on train set:  -48.601645706557676\n",
      "Iter:  2566  Loss on train set:  -51.76658310448158\n",
      "Iter:  2567  Loss on train set:  -49.87849453639757\n",
      "Iter:  2568  Loss on train set:  -51.597757034989606\n",
      "Iter:  2569  Loss on train set:  -52.13924789683155\n",
      "Iter:  2570  Loss on train set:  -51.56352875336678\n",
      "Iter:  2571  Loss on train set:  -52.089643207737154\n",
      "Iter:  2572  Loss on train set:  -51.280677180861304\n",
      "Iter:  2573  Loss on train set:  -51.890574014620825\n",
      "Iter:  2574  Loss on train set:  -52.1373262661686\n",
      "Iter:  2575  Loss on train set:  -51.95818090740895\n",
      "Iter:  2576  Loss on train set:  -52.13924418782395\n",
      "Iter:  2577  Loss on train set:  -52.0383395637668\n",
      "Iter:  2578  Loss on train set:  -52.058405759879506\n",
      "Iter:  2579  Loss on train set:  -52.10500365026113\n",
      "Iter:  2580  Loss on train set:  -52.093881820974204\n",
      "Iter:  2581  Loss on train set:  -52.127020358112496\n",
      "Iter:  2582  Loss on train set:  -52.14135326752506\n",
      "Iter:  2583  Loss on train set:  -52.11455296964216\n",
      "Iter:  2584  Loss on train set:  -52.14135430442988\n",
      "Iter:  2585  Loss on train set:  -52.132673657039476\n",
      "Iter:  2586  Loss on train set:  -52.14356265681473\n",
      "Iter:  2587  Loss on train set:  -52.15035274667823\n",
      "Iter:  2588  Loss on train set:  -52.14307253397713\n",
      "Iter:  2589  Loss on train set:  -52.14835030917584\n",
      "Iter:  2590  Loss on train set:  -52.14300124411003\n",
      "Iter:  2591  Loss on train set:  -52.150344953582746\n",
      "Iter:  2592  Loss on train set:  -52.14049886587161\n",
      "Iter:  2593  Loss on train set:  -52.147271007343086\n",
      "Iter:  2594  Loss on train set:  -52.15035230650891\n",
      "Iter:  2595  Loss on train set:  -52.146643038481734\n",
      "Iter:  2596  Loss on train set:  -52.14694247814785\n",
      "Iter:  2597  Loss on train set:  -52.15008111101284\n",
      "Iter:  2598  Loss on train set:  -52.15032948868593\n",
      "Iter:  2599  Loss on train set:  -52.15158029644678\n",
      "Iter:  2600  Loss on train set:  -52.15172255406881\n",
      "Iter:  2601  Loss on train set:  -52.15158029644678\n",
      "     fun: -52.15158029644678\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.13322129, -0.19095588, -0.81023933, -0.18047443,  4.12826536,\n",
      "        0.00591277, -0.06269839,  4.18482702, -0.04761579])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2602  Loss on train set:  -52.15157979629726\n",
      "Iter:  2603  Loss on train set:  -43.07516566751186\n",
      "Iter:  2604  Loss on train set:  -52.15157835458953\n",
      "Iter:  2605  Loss on train set:  -41.351648948126495\n",
      "Iter:  2606  Loss on train set:  -50.748256973656254\n",
      "Iter:  2607  Loss on train set:  -40.015205000689164\n",
      "Iter:  2608  Loss on train set:  -43.14832340980933\n",
      "Iter:  2609  Loss on train set:  -49.396391368845414\n",
      "Iter:  2610  Loss on train set:  -46.6827426697227\n",
      "Iter:  2611  Loss on train set:  -41.55337376252575\n",
      "Iter:  2612  Loss on train set:  -49.14555828430166\n",
      "Iter:  2613  Loss on train set:  -51.35911822277311\n",
      "Iter:  2614  Loss on train set:  -52.151579443275594\n",
      "Iter:  2615  Loss on train set:  -51.611541770743855\n",
      "Iter:  2616  Loss on train set:  -52.15157937417346\n",
      "Iter:  2617  Loss on train set:  -51.328461583717996\n",
      "Iter:  2618  Loss on train set:  -52.10634184952755\n",
      "Iter:  2619  Loss on train set:  -51.71198469699701\n",
      "Iter:  2620  Loss on train set:  -52.087222752663195\n",
      "Iter:  2621  Loss on train set:  -51.73332823523641\n",
      "Iter:  2622  Loss on train set:  -51.993003917733134\n",
      "Iter:  2623  Loss on train set:  -52.133906807740374\n",
      "Iter:  2624  Loss on train set:  -52.13406773251733\n",
      "Iter:  2625  Loss on train set:  -52.07602069050855\n",
      "Iter:  2626  Loss on train set:  -52.146689032778994\n",
      "Iter:  2627  Loss on train set:  -52.10326876889083\n",
      "Iter:  2628  Loss on train set:  -52.146295870978804\n",
      "Iter:  2629  Loss on train set:  -52.15158331894955\n",
      "Iter:  2630  Loss on train set:  -52.14630134282234\n",
      "Iter:  2631  Loss on train set:  -52.151578063505376\n",
      "Iter:  2632  Loss on train set:  -52.152877830786764\n",
      "Iter:  2633  Loss on train set:  -52.153328742909935\n",
      "Iter:  2634  Loss on train set:  -52.15148182267013\n",
      "Iter:  2635  Loss on train set:  -52.16305346348781\n",
      "Iter:  2636  Loss on train set:  -52.16826084272192\n",
      "Iter:  2637  Loss on train set:  -52.16653931531927\n",
      "Iter:  2638  Loss on train set:  -52.167525299862405\n",
      "Iter:  2639  Loss on train set:  -52.171338677398815\n",
      "Iter:  2640  Loss on train set:  -52.17133265071347\n",
      "Iter:  2641  Loss on train set:  -52.17110245725553\n",
      "Iter:  2642  Loss on train set:  -52.17133598175762\n",
      "Iter:  2643  Loss on train set:  -52.16644884250712\n",
      "Iter:  2644  Loss on train set:  -52.16985954416519\n",
      "Iter:  2645  Loss on train set:  -52.171714480157476\n",
      "Iter:  2646  Loss on train set:  -52.170490633732165\n",
      "Iter:  2647  Loss on train set:  -52.17211725523498\n",
      "Iter:  2648  Loss on train set:  -52.17392729315621\n",
      "Iter:  2649  Loss on train set:  -52.175813342526254\n",
      "Iter:  2650  Loss on train set:  -52.177037741182744\n",
      "Iter:  2651  Loss on train set:  -52.17747769776393\n",
      "Iter:  2652  Loss on train set:  -52.177037741182744\n",
      "     fun: -52.177037741182744\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 4.12827276, -1.4268718 ,  4.20045767,  0.10760902,  3.16415747,\n",
      "       -0.22264764, -0.0096609 , -0.07144321, -0.0287874 ])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2653  Loss on train set:  -40.583736274266066\n",
      "Iter:  2654  Loss on train set:  -40.19887472939921\n",
      "Iter:  2655  Loss on train set:  -47.385116886191874\n",
      "Iter:  2656  Loss on train set:  -47.478155216855804\n",
      "Iter:  2657  Loss on train set:  -47.78045157478181\n",
      "Iter:  2658  Loss on train set:  -42.70656415580345\n",
      "Iter:  2659  Loss on train set:  -52.177035911101214\n",
      "Iter:  2660  Loss on train set:  -46.265351953545874\n",
      "Iter:  2661  Loss on train set:  -42.450106225113764\n",
      "Iter:  2662  Loss on train set:  -41.58110667958385\n",
      "Iter:  2663  Loss on train set:  -49.482457005756665\n",
      "Iter:  2664  Loss on train set:  -51.01699578646465\n",
      "Iter:  2665  Loss on train set:  -52.092930518912425\n",
      "Iter:  2666  Loss on train set:  -51.44552983633239\n",
      "Iter:  2667  Loss on train set:  -52.11055574453564\n",
      "Iter:  2668  Loss on train set:  -51.813937860627384\n",
      "Iter:  2669  Loss on train set:  -52.177038162651705\n",
      "Iter:  2670  Loss on train set:  -51.96340020359162\n",
      "Iter:  2671  Loss on train set:  -52.04524207917169\n",
      "Iter:  2672  Loss on train set:  -51.17535719295234\n",
      "Iter:  2673  Loss on train set:  -52.10378260607668\n",
      "Iter:  2674  Loss on train set:  -51.73935755708722\n",
      "Iter:  2675  Loss on train set:  -51.83961466431236\n",
      "Iter:  2676  Loss on train set:  -52.14985433405954\n",
      "Iter:  2677  Loss on train set:  -52.02397928043208\n",
      "Iter:  2678  Loss on train set:  -52.122254460489096\n",
      "Iter:  2679  Loss on train set:  -52.16854156078918\n",
      "Iter:  2680  Loss on train set:  -52.16719982793575\n",
      "Iter:  2681  Loss on train set:  -52.167748547874794\n",
      "Iter:  2682  Loss on train set:  -52.16189045069557\n",
      "Iter:  2683  Loss on train set:  -52.176568899685215\n",
      "Iter:  2684  Loss on train set:  -52.11826884183776\n",
      "Iter:  2685  Loss on train set:  -52.17358685401162\n",
      "Iter:  2686  Loss on train set:  -52.17772478498338\n",
      "Iter:  2687  Loss on train set:  -52.16053295859891\n",
      "Iter:  2688  Loss on train set:  -52.176197100800515\n",
      "Iter:  2689  Loss on train set:  -52.17761220577366\n",
      "Iter:  2690  Loss on train set:  -52.176138875234834\n",
      "Iter:  2691  Loss on train set:  -52.17707859171346\n",
      "Iter:  2692  Loss on train set:  -52.18070728550303\n",
      "Iter:  2693  Loss on train set:  -52.1798220644609\n",
      "Iter:  2694  Loss on train set:  -52.17938444051624\n",
      "Iter:  2695  Loss on train set:  -52.18231385074414\n",
      "Iter:  2696  Loss on train set:  -52.180854766915964\n",
      "Iter:  2697  Loss on train set:  -52.182085918105315\n",
      "Iter:  2698  Loss on train set:  -52.182388894005435\n",
      "Iter:  2699  Loss on train set:  -52.181449803118255\n",
      "Iter:  2700  Loss on train set:  -52.18172478893868\n",
      "Iter:  2701  Loss on train set:  -52.18220797412742\n",
      "Iter:  2702  Loss on train set:  -52.18247269352675\n",
      "Iter:  2703  Loss on train set:  -52.182388894005435\n",
      "     fun: -52.182388894005435\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.77657529,  3.82723808,  0.03723391, -0.02444817, -0.03110374,\n",
      "       -0.69757192,  4.0721243 ,  0.32575201,  0.15040167])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2704  Loss on train set:  -52.18238736294915\n",
      "Iter:  2705  Loss on train set:  -37.17113064851103\n",
      "Iter:  2706  Loss on train set:  -52.18238144559604\n",
      "Iter:  2707  Loss on train set:  -52.18238263061934\n",
      "Iter:  2708  Loss on train set:  -52.18238169266218\n",
      "Iter:  2709  Loss on train set:  -44.051961185646164\n",
      "Iter:  2710  Loss on train set:  -45.86879102174274\n",
      "Iter:  2711  Loss on train set:  -52.18238224014496\n",
      "Iter:  2712  Loss on train set:  -49.27425091179485\n",
      "Iter:  2713  Loss on train set:  -41.736752980648056\n",
      "Iter:  2714  Loss on train set:  -49.46423395782527\n",
      "Iter:  2715  Loss on train set:  -51.410777673696096\n",
      "Iter:  2716  Loss on train set:  -52.182395915698706\n",
      "Iter:  2717  Loss on train set:  -51.374855109848376\n",
      "Iter:  2718  Loss on train set:  -52.182391347547764\n",
      "Iter:  2719  Loss on train set:  -51.8304599891905\n",
      "Iter:  2720  Loss on train set:  -52.182392848946954\n",
      "Iter:  2721  Loss on train set:  -51.99408698220982\n",
      "Iter:  2722  Loss on train set:  -52.18238797009784\n",
      "Iter:  2723  Loss on train set:  -51.433015623634034\n",
      "Iter:  2724  Loss on train set:  -52.18238550786493\n",
      "Iter:  2725  Loss on train set:  -51.43921801865373\n",
      "Iter:  2726  Loss on train set:  -51.95711404103356\n",
      "Iter:  2727  Loss on train set:  -52.1451027207927\n",
      "Iter:  2728  Loss on train set:  -52.1778668881152\n",
      "Iter:  2729  Loss on train set:  -52.177622627759966\n",
      "Iter:  2730  Loss on train set:  -52.144658545315345\n",
      "Iter:  2731  Loss on train set:  -52.159483356419976\n",
      "Iter:  2732  Loss on train set:  -52.182386999044176\n",
      "Iter:  2733  Loss on train set:  -52.17639816910308\n",
      "Iter:  2734  Loss on train set:  -52.18239169623146\n",
      "Iter:  2735  Loss on train set:  -52.178920331271925\n",
      "Iter:  2736  Loss on train set:  -52.182389155552244\n",
      "Iter:  2737  Loss on train set:  -52.159748937111374\n",
      "Iter:  2738  Loss on train set:  -52.182382925944715\n",
      "Iter:  2739  Loss on train set:  -52.18542507366423\n",
      "Iter:  2740  Loss on train set:  -52.18542108799406\n",
      "Iter:  2741  Loss on train set:  -52.17157768715978\n",
      "Iter:  2742  Loss on train set:  -52.185896250079225\n",
      "Iter:  2743  Loss on train set:  -52.17540546528732\n",
      "Iter:  2744  Loss on train set:  -52.18554880309433\n",
      "Iter:  2745  Loss on train set:  -52.18594141170172\n",
      "Iter:  2746  Loss on train set:  -52.18446256588324\n",
      "Iter:  2747  Loss on train set:  -52.185802585865446\n",
      "Iter:  2748  Loss on train set:  -52.18069233650756\n",
      "Iter:  2749  Loss on train set:  -52.18586583596113\n",
      "Iter:  2750  Loss on train set:  -52.18445703618694\n",
      "Iter:  2751  Loss on train set:  -52.18576943676954\n",
      "Iter:  2752  Loss on train set:  -52.186056140492624\n",
      "Iter:  2753  Loss on train set:  -52.18606778025324\n",
      "Iter:  2754  Loss on train set:  -52.186056140492624\n",
      "     fun: -52.186056140492624\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 6.70328587e+00,  1.65613793e-03,  4.42134549e+00,  4.12228299e+00,\n",
      "        3.67654994e+00,  2.63324202e-01, -2.21849347e-01,  1.52599273e+00,\n",
      "       -5.99658261e-02])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2755  Loss on train set:  -42.60716086488102\n",
      "Iter:  2756  Loss on train set:  -47.22766133997591\n",
      "Iter:  2757  Loss on train set:  -49.208781159490734\n",
      "Iter:  2758  Loss on train set:  -43.64802986853428\n",
      "Iter:  2759  Loss on train set:  -52.18605739757738\n",
      "Iter:  2760  Loss on train set:  -45.99563142395378\n",
      "Iter:  2761  Loss on train set:  -41.40217719698617\n",
      "Iter:  2762  Loss on train set:  -46.27949000376116\n",
      "Iter:  2763  Loss on train set:  -37.29415463024043\n",
      "Iter:  2764  Loss on train set:  -40.32005772347782\n",
      "Iter:  2765  Loss on train set:  -49.581823520671534\n",
      "Iter:  2766  Loss on train set:  -51.962151709354906\n",
      "Iter:  2767  Loss on train set:  -50.91801228838378\n",
      "Iter:  2768  Loss on train set:  -50.07245402592855\n",
      "Iter:  2769  Loss on train set:  -51.41006637305367\n",
      "Iter:  2770  Loss on train set:  -52.186060054992375\n",
      "Iter:  2771  Loss on train set:  -51.6777969693857\n",
      "Iter:  2772  Loss on train set:  -52.0566631524477\n",
      "Iter:  2773  Loss on train set:  -51.595178513782955\n",
      "Iter:  2774  Loss on train set:  -51.33412717057476\n",
      "Iter:  2775  Loss on train set:  -52.024215319871814\n",
      "Iter:  2776  Loss on train set:  -52.158155046654905\n",
      "Iter:  2777  Loss on train set:  -52.06377802842049\n",
      "Iter:  2778  Loss on train set:  -52.16857903975117\n",
      "Iter:  2779  Loss on train set:  -52.01731210472473\n",
      "Iter:  2780  Loss on train set:  -52.135739889277964\n",
      "Iter:  2781  Loss on train set:  -52.16929700285239\n",
      "Iter:  2782  Loss on train set:  -52.142842163330414\n",
      "Iter:  2783  Loss on train set:  -52.18175102838263\n",
      "Iter:  2784  Loss on train set:  -52.18518480210511\n",
      "Iter:  2785  Loss on train set:  -52.185951444880615\n",
      "Iter:  2786  Loss on train set:  -52.18606422375825\n",
      "Iter:  2787  Loss on train set:  -52.17920308211902\n",
      "Iter:  2788  Loss on train set:  -52.18186791085367\n",
      "Iter:  2789  Loss on train set:  -52.179473804630994\n",
      "Iter:  2790  Loss on train set:  -52.18689002919171\n",
      "Iter:  2791  Loss on train set:  -52.18630836381863\n",
      "Iter:  2792  Loss on train set:  -52.18870878337909\n",
      "Iter:  2793  Loss on train set:  -52.18710194623101\n",
      "Iter:  2794  Loss on train set:  -52.183766249067176\n",
      "Iter:  2795  Loss on train set:  -52.18690950592294\n",
      "Iter:  2796  Loss on train set:  -52.193577141761985\n",
      "Iter:  2797  Loss on train set:  -52.19420454680815\n",
      "Iter:  2798  Loss on train set:  -52.19536562769305\n",
      "Iter:  2799  Loss on train set:  -52.195376174490455\n",
      "Iter:  2800  Loss on train set:  -52.19555946511819\n",
      "Iter:  2801  Loss on train set:  -52.195667992860436\n",
      "Iter:  2802  Loss on train set:  -52.19627886216617\n",
      "Iter:  2803  Loss on train set:  -52.193332716041674\n",
      "Iter:  2804  Loss on train set:  -52.19545725700079\n",
      "Iter:  2805  Loss on train set:  -52.19627886216617\n",
      "     fun: -52.19627886216617\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 1.30495416e-02, -6.06811228e-02, -5.64028237e-02, -3.75241039e-01,\n",
      "        7.86446344e+00, -5.83954990e-01,  1.23042359e-01, -1.67537933e-02,\n",
      "       -4.27299683e-03])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2806  Loss on train set:  -45.3364465731751\n",
      "Iter:  2807  Loss on train set:  -39.77161699897165\n",
      "Iter:  2808  Loss on train set:  -46.4810892161357\n",
      "Iter:  2809  Loss on train set:  -41.124354995667645\n",
      "Iter:  2810  Loss on train set:  -52.19627043702954\n",
      "Iter:  2811  Loss on train set:  -50.20967320278911\n",
      "Iter:  2812  Loss on train set:  -45.47435319178316\n",
      "Iter:  2813  Loss on train set:  -39.05968908299754\n",
      "Iter:  2814  Loss on train set:  -47.436726753252906\n",
      "Iter:  2815  Loss on train set:  -44.001281339537535\n",
      "Iter:  2816  Loss on train set:  -50.050609077802704\n",
      "Iter:  2817  Loss on train set:  -51.6013610798411\n",
      "Iter:  2818  Loss on train set:  -52.03105038144305\n",
      "Iter:  2819  Loss on train set:  -51.517014358293295\n",
      "Iter:  2820  Loss on train set:  -52.19627686403078\n",
      "Iter:  2821  Loss on train set:  -51.1452099104022\n",
      "Iter:  2822  Loss on train set:  -52.14535128732166\n",
      "Iter:  2823  Loss on train set:  -51.688296999500686\n",
      "Iter:  2824  Loss on train set:  -52.128805731601936\n",
      "Iter:  2825  Loss on train set:  -51.504377040549926\n",
      "Iter:  2826  Loss on train set:  -52.080291730172874\n",
      "Iter:  2827  Loss on train set:  -52.19262195027129\n",
      "Iter:  2828  Loss on train set:  -52.17891825054876\n",
      "Iter:  2829  Loss on train set:  -52.12174608887134\n",
      "Iter:  2830  Loss on train set:  -52.18247221954576\n",
      "Iter:  2831  Loss on train set:  -52.16770178407936\n",
      "Iter:  2832  Loss on train set:  -52.19888141293541\n",
      "Iter:  2833  Loss on train set:  -52.19325762782559\n",
      "Iter:  2834  Loss on train set:  -52.21192553264304\n",
      "Iter:  2835  Loss on train set:  -52.21386863171957\n",
      "Iter:  2836  Loss on train set:  -52.213874423479105\n",
      "Iter:  2837  Loss on train set:  -52.203051469758556\n",
      "Iter:  2838  Loss on train set:  -52.1945052693216\n",
      "Iter:  2839  Loss on train set:  -52.240523599972946\n",
      "Iter:  2840  Loss on train set:  -52.24620044060482\n",
      "Iter:  2841  Loss on train set:  -52.238868199960514\n",
      "Iter:  2842  Loss on train set:  -52.24065953602779\n",
      "Iter:  2843  Loss on train set:  -52.250824123216596\n",
      "Iter:  2844  Loss on train set:  -52.252470466986914\n",
      "Iter:  2845  Loss on train set:  -52.247539666733296\n",
      "Iter:  2846  Loss on train set:  -52.26381681791921\n",
      "Iter:  2847  Loss on train set:  -52.26266193209891\n",
      "Iter:  2848  Loss on train set:  -52.262466068387084\n",
      "Iter:  2849  Loss on train set:  -52.25712436036655\n",
      "Iter:  2850  Loss on train set:  -52.2601188390772\n",
      "Iter:  2851  Loss on train set:  -52.26570568153079\n",
      "Iter:  2852  Loss on train set:  -52.26939013296725\n",
      "Iter:  2853  Loss on train set:  -52.268262564031005\n",
      "Iter:  2854  Loss on train set:  -52.26811053885156\n",
      "Iter:  2855  Loss on train set:  -52.26781655110227\n",
      "Iter:  2856  Loss on train set:  -52.26939013296725\n",
      "     fun: -52.26939013296725\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.07148344,  2.95607828,  0.29893004, -0.30698657,  4.95806864,\n",
      "       -0.05097595, -0.24989731, -1.06057913,  0.05425404])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2857  Loss on train set:  -48.14359553838554\n",
      "Iter:  2858  Loss on train set:  -45.950639807100025\n",
      "Iter:  2859  Loss on train set:  -51.886831665968295\n",
      "Iter:  2860  Loss on train set:  -52.26940004038072\n",
      "Iter:  2861  Loss on train set:  -52.26940426048049\n",
      "Iter:  2862  Loss on train set:  -50.892298652657864\n",
      "Iter:  2863  Loss on train set:  -47.38429409892912\n",
      "Iter:  2864  Loss on train set:  -51.72994189957322\n",
      "Iter:  2865  Loss on train set:  -43.227390851925136\n",
      "Iter:  2866  Loss on train set:  -45.16255955651573\n",
      "Iter:  2867  Loss on train set:  -50.927579833835644\n",
      "Iter:  2868  Loss on train set:  -52.21766237773334\n",
      "Iter:  2869  Loss on train set:  -50.49865624809452\n",
      "Iter:  2870  Loss on train set:  -52.2694001856569\n",
      "Iter:  2871  Loss on train set:  -50.56702447736448\n",
      "Iter:  2872  Loss on train set:  -51.9890510512968\n",
      "Iter:  2873  Loss on train set:  -52.26939335389956\n",
      "Iter:  2874  Loss on train set:  -52.1861499405269\n",
      "Iter:  2875  Loss on train set:  -52.26009588568721\n",
      "Iter:  2876  Loss on train set:  -51.57320579500147\n",
      "Iter:  2877  Loss on train set:  -52.207524592888824\n",
      "Iter:  2878  Loss on train set:  -52.25366026468276\n",
      "Iter:  2879  Loss on train set:  -52.12867054944577\n",
      "Iter:  2880  Loss on train set:  -52.24904607648049\n",
      "Iter:  2881  Loss on train set:  -52.26862562869118\n",
      "Iter:  2882  Loss on train set:  -52.216348051408964\n",
      "Iter:  2883  Loss on train set:  -52.27170748518905\n",
      "Iter:  2884  Loss on train set:  -52.239327901932995\n",
      "Iter:  2885  Loss on train set:  -52.271712115587846\n",
      "Iter:  2886  Loss on train set:  -52.25502745375266\n",
      "Iter:  2887  Loss on train set:  -52.27171001054188\n",
      "Iter:  2888  Loss on train set:  -52.25397997584764\n",
      "Iter:  2889  Loss on train set:  -52.271076091958136\n",
      "Iter:  2890  Loss on train set:  -52.26905624696362\n",
      "Iter:  2891  Loss on train set:  -52.22988277071296\n",
      "Iter:  2892  Loss on train set:  -52.26456099157152\n",
      "Iter:  2893  Loss on train set:  -52.26221989661721\n",
      "Iter:  2894  Loss on train set:  -52.27117144053394\n",
      "Iter:  2895  Loss on train set:  -52.2715337484461\n",
      "Iter:  2896  Loss on train set:  -52.26953104277881\n",
      "Iter:  2897  Loss on train set:  -52.27207551028194\n",
      "Iter:  2898  Loss on train set:  -52.27258873476334\n",
      "Iter:  2899  Loss on train set:  -52.27285254725218\n",
      "Iter:  2900  Loss on train set:  -52.26919399564189\n",
      "Iter:  2901  Loss on train set:  -52.273022439717344\n",
      "Iter:  2902  Loss on train set:  -52.27239972021326\n",
      "Iter:  2903  Loss on train set:  -52.27302520755288\n",
      "Iter:  2904  Loss on train set:  -52.27177954199994\n",
      "Iter:  2905  Loss on train set:  -52.273026524959604\n",
      "Iter:  2906  Loss on train set:  -52.27270595168213\n",
      "Iter:  2907  Loss on train set:  -52.273026524959604\n",
      "     fun: -52.273026524959604\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.08127036,  0.05715235, -0.052699  ,  5.87272601,  2.53381186,\n",
      "        0.02295641,  0.03995662,  0.02189635, -0.01357247])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2908  Loss on train set:  -42.90972157187042\n",
      "Iter:  2909  Loss on train set:  -41.61417214096756\n",
      "Iter:  2910  Loss on train set:  -45.884721512129225\n",
      "Iter:  2911  Loss on train set:  -41.426989446431215\n",
      "Iter:  2912  Loss on train set:  -51.857616508400696\n",
      "Iter:  2913  Loss on train set:  -40.65458628571699\n",
      "Iter:  2914  Loss on train set:  -46.73948633847266\n",
      "Iter:  2915  Loss on train set:  -47.55341618515156\n",
      "Iter:  2916  Loss on train set:  -47.51500703579805\n",
      "Iter:  2917  Loss on train set:  -38.75525967105268\n",
      "Iter:  2918  Loss on train set:  -48.944377086182214\n",
      "Iter:  2919  Loss on train set:  -51.56895914769308\n",
      "Iter:  2920  Loss on train set:  -52.14219555810357\n",
      "Iter:  2921  Loss on train set:  -51.795417864068575\n",
      "Iter:  2922  Loss on train set:  -52.261601913178026\n",
      "Iter:  2923  Loss on train set:  -51.5626413904374\n",
      "Iter:  2924  Loss on train set:  -52.09831160717663\n",
      "Iter:  2925  Loss on train set:  -51.57824617850312\n",
      "Iter:  2926  Loss on train set:  -52.195407031099194\n",
      "Iter:  2927  Loss on train set:  -51.96076239527613\n",
      "Iter:  2928  Loss on train set:  -52.101279477847925\n",
      "Iter:  2929  Loss on train set:  -52.25412374116654\n",
      "Iter:  2930  Loss on train set:  -52.28178929510947\n",
      "Iter:  2931  Loss on train set:  -52.229279107092985\n",
      "Iter:  2932  Loss on train set:  -52.286074035931016\n",
      "Iter:  2933  Loss on train set:  -52.25012721382369\n",
      "Iter:  2934  Loss on train set:  -52.26981670366679\n",
      "Iter:  2935  Loss on train set:  -52.225727780416875\n",
      "Iter:  2936  Loss on train set:  -52.28817781530583\n",
      "Iter:  2937  Loss on train set:  -52.279157851122186\n",
      "Iter:  2938  Loss on train set:  -52.28779363635999\n",
      "Iter:  2939  Loss on train set:  -52.27614445297193\n",
      "Iter:  2940  Loss on train set:  -52.29455877653857\n",
      "Iter:  2941  Loss on train set:  -52.289798412775525\n",
      "Iter:  2942  Loss on train set:  -52.27790757204322\n",
      "Iter:  2943  Loss on train set:  -52.29111571147148\n",
      "Iter:  2944  Loss on train set:  -52.30152753467662\n",
      "Iter:  2945  Loss on train set:  -52.31352265672809\n",
      "Iter:  2946  Loss on train set:  -52.30790321774536\n",
      "Iter:  2947  Loss on train set:  -52.31352630106061\n",
      "Iter:  2948  Loss on train set:  -52.301856176541854\n",
      "Iter:  2949  Loss on train set:  -52.31005125394088\n",
      "Iter:  2950  Loss on train set:  -52.31503814888219\n",
      "Iter:  2951  Loss on train set:  -52.31437919251814\n",
      "Iter:  2952  Loss on train set:  -52.31434943858135\n",
      "Iter:  2953  Loss on train set:  -52.31232059814982\n",
      "Iter:  2954  Loss on train set:  -52.31394923164429\n",
      "Iter:  2955  Loss on train set:  -52.313933301711266\n",
      "Iter:  2956  Loss on train set:  -52.31447920332859\n",
      "Iter:  2957  Loss on train set:  -52.31365453485701\n",
      "Iter:  2958  Loss on train set:  -52.31503814888219\n",
      "     fun: -52.31503814888219\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-1.50506078e+00,  1.57937383e-01, -1.30260754e-03, -2.84426915e-01,\n",
      "        1.20029487e-02, -8.07121689e-01, -5.72866775e-01, -7.56708942e-03,\n",
      "        4.38703740e-02])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2959  Loss on train set:  -45.9332668080285\n",
      "Iter:  2960  Loss on train set:  -47.53113066821723\n",
      "Iter:  2961  Loss on train set:  -48.88153640402279\n",
      "Iter:  2962  Loss on train set:  -38.05045459304297\n",
      "Iter:  2963  Loss on train set:  -41.659055462202915\n",
      "Iter:  2964  Loss on train set:  -52.315043897990314\n",
      "Iter:  2965  Loss on train set:  -41.102940243551245\n",
      "Iter:  2966  Loss on train set:  -52.315041246119826\n",
      "Iter:  2967  Loss on train set:  -52.31504000314927\n",
      "Iter:  2968  Loss on train set:  -35.86724676052489\n",
      "Iter:  2969  Loss on train set:  -50.55199553490357\n",
      "Iter:  2970  Loss on train set:  -52.0738741554411\n",
      "Iter:  2971  Loss on train set:  -51.007642765992124\n",
      "Iter:  2972  Loss on train set:  -51.67678634659449\n",
      "Iter:  2973  Loss on train set:  -49.486614177133255\n",
      "Iter:  2974  Loss on train set:  -51.13291074662276\n",
      "Iter:  2975  Loss on train set:  -52.31503487398553\n",
      "Iter:  2976  Loss on train set:  -51.69996392834672\n",
      "Iter:  2977  Loss on train set:  -52.3150418401299\n",
      "Iter:  2978  Loss on train set:  -51.53440062889409\n",
      "Iter:  2979  Loss on train set:  -52.315045045089576\n",
      "Iter:  2980  Loss on train set:  -52.01876740443882\n",
      "Iter:  2981  Loss on train set:  -52.228757897762144\n",
      "Iter:  2982  Loss on train set:  -52.19690727317173\n",
      "Iter:  2983  Loss on train set:  -52.137763876804414\n",
      "Iter:  2984  Loss on train set:  -52.29599014859005\n",
      "Iter:  2985  Loss on train set:  -52.164340522171585\n",
      "Iter:  2986  Loss on train set:  -52.25520610649962\n",
      "Iter:  2987  Loss on train set:  -52.155150004302264\n",
      "Iter:  2988  Loss on train set:  -52.30752865843941\n",
      "Iter:  2989  Loss on train set:  -52.31504095751333\n",
      "Iter:  2990  Loss on train set:  -52.29070440398042\n",
      "Iter:  2991  Loss on train set:  -52.315041878223\n",
      "Iter:  2992  Loss on train set:  -52.266556151807315\n",
      "Iter:  2993  Loss on train set:  -52.30815459680818\n",
      "Iter:  2994  Loss on train set:  -52.31094509621875\n",
      "Iter:  2995  Loss on train set:  -52.311090779886804\n",
      "Iter:  2996  Loss on train set:  -52.31381893108944\n",
      "Iter:  2997  Loss on train set:  -52.31529029474309\n",
      "Iter:  2998  Loss on train set:  -52.31396188503124\n",
      "Iter:  2999  Loss on train set:  -52.315680785191255\n",
      "Iter:  3000  Loss on train set:  -52.310752512482324\n",
      "Iter:  3001  Loss on train set:  -52.3149409447703\n",
      "Iter:  3002  Loss on train set:  -52.317267603174486\n",
      "Iter:  3003  Loss on train set:  -52.314820595014666\n",
      "Iter:  3004  Loss on train set:  -52.31726088900643\n",
      "Iter:  3005  Loss on train set:  -52.31395021060426\n",
      "Iter:  3006  Loss on train set:  -52.31726081024512\n",
      "Iter:  3007  Loss on train set:  -52.31662735650106\n",
      "Iter:  3008  Loss on train set:  -52.3165791805507\n",
      "Iter:  3009  Loss on train set:  -52.317267603174486\n",
      "     fun: -52.317267603174486\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 5.61821763e-03, -6.33855914e-03, -6.09432090e-02,  1.24018356e-02,\n",
      "        1.62599245e-01,  5.12486962e+00, -8.07911275e-01,  5.87014086e+00,\n",
      "        3.54896745e+00])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3010  Loss on train set:  -46.85297365576895\n",
      "Iter:  3011  Loss on train set:  -52.31727233698507\n",
      "Iter:  3012  Loss on train set:  -51.7878983697465\n",
      "Iter:  3013  Loss on train set:  -50.931814431674816\n",
      "Iter:  3014  Loss on train set:  -47.719465946484455\n",
      "Iter:  3015  Loss on train set:  -45.85332038913634\n",
      "Iter:  3016  Loss on train set:  -52.31726960545891\n",
      "Iter:  3017  Loss on train set:  -41.50067924381031\n",
      "Iter:  3018  Loss on train set:  -51.85880726832876\n",
      "Iter:  3019  Loss on train set:  -43.244148704366374\n",
      "Iter:  3020  Loss on train set:  -50.73777911918824\n",
      "Iter:  3021  Loss on train set:  -52.225070332270754\n",
      "Iter:  3022  Loss on train set:  -52.3172770899159\n",
      "Iter:  3023  Loss on train set:  -52.28455703040166\n",
      "Iter:  3024  Loss on train set:  -52.251745979499375\n",
      "Iter:  3025  Loss on train set:  -51.735164271186726\n",
      "Iter:  3026  Loss on train set:  -52.31727291789592\n",
      "Iter:  3027  Loss on train set:  -51.82887127381153\n",
      "Iter:  3028  Loss on train set:  -52.311162136229854\n",
      "Iter:  3029  Loss on train set:  -51.66125612723371\n",
      "Iter:  3030  Loss on train set:  -52.219241214052616\n",
      "Iter:  3031  Loss on train set:  -52.310588646069625\n",
      "Iter:  3032  Loss on train set:  -52.13717340636601\n",
      "Iter:  3033  Loss on train set:  -52.28768158376565\n",
      "Iter:  3034  Loss on train set:  -52.31647960265833\n",
      "Iter:  3035  Loss on train set:  -52.28295927787699\n",
      "Iter:  3036  Loss on train set:  -52.31558599399984\n",
      "Iter:  3037  Loss on train set:  -52.31726891316462\n",
      "Iter:  3038  Loss on train set:  -52.307277113626014\n",
      "Iter:  3039  Loss on train set:  -52.31687589555524\n",
      "Iter:  3040  Loss on train set:  -52.30998896868116\n",
      "Iter:  3041  Loss on train set:  -52.31727146198539\n",
      "Iter:  3042  Loss on train set:  -52.310392508479275\n",
      "Iter:  3043  Loss on train set:  -52.3167984522113\n",
      "Iter:  3044  Loss on train set:  -52.31516563974637\n",
      "Iter:  3045  Loss on train set:  -52.3172776671984\n",
      "Iter:  3046  Loss on train set:  -52.317308873386935\n",
      "Iter:  3047  Loss on train set:  -52.316794323758955\n",
      "Iter:  3048  Loss on train set:  -52.31749849712618\n",
      "Iter:  3049  Loss on train set:  -52.31779158416646\n",
      "Iter:  3050  Loss on train set:  -52.31773684856315\n",
      "Iter:  3051  Loss on train set:  -52.317582152707956\n",
      "Iter:  3052  Loss on train set:  -52.31778400287369\n",
      "Iter:  3053  Loss on train set:  -52.31761223379745\n",
      "Iter:  3054  Loss on train set:  -52.31779087645925\n",
      "Iter:  3055  Loss on train set:  -52.31751814611706\n",
      "Iter:  3056  Loss on train set:  -52.3176361449081\n",
      "Iter:  3057  Loss on train set:  -52.317589960211045\n",
      "Iter:  3058  Loss on train set:  -52.31789093000704\n",
      "Iter:  3059  Loss on train set:  -52.31792915811265\n",
      "Iter:  3060  Loss on train set:  -52.31789093000704\n",
      "     fun: -52.31789093000704\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-3.32749542e-02,  6.08306137e+00,  2.13433653e-02,  2.32013106e-02,\n",
      "       -3.85271757e-02,  1.81890164e-03,  5.87016099e+00,  1.57126936e-01,\n",
      "        1.10811870e-02])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3061  Loss on train set:  -51.86145265366273\n",
      "Iter:  3062  Loss on train set:  -39.285801064092354\n",
      "Iter:  3063  Loss on train set:  -52.31788971197845\n",
      "Iter:  3064  Loss on train set:  -52.317887789316785\n",
      "Iter:  3065  Loss on train set:  -43.76997668323827\n",
      "Iter:  3066  Loss on train set:  -50.787294600799044\n",
      "Iter:  3067  Loss on train set:  -47.27673482905241\n",
      "Iter:  3068  Loss on train set:  -52.31788954702928\n",
      "Iter:  3069  Loss on train set:  -52.317891666556754\n",
      "Iter:  3070  Loss on train set:  -47.42019428491283\n",
      "Iter:  3071  Loss on train set:  -47.591323054476646\n",
      "Iter:  3072  Loss on train set:  -52.29279749453469\n",
      "Iter:  3073  Loss on train set:  -49.98986879939496\n",
      "Iter:  3074  Loss on train set:  -52.31789269405207\n",
      "Iter:  3075  Loss on train set:  -50.88192507682395\n",
      "Iter:  3076  Loss on train set:  -52.317893632079816\n",
      "Iter:  3077  Loss on train set:  -51.94684108729724\n",
      "Iter:  3078  Loss on train set:  -52.3178939621653\n",
      "Iter:  3079  Loss on train set:  -50.92897058437239\n",
      "Iter:  3080  Loss on train set:  -52.31789583942194\n",
      "Iter:  3081  Loss on train set:  -47.528959661334966\n",
      "Iter:  3082  Loss on train set:  -51.74965465163554\n",
      "Iter:  3083  Loss on train set:  -52.2737759576412\n",
      "Iter:  3084  Loss on train set:  -51.95126785822069\n",
      "Iter:  3085  Loss on train set:  -52.310063280495676\n",
      "Iter:  3086  Loss on train set:  -52.004614764532356\n",
      "Iter:  3087  Loss on train set:  -51.94625793835412\n",
      "Iter:  3088  Loss on train set:  -52.31790206796423\n",
      "Iter:  3089  Loss on train set:  -52.217525224198894\n",
      "Iter:  3090  Loss on train set:  -52.31789215144731\n",
      "Iter:  3091  Loss on train set:  -52.28868247273787\n",
      "Iter:  3092  Loss on train set:  -52.31789115519129\n",
      "Iter:  3093  Loss on train set:  -52.229181017105205\n",
      "Iter:  3094  Loss on train set:  -52.2189239210449\n",
      "Iter:  3095  Loss on train set:  -52.31789233629296\n",
      "Iter:  3096  Loss on train set:  -52.30217896978058\n",
      "Iter:  3097  Loss on train set:  -52.322787461473915\n",
      "Iter:  3098  Loss on train set:  -52.33497466839471\n",
      "Iter:  3099  Loss on train set:  -52.329009196427684\n",
      "Iter:  3100  Loss on train set:  -52.25690474485717\n",
      "Iter:  3101  Loss on train set:  -52.32991998898127\n",
      "Iter:  3102  Loss on train set:  -52.33497751832341\n",
      "Iter:  3103  Loss on train set:  -52.330922669165446\n",
      "Iter:  3104  Loss on train set:  -52.33497307034395\n",
      "Iter:  3105  Loss on train set:  -52.336201243539705\n",
      "Iter:  3106  Loss on train set:  -52.336196691419936\n",
      "Iter:  3107  Loss on train set:  -52.33647440630846\n",
      "Iter:  3108  Loss on train set:  -52.331612571402\n",
      "Iter:  3109  Loss on train set:  -52.336445965539205\n",
      "Iter:  3110  Loss on train set:  -52.33593329835465\n",
      "Iter:  3111  Loss on train set:  -52.33647440630846\n",
      "     fun: -52.33647440630846\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.03317469, -0.2759432 ,  5.76925406,  4.87482121, -1.5274142 ,\n",
      "        3.16132711, -0.08207118,  2.29943909,  9.11187976])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3112  Loss on train set:  -48.19397255296418\n",
      "Iter:  3113  Loss on train set:  -40.94228625255934\n",
      "Iter:  3114  Loss on train set:  -43.721225778634945\n",
      "Iter:  3115  Loss on train set:  -46.41879638301234\n",
      "Iter:  3116  Loss on train set:  -52.33647071672778\n",
      "Iter:  3117  Loss on train set:  -42.1845962642988\n",
      "Iter:  3118  Loss on train set:  -41.00192884510467\n",
      "Iter:  3119  Loss on train set:  -40.19771735115601\n",
      "Iter:  3120  Loss on train set:  -50.94887050038743\n",
      "Iter:  3121  Loss on train set:  -36.708652632839645\n",
      "Iter:  3122  Loss on train set:  -47.89837203695682\n",
      "Iter:  3123  Loss on train set:  -51.740961970514704\n",
      "Iter:  3124  Loss on train set:  -52.24912077164022\n",
      "Iter:  3125  Loss on train set:  -51.65103250053887\n",
      "Iter:  3126  Loss on train set:  -52.33647565816679\n",
      "Iter:  3127  Loss on train set:  -51.90950891922941\n",
      "Iter:  3128  Loss on train set:  -52.16394907347437\n",
      "Iter:  3129  Loss on train set:  -51.80733080795708\n",
      "Iter:  3130  Loss on train set:  -52.312077370472686\n",
      "Iter:  3131  Loss on train set:  -51.553522067128654\n",
      "Iter:  3132  Loss on train set:  -52.04254922681825\n",
      "Iter:  3133  Loss on train set:  -52.31092905512877\n",
      "Iter:  3134  Loss on train set:  -52.105892198033594\n",
      "Iter:  3135  Loss on train set:  -52.29324074743899\n",
      "Iter:  3136  Loss on train set:  -52.33595002492517\n",
      "Iter:  3137  Loss on train set:  -52.323369927105475\n",
      "Iter:  3138  Loss on train set:  -52.316131469431305\n",
      "Iter:  3139  Loss on train set:  -52.321923355876635\n",
      "Iter:  3140  Loss on train set:  -52.33647744616497\n",
      "Iter:  3141  Loss on train set:  -52.33225953235106\n",
      "Iter:  3142  Loss on train set:  -52.33530467538149\n",
      "Iter:  3143  Loss on train set:  -52.323495836174104\n",
      "Iter:  3144  Loss on train set:  -52.33652742304697\n",
      "Iter:  3145  Loss on train set:  -52.335455901842614\n",
      "Iter:  3146  Loss on train set:  -52.33040465018884\n",
      "Iter:  3147  Loss on train set:  -52.33967485907834\n",
      "Iter:  3148  Loss on train set:  -52.338776382064225\n",
      "Iter:  3149  Loss on train set:  -52.33580154659083\n",
      "Iter:  3150  Loss on train set:  -52.344826643082875\n",
      "Iter:  3151  Loss on train set:  -52.35002697813324\n",
      "Iter:  3152  Loss on train set:  -52.35115128263803\n",
      "Iter:  3153  Loss on train set:  -52.35115014352713\n",
      "Iter:  3154  Loss on train set:  -52.353012011635144\n",
      "Iter:  3155  Loss on train set:  -52.35027268990987\n",
      "Iter:  3156  Loss on train set:  -52.35243546376418\n",
      "Iter:  3157  Loss on train set:  -52.34974365666245\n",
      "Iter:  3158  Loss on train set:  -52.35187496376436\n",
      "Iter:  3159  Loss on train set:  -52.354329522254076\n",
      "Iter:  3160  Loss on train set:  -52.35213262822936\n",
      "Iter:  3161  Loss on train set:  -52.35175692230811\n",
      "Iter:  3162  Loss on train set:  -52.354329522254076\n",
      "     fun: -52.354329522254076\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.07809164, -0.30705183, -1.54221703, -0.58767882,  8.97125593,\n",
      "        0.19716831,  0.48527624, -0.29257237,  0.03247865])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3163  Loss on train set:  -43.322936383094785\n",
      "Iter:  3164  Loss on train set:  -50.08649487185938\n",
      "Iter:  3165  Loss on train set:  -40.204350110832834\n",
      "Iter:  3166  Loss on train set:  -42.18400201903135\n",
      "Iter:  3167  Loss on train set:  -40.56690280798239\n",
      "Iter:  3168  Loss on train set:  -42.2749262459014\n",
      "Iter:  3169  Loss on train set:  -46.890942237783875\n",
      "Iter:  3170  Loss on train set:  -47.65231093517237\n",
      "Iter:  3171  Loss on train set:  -52.35433116458924\n",
      "Iter:  3172  Loss on train set:  -40.5881565895729\n",
      "Iter:  3173  Loss on train set:  -50.025354853532804\n",
      "Iter:  3174  Loss on train set:  -52.19508051638471\n",
      "Iter:  3175  Loss on train set:  -49.02470570456936\n",
      "Iter:  3176  Loss on train set:  -51.59136705619245\n",
      "Iter:  3177  Loss on train set:  -50.2052170202808\n",
      "Iter:  3178  Loss on train set:  -51.829047097313214\n",
      "Iter:  3179  Loss on train set:  -49.70552415503064\n",
      "Iter:  3180  Loss on train set:  -51.063068325971656\n",
      "Iter:  3181  Loss on train set:  -51.61945615319907\n",
      "Iter:  3182  Loss on train set:  -52.35433774141048\n",
      "Iter:  3183  Loss on train set:  -52.025674433248625\n",
      "Iter:  3184  Loss on train set:  -52.22055566075158\n",
      "Iter:  3185  Loss on train set:  -52.10864865715304\n",
      "Iter:  3186  Loss on train set:  -52.237637420641974\n",
      "Iter:  3187  Loss on train set:  -52.26168610367628\n",
      "Iter:  3188  Loss on train set:  -52.22052091872097\n",
      "Iter:  3189  Loss on train set:  -52.33073794987123\n",
      "Iter:  3190  Loss on train set:  -52.11280125736015\n",
      "Iter:  3191  Loss on train set:  -52.344970388879744\n",
      "Iter:  3192  Loss on train set:  -52.21241972144231\n",
      "Iter:  3193  Loss on train set:  -52.344476512052914\n",
      "Iter:  3194  Loss on train set:  -52.347729180639234\n",
      "Iter:  3195  Loss on train set:  -52.33233682964189\n",
      "Iter:  3196  Loss on train set:  -52.35476399848293\n",
      "Iter:  3197  Loss on train set:  -52.356796229718285\n",
      "Iter:  3198  Loss on train set:  -52.354369986878496\n",
      "Iter:  3199  Loss on train set:  -52.35660373352289\n",
      "Iter:  3200  Loss on train set:  -52.355948210400264\n",
      "Iter:  3201  Loss on train set:  -52.35391218592238\n",
      "Iter:  3202  Loss on train set:  -52.357394450744344\n",
      "Iter:  3203  Loss on train set:  -52.350246718521134\n",
      "Iter:  3204  Loss on train set:  -52.35459487862414\n",
      "Iter:  3205  Loss on train set:  -52.356579494287644\n",
      "Iter:  3206  Loss on train set:  -52.35573062352812\n",
      "Iter:  3207  Loss on train set:  -52.35447807642568\n",
      "Iter:  3208  Loss on train set:  -52.35911083452145\n",
      "Iter:  3209  Loss on train set:  -52.36094404417632\n",
      "Iter:  3210  Loss on train set:  -52.35904084531842\n",
      "Iter:  3211  Loss on train set:  -52.3583262370551\n",
      "Iter:  3212  Loss on train set:  -52.362084961892755\n",
      "Iter:  3213  Loss on train set:  -52.36094404417632\n",
      "     fun: -52.36094404417632\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.03111147, -0.11560715, -0.29021087,  0.12946517,  0.49131907,\n",
      "       -0.71066908, -0.02949704, -0.01049358,  5.99375075])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3214  Loss on train set:  -52.36093348420006\n",
      "Iter:  3215  Loss on train set:  -46.3848033504802\n",
      "Iter:  3216  Loss on train set:  -50.75472625087411\n",
      "Iter:  3217  Loss on train set:  -40.08608480150408\n",
      "Iter:  3218  Loss on train set:  -49.98804505689895\n",
      "Iter:  3219  Loss on train set:  -43.68937655999968\n",
      "Iter:  3220  Loss on train set:  -51.88815410107288\n",
      "Iter:  3221  Loss on train set:  -37.22495866896752\n",
      "Iter:  3222  Loss on train set:  -49.85738843423796\n",
      "Iter:  3223  Loss on train set:  -41.77217566912795\n",
      "Iter:  3224  Loss on train set:  -49.5685349138229\n",
      "Iter:  3225  Loss on train set:  -51.85697477251155\n",
      "Iter:  3226  Loss on train set:  -52.360940570571074\n",
      "Iter:  3227  Loss on train set:  -52.279464388453626\n",
      "Iter:  3228  Loss on train set:  -52.31850357717138\n",
      "Iter:  3229  Loss on train set:  -51.541098594873034\n",
      "Iter:  3230  Loss on train set:  -52.361823932339355\n",
      "Iter:  3231  Loss on train set:  -51.781974111062965\n",
      "Iter:  3232  Loss on train set:  -52.30084853694107\n",
      "Iter:  3233  Loss on train set:  -51.304471623170215\n",
      "Iter:  3234  Loss on train set:  -52.235030701215926\n",
      "Iter:  3235  Loss on train set:  -52.34751002683797\n",
      "Iter:  3236  Loss on train set:  -52.21068014307673\n",
      "Iter:  3237  Loss on train set:  -52.16917241741827\n",
      "Iter:  3238  Loss on train set:  -52.27857498138191\n",
      "Iter:  3239  Loss on train set:  -52.3618126267595\n",
      "Iter:  3240  Loss on train set:  -52.343987412205124\n",
      "Iter:  3241  Loss on train set:  -52.360940143699466\n",
      "Iter:  3242  Loss on train set:  -52.31670424530985\n",
      "Iter:  3243  Loss on train set:  -52.36260531628933\n",
      "Iter:  3244  Loss on train set:  -52.36023966290358\n",
      "Iter:  3245  Loss on train set:  -52.346786324259604\n",
      "Iter:  3246  Loss on train set:  -52.36177996822922\n",
      "Iter:  3247  Loss on train set:  -52.34642005220467\n",
      "Iter:  3248  Loss on train set:  -52.35718047009439\n",
      "Iter:  3249  Loss on train set:  -52.36271312644781\n",
      "Iter:  3250  Loss on train set:  -52.36240718832042\n",
      "Iter:  3251  Loss on train set:  -52.35082519112495\n",
      "Iter:  3252  Loss on train set:  -52.36462699801174\n",
      "Iter:  3253  Loss on train set:  -52.363862557689174\n",
      "Iter:  3254  Loss on train set:  -52.36385943347335\n",
      "Iter:  3255  Loss on train set:  -52.36250551133231\n",
      "Iter:  3256  Loss on train set:  -52.36464769534258\n",
      "Iter:  3257  Loss on train set:  -52.366341268995534\n",
      "Iter:  3258  Loss on train set:  -52.36388095158009\n",
      "Iter:  3259  Loss on train set:  -52.367668336895555\n",
      "Iter:  3260  Loss on train set:  -52.36649087819745\n",
      "Iter:  3261  Loss on train set:  -52.36705833327525\n",
      "Iter:  3262  Loss on train set:  -52.369118918547045\n",
      "Iter:  3263  Loss on train set:  -52.36795955904433\n",
      "Iter:  3264  Loss on train set:  -52.369118918547045\n",
      "     fun: -52.369118918547045\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 5.98586287e+00,  2.75264553e-01, -3.33492413e-02, -2.96500793e-01,\n",
      "       -1.12194074e-01, -1.49142993e-02, -8.00539888e-02, -2.67373693e-03,\n",
      "       -2.77523306e-02])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3265  Loss on train set:  -40.597250903919914\n",
      "Iter:  3266  Loss on train set:  -43.41866240712242\n",
      "Iter:  3267  Loss on train set:  -52.36912061693304\n",
      "Iter:  3268  Loss on train set:  -39.699888082440914\n",
      "Iter:  3269  Loss on train set:  -52.36911875093012\n",
      "Iter:  3270  Loss on train set:  -40.08337219599147\n",
      "Iter:  3271  Loss on train set:  -37.617352191312285\n",
      "Iter:  3272  Loss on train set:  -47.87025681875037\n",
      "Iter:  3273  Loss on train set:  -50.67881485683239\n",
      "Iter:  3274  Loss on train set:  -44.62032540672833\n",
      "Iter:  3275  Loss on train set:  -49.70390041405519\n",
      "Iter:  3276  Loss on train set:  -50.01840142241754\n",
      "Iter:  3277  Loss on train set:  -51.7936239388746\n",
      "Iter:  3278  Loss on train set:  -52.36912055021387\n",
      "Iter:  3279  Loss on train set:  -51.13084950700319\n",
      "Iter:  3280  Loss on train set:  -52.369118558393986\n",
      "Iter:  3281  Loss on train set:  -51.542055093235774\n",
      "Iter:  3282  Loss on train set:  -52.28273901477908\n",
      "Iter:  3283  Loss on train set:  -51.34096323476474\n",
      "Iter:  3284  Loss on train set:  -52.335339814583094\n",
      "Iter:  3285  Loss on train set:  -51.808548870404415\n",
      "Iter:  3286  Loss on train set:  -52.208442312304555\n",
      "Iter:  3287  Loss on train set:  -52.33994049362594\n",
      "Iter:  3288  Loss on train set:  -52.35311153106678\n",
      "Iter:  3289  Loss on train set:  -52.284361277071305\n",
      "Iter:  3290  Loss on train set:  -52.356726826314706\n",
      "Iter:  3291  Loss on train set:  -52.31400000332338\n",
      "Iter:  3292  Loss on train set:  -52.36142918317042\n",
      "Iter:  3293  Loss on train set:  -52.3691174885057\n",
      "Iter:  3294  Loss on train set:  -52.36755924917385\n",
      "Iter:  3295  Loss on train set:  -52.36912118477155\n",
      "Iter:  3296  Loss on train set:  -52.36511558035505\n",
      "Iter:  3297  Loss on train set:  -52.355860481486175\n",
      "Iter:  3298  Loss on train set:  -52.3633890644419\n",
      "Iter:  3299  Loss on train set:  -52.369512865367255\n",
      "Iter:  3300  Loss on train set:  -52.36739818567874\n",
      "Iter:  3301  Loss on train set:  -52.37006873926182\n",
      "Iter:  3302  Loss on train set:  -52.36577156670897\n",
      "Iter:  3303  Loss on train set:  -52.369975557117314\n",
      "Iter:  3304  Loss on train set:  -52.36729467831463\n",
      "Iter:  3305  Loss on train set:  -52.36876479788881\n",
      "Iter:  3306  Loss on train set:  -52.36876894468117\n",
      "Iter:  3307  Loss on train set:  -52.37007475338013\n",
      "Iter:  3308  Loss on train set:  -52.36986422762231\n",
      "Iter:  3309  Loss on train set:  -52.37007219778826\n",
      "Iter:  3310  Loss on train set:  -52.36976694155029\n",
      "Iter:  3311  Loss on train set:  -52.370120743672516\n",
      "Iter:  3312  Loss on train set:  -52.36996927291348\n",
      "Iter:  3313  Loss on train set:  -52.369780967043525\n",
      "Iter:  3314  Loss on train set:  -52.3701545427934\n",
      "Iter:  3315  Loss on train set:  -52.370120743672516\n",
      "     fun: -52.370120743672516\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.49366496, -0.01258562,  5.07603575,  3.81716514,  4.43696831,\n",
      "       -0.29577988, -0.00643897, -0.04209521, -0.03141986])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3316  Loss on train set:  -39.86105006223363\n",
      "Iter:  3317  Loss on train set:  -43.673308699358955\n",
      "Iter:  3318  Loss on train set:  -40.57200238714511\n",
      "Iter:  3319  Loss on train set:  -46.6279885742531\n",
      "Iter:  3320  Loss on train set:  -41.71824077577677\n",
      "Iter:  3321  Loss on train set:  -45.809912046249906\n",
      "Iter:  3322  Loss on train set:  -49.834458189549345\n",
      "Iter:  3323  Loss on train set:  -47.03356325203689\n",
      "Iter:  3324  Loss on train set:  -50.041576748578606\n",
      "Iter:  3325  Loss on train set:  -41.72027842096594\n",
      "Iter:  3326  Loss on train set:  -49.58489437220742\n",
      "Iter:  3327  Loss on train set:  -51.62312398431105\n",
      "Iter:  3328  Loss on train set:  -52.16548908040602\n",
      "Iter:  3329  Loss on train set:  -51.758493316663056\n",
      "Iter:  3330  Loss on train set:  -52.26808951536623\n",
      "Iter:  3331  Loss on train set:  -51.844070439176356\n",
      "Iter:  3332  Loss on train set:  -52.30788360975409\n",
      "Iter:  3333  Loss on train set:  -51.88773643517333\n",
      "Iter:  3334  Loss on train set:  -52.311532291511796\n",
      "Iter:  3335  Loss on train set:  -52.04914153805148\n",
      "Iter:  3336  Loss on train set:  -52.20642325481625\n",
      "Iter:  3337  Loss on train set:  -52.34980005460297\n",
      "Iter:  3338  Loss on train set:  -52.36625778490898\n",
      "Iter:  3339  Loss on train set:  -52.329326717724456\n",
      "Iter:  3340  Loss on train set:  -52.3732694641887\n",
      "Iter:  3341  Loss on train set:  -52.32314648397226\n",
      "Iter:  3342  Loss on train set:  -52.368322411028515\n",
      "Iter:  3343  Loss on train set:  -52.37126846341023\n",
      "Iter:  3344  Loss on train set:  -52.3529429897998\n",
      "Iter:  3345  Loss on train set:  -52.36972812224881\n",
      "Iter:  3346  Loss on train set:  -52.37252465826986\n",
      "Iter:  3347  Loss on train set:  -52.36971614550271\n",
      "Iter:  3348  Loss on train set:  -52.37257788009901\n",
      "Iter:  3349  Loss on train set:  -52.37188096931226\n",
      "Iter:  3350  Loss on train set:  -52.37235927833078\n",
      "Iter:  3351  Loss on train set:  -52.37624871834602\n",
      "Iter:  3352  Loss on train set:  -52.372502064797175\n",
      "Iter:  3353  Loss on train set:  -52.37662817441737\n",
      "Iter:  3354  Loss on train set:  -52.37866495704784\n",
      "Iter:  3355  Loss on train set:  -52.37915819800571\n",
      "Iter:  3356  Loss on train set:  -52.377348033106145\n",
      "Iter:  3357  Loss on train set:  -52.37929926769211\n",
      "Iter:  3358  Loss on train set:  -52.379307742180835\n",
      "Iter:  3359  Loss on train set:  -52.37780299791089\n",
      "Iter:  3360  Loss on train set:  -52.37955772961153\n",
      "Iter:  3361  Loss on train set:  -52.37935669437791\n",
      "Iter:  3362  Loss on train set:  -52.37915590361076\n",
      "Iter:  3363  Loss on train set:  -52.37935569278209\n",
      "Iter:  3364  Loss on train set:  -52.379386780439354\n",
      "Iter:  3365  Loss on train set:  -52.37991183022203\n",
      "Iter:  3366  Loss on train set:  -52.37955772961153\n",
      "     fun: -52.37955772961153\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 3.81805587, -0.39821164,  0.48988244,  0.28546892,  0.19683403,\n",
      "        0.02760644, -0.0434537 , -0.02305939, -0.10717302])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3367  Loss on train set:  -47.86829071631725\n",
      "Iter:  3368  Loss on train set:  -46.99277867680425\n",
      "Iter:  3369  Loss on train set:  -52.37954679954044\n",
      "Iter:  3370  Loss on train set:  -44.516131751029675\n",
      "Iter:  3371  Loss on train set:  -50.64950261181858\n",
      "Iter:  3372  Loss on train set:  -42.51495352055616\n",
      "Iter:  3373  Loss on train set:  -48.51891346501852\n",
      "Iter:  3374  Loss on train set:  -39.71268927281182\n",
      "Iter:  3375  Loss on train set:  -50.900092942152796\n",
      "Iter:  3376  Loss on train set:  -43.87802520743676\n",
      "Iter:  3377  Loss on train set:  -50.102718608110266\n",
      "Iter:  3378  Loss on train set:  -52.01257835243631\n",
      "Iter:  3379  Loss on train set:  -52.37955453646255\n",
      "Iter:  3380  Loss on train set:  -52.074670029578456\n",
      "Iter:  3381  Loss on train set:  -52.34308435809055\n",
      "Iter:  3382  Loss on train set:  -51.80665497000522\n",
      "Iter:  3383  Loss on train set:  -52.279722007495565\n",
      "Iter:  3384  Loss on train set:  -51.74068789192888\n",
      "Iter:  3385  Loss on train set:  -52.35534772894661\n",
      "Iter:  3386  Loss on train set:  -51.48770159718696\n",
      "Iter:  3387  Loss on train set:  -52.284349142665626\n",
      "Iter:  3388  Loss on train set:  -52.36135561474997\n",
      "Iter:  3389  Loss on train set:  -52.37871361694575\n",
      "Iter:  3390  Loss on train set:  -52.33262359981526\n",
      "Iter:  3391  Loss on train set:  -52.36773728431834\n",
      "Iter:  3392  Loss on train set:  -52.345541284397065\n",
      "Iter:  3393  Loss on train set:  -52.36733640082012\n",
      "Iter:  3394  Loss on train set:  -52.37955400799183\n",
      "Iter:  3395  Loss on train set:  -52.37613180603992\n",
      "Iter:  3396  Loss on train set:  -52.379975708349065\n",
      "Iter:  3397  Loss on train set:  -52.38137068439935\n",
      "Iter:  3398  Loss on train set:  -52.37777890304379\n",
      "Iter:  3399  Loss on train set:  -52.377336900936406\n",
      "Iter:  3400  Loss on train set:  -52.38134652134805\n",
      "Iter:  3401  Loss on train set:  -52.37768161283893\n",
      "Iter:  3402  Loss on train set:  -52.37925175915039\n",
      "Iter:  3403  Loss on train set:  -52.38066178348303\n",
      "Iter:  3404  Loss on train set:  -52.379312618480284\n",
      "Iter:  3405  Loss on train set:  -52.37960574259121\n",
      "Iter:  3406  Loss on train set:  -52.381498753842166\n",
      "Iter:  3407  Loss on train set:  -52.38059067977912\n",
      "Iter:  3408  Loss on train set:  -52.380842450391626\n",
      "Iter:  3409  Loss on train set:  -52.38299319613765\n",
      "Iter:  3410  Loss on train set:  -52.38301024708725\n",
      "Iter:  3411  Loss on train set:  -52.38342844396396\n",
      "Iter:  3412  Loss on train set:  -52.38336405398252\n",
      "Iter:  3413  Loss on train set:  -52.383504592314495\n",
      "Iter:  3414  Loss on train set:  -52.382707422030144\n",
      "Iter:  3415  Loss on train set:  -52.3827945728382\n",
      "Iter:  3416  Loss on train set:  -52.38314613061928\n",
      "Iter:  3417  Loss on train set:  -52.383504592314495\n",
      "     fun: -52.383504592314495\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-0.03651841, -0.034579  ,  5.98284635,  0.26365902, -0.02515859,\n",
      "       -0.72890641, -0.05027584, -0.18233186,  0.033705  ])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3418  Loss on train set:  -52.38349881882599\n",
      "Iter:  3419  Loss on train set:  -52.383505237827805\n",
      "Iter:  3420  Loss on train set:  -37.750618323898884\n",
      "Iter:  3421  Loss on train set:  -40.107570534206516\n",
      "Iter:  3422  Loss on train set:  -40.91343660223308\n",
      "Iter:  3423  Loss on train set:  -42.817716445506285\n",
      "Iter:  3424  Loss on train set:  -39.960233138663455\n",
      "Iter:  3425  Loss on train set:  -39.11307298909649\n",
      "Iter:  3426  Loss on train set:  -52.383499134778944\n",
      "Iter:  3427  Loss on train set:  -45.712305195691286\n",
      "Iter:  3428  Loss on train set:  -50.63987928258915\n",
      "Iter:  3429  Loss on train set:  -52.383497751597446\n",
      "Iter:  3430  Loss on train set:  -47.95962301694544\n",
      "Iter:  3431  Loss on train set:  -51.451566980340345\n",
      "Iter:  3432  Loss on train set:  -52.38350598978796\n",
      "Iter:  3433  Loss on train set:  -51.439870098013635\n",
      "Iter:  3434  Loss on train set:  -52.19650722678827\n",
      "Iter:  3435  Loss on train set:  -51.237235232235484\n",
      "Iter:  3436  Loss on train set:  -52.3834974230885\n",
      "Iter:  3437  Loss on train set:  -51.56756359345796\n",
      "Iter:  3438  Loss on train set:  -52.1442956956536\n",
      "Iter:  3439  Loss on train set:  -52.38350052840105\n",
      "Iter:  3440  Loss on train set:  -52.246532887579754\n",
      "Iter:  3441  Loss on train set:  -52.14437020742033\n",
      "Iter:  3442  Loss on train set:  -52.316878411466114\n",
      "Iter:  3443  Loss on train set:  -52.39058185353744\n",
      "Iter:  3444  Loss on train set:  -52.33103750658271\n",
      "Iter:  3445  Loss on train set:  -52.37709458854925\n",
      "Iter:  3446  Loss on train set:  -52.390578694192506\n",
      "Iter:  3447  Loss on train set:  -52.372305251953414\n",
      "Iter:  3448  Loss on train set:  -52.39058275106993\n",
      "Iter:  3449  Loss on train set:  -52.373040541056284\n",
      "Iter:  3450  Loss on train set:  -52.39057745413546\n",
      "Iter:  3451  Loss on train set:  -52.372669730491005\n",
      "Iter:  3452  Loss on train set:  -52.38125864075993\n",
      "Iter:  3453  Loss on train set:  -52.39197744226826\n",
      "Iter:  3454  Loss on train set:  -52.38836260887769\n",
      "Iter:  3455  Loss on train set:  -52.38718054595476\n",
      "Iter:  3456  Loss on train set:  -52.39233543776525\n",
      "Iter:  3457  Loss on train set:  -52.39332066395492\n",
      "Iter:  3458  Loss on train set:  -52.391555596795385\n",
      "Iter:  3459  Loss on train set:  -52.395952595043454\n",
      "Iter:  3460  Loss on train set:  -52.39684721483067\n",
      "Iter:  3461  Loss on train set:  -52.39685183050438\n",
      "Iter:  3462  Loss on train set:  -52.39043491073986\n",
      "Iter:  3463  Loss on train set:  -52.39684549181413\n",
      "Iter:  3464  Loss on train set:  -52.397467000331936\n",
      "Iter:  3465  Loss on train set:  -52.397474887698856\n",
      "Iter:  3466  Loss on train set:  -52.392017069207185\n",
      "Iter:  3467  Loss on train set:  -52.39594002048215\n",
      "Iter:  3468  Loss on train set:  -52.397474887698856\n",
      "     fun: -52.397474887698856\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 4.43695702,  7.01860344,  0.0093482 , -0.28838197, -0.29437515,\n",
      "        0.02142383,  3.8391141 , -1.02482189,  2.29160004])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3469  Loss on train set:  -42.643535422941206\n",
      "Iter:  3470  Loss on train set:  -52.01475193511551\n",
      "Iter:  3471  Loss on train set:  -50.166732160712314\n",
      "Iter:  3472  Loss on train set:  -45.98556104630393\n",
      "Iter:  3473  Loss on train set:  -42.911543261693694\n",
      "Iter:  3474  Loss on train set:  -49.76173181360416\n",
      "Iter:  3475  Loss on train set:  -42.851569871285655\n",
      "Iter:  3476  Loss on train set:  -52.397472724775184\n",
      "Iter:  3477  Loss on train set:  -48.376739431448925\n",
      "Iter:  3478  Loss on train set:  -39.0312823277533\n",
      "Iter:  3479  Loss on train set:  -48.740349119978646\n",
      "Iter:  3480  Loss on train set:  -51.65421257836665\n",
      "Iter:  3481  Loss on train set:  -52.38195357087255\n",
      "Iter:  3482  Loss on train set:  -52.258971823331294\n",
      "Iter:  3483  Loss on train set:  -52.261044338602254\n",
      "Iter:  3484  Loss on train set:  -52.186216219096956\n",
      "Iter:  3485  Loss on train set:  -52.34145911046811\n",
      "Iter:  3486  Loss on train set:  -51.54415643280496\n",
      "Iter:  3487  Loss on train set:  -52.39747009185056\n",
      "Iter:  3488  Loss on train set:  -52.0689337023481\n",
      "Iter:  3489  Loss on train set:  -52.221132696078044\n",
      "Iter:  3490  Loss on train set:  -52.34038996307452\n",
      "Iter:  3491  Loss on train set:  -52.39361221281142\n",
      "Iter:  3492  Loss on train set:  -52.37658471395995\n",
      "Iter:  3493  Loss on train set:  -52.39522445667404\n",
      "Iter:  3494  Loss on train set:  -52.33959033987887\n",
      "Iter:  3495  Loss on train set:  -52.39148523176161\n",
      "Iter:  3496  Loss on train set:  -52.3974717059256\n",
      "Iter:  3497  Loss on train set:  -52.38966997404556\n",
      "Iter:  3498  Loss on train set:  -52.39808488474039\n",
      "Iter:  3499  Loss on train set:  -52.392775051307574\n",
      "Iter:  3500  Loss on train set:  -52.39598917821373\n",
      "Iter:  3501  Loss on train set:  -52.39639578505207\n",
      "Iter:  3502  Loss on train set:  -52.39090875067853\n",
      "Iter:  3503  Loss on train set:  -52.39840196954444\n",
      "Iter:  3504  Loss on train set:  -52.39819502935538\n",
      "Iter:  3505  Loss on train set:  -52.398391095350526\n",
      "Iter:  3506  Loss on train set:  -52.39848500826261\n",
      "Iter:  3507  Loss on train set:  -52.39885124726029\n",
      "Iter:  3508  Loss on train set:  -52.39785303521819\n",
      "Iter:  3509  Loss on train set:  -52.39892962581254\n",
      "Iter:  3510  Loss on train set:  -52.39650497384496\n",
      "Iter:  3511  Loss on train set:  -52.39849000906628\n",
      "Iter:  3512  Loss on train set:  -52.39782045232392\n",
      "Iter:  3513  Loss on train set:  -52.399618365460135\n",
      "Iter:  3514  Loss on train set:  -52.39975325422007\n",
      "Iter:  3515  Loss on train set:  -52.39962983988635\n",
      "Iter:  3516  Loss on train set:  -52.399421141989414\n",
      "Iter:  3517  Loss on train set:  -52.400449756227225\n",
      "Iter:  3518  Loss on train set:  -52.40042794311963\n",
      "Iter:  3519  Loss on train set:  -52.400449756227225\n",
      "     fun: -52.400449756227225\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.01139874, -0.07017771, -0.10166069, -0.0060626 , -0.73388269,\n",
      "       -0.00367272,  0.14349954,  2.28671245,  0.07759463])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3520  Loss on train set:  -50.88410684132998\n",
      "Iter:  3521  Loss on train set:  -40.09883696856634\n",
      "Iter:  3522  Loss on train set:  -42.69510796917735\n",
      "Iter:  3523  Loss on train set:  -43.33790903116205\n",
      "Iter:  3524  Loss on train set:  -47.90328425762148\n",
      "Iter:  3525  Loss on train set:  -44.777090405139155\n",
      "Iter:  3526  Loss on train set:  -52.00680527281643\n",
      "Iter:  3527  Loss on train set:  -42.69090457804056\n",
      "Iter:  3528  Loss on train set:  -46.47819149235644\n",
      "Iter:  3529  Loss on train set:  -39.31576781461435\n",
      "Iter:  3530  Loss on train set:  -48.698602231644564\n",
      "Iter:  3531  Loss on train set:  -51.61246103681278\n",
      "Iter:  3532  Loss on train set:  -52.38079467483209\n",
      "Iter:  3533  Loss on train set:  -51.99438016867847\n",
      "Iter:  3534  Loss on train set:  -52.333663310204216\n",
      "Iter:  3535  Loss on train set:  -51.742245418331336\n",
      "Iter:  3536  Loss on train set:  -52.38513990931513\n",
      "Iter:  3537  Loss on train set:  -51.90894106669574\n",
      "Iter:  3538  Loss on train set:  -52.30464627879658\n",
      "Iter:  3539  Loss on train set:  -51.623981567066174\n",
      "Iter:  3540  Loss on train set:  -52.16076455036389\n",
      "Iter:  3541  Loss on train set:  -52.353432721181186\n",
      "Iter:  3542  Loss on train set:  -52.395434083862234\n",
      "Iter:  3543  Loss on train set:  -52.35365166722389\n",
      "Iter:  3544  Loss on train set:  -52.38782938561019\n",
      "Iter:  3545  Loss on train set:  -52.378032835113345\n",
      "Iter:  3546  Loss on train set:  -52.38210823146531\n",
      "Iter:  3547  Loss on train set:  -52.39842946383801\n",
      "Iter:  3548  Loss on train set:  -52.39534131479413\n",
      "Iter:  3549  Loss on train set:  -52.401277825044374\n",
      "Iter:  3550  Loss on train set:  -52.39649380080082\n",
      "Iter:  3551  Loss on train set:  -52.40236073621879\n",
      "Iter:  3552  Loss on train set:  -52.39577654385269\n",
      "Iter:  3553  Loss on train set:  -52.397749563812596\n",
      "Iter:  3554  Loss on train set:  -52.38900380952411\n",
      "Iter:  3555  Loss on train set:  -52.40091390029163\n",
      "Iter:  3556  Loss on train set:  -52.40117316709595\n",
      "Iter:  3557  Loss on train set:  -52.40232868082023\n",
      "Iter:  3558  Loss on train set:  -52.4015843963719\n",
      "Iter:  3559  Loss on train set:  -52.40068890978704\n",
      "Iter:  3560  Loss on train set:  -52.40221301481074\n",
      "Iter:  3561  Loss on train set:  -52.40259768735443\n",
      "Iter:  3562  Loss on train set:  -52.40261298565251\n",
      "Iter:  3563  Loss on train set:  -52.40241676094665\n",
      "Iter:  3564  Loss on train set:  -52.402235942155265\n",
      "Iter:  3565  Loss on train set:  -52.40322947326973\n",
      "Iter:  3566  Loss on train set:  -52.40339040250018\n",
      "Iter:  3567  Loss on train set:  -52.403283205720136\n",
      "Iter:  3568  Loss on train set:  -52.40345221919953\n",
      "Iter:  3569  Loss on train set:  -52.40324212666157\n",
      "Iter:  3570  Loss on train set:  -52.40345221919953\n",
      "     fun: -52.40345221919953\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.0268233 , -0.17557994, -0.80460498, -0.01926676,  0.05193464,\n",
      "        0.273237  , -0.05590116,  0.13909981,  0.27975895])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3571  Loss on train set:  -42.6764990979396\n",
      "Iter:  3572  Loss on train set:  -43.823317768647655\n",
      "Iter:  3573  Loss on train set:  -41.12881992129562\n",
      "Iter:  3574  Loss on train set:  -40.11419965918236\n",
      "Iter:  3575  Loss on train set:  -49.700095639909414\n",
      "Iter:  3576  Loss on train set:  -43.45076391005316\n",
      "Iter:  3577  Loss on train set:  -52.40344933665163\n",
      "Iter:  3578  Loss on train set:  -52.40344992495726\n",
      "Iter:  3579  Loss on train set:  -52.40345356737961\n",
      "Iter:  3580  Loss on train set:  -46.85781184256529\n",
      "Iter:  3581  Loss on train set:  -49.368160593831135\n",
      "Iter:  3582  Loss on train set:  -51.596080141465734\n",
      "Iter:  3583  Loss on train set:  -50.06382390183815\n",
      "Iter:  3584  Loss on train set:  -52.19176180569904\n",
      "Iter:  3585  Loss on train set:  -48.521599220467664\n",
      "Iter:  3586  Loss on train set:  -52.403451720020314\n",
      "Iter:  3587  Loss on train set:  -49.96981020390989\n",
      "Iter:  3588  Loss on train set:  -52.40344299786783\n",
      "Iter:  3589  Loss on train set:  -50.90897977728063\n",
      "Iter:  3590  Loss on train set:  -51.49849315019065\n",
      "Iter:  3591  Loss on train set:  -52.40344827997726\n",
      "Iter:  3592  Loss on train set:  -51.50415552963665\n",
      "Iter:  3593  Loss on train set:  -52.30604800320071\n",
      "Iter:  3594  Loss on train set:  -52.348377762945326\n",
      "Iter:  3595  Loss on train set:  -52.239306021544124\n",
      "Iter:  3596  Loss on train set:  -52.36771617221132\n",
      "Iter:  3597  Loss on train set:  -52.40345201944056\n",
      "Iter:  3598  Loss on train set:  -52.32778926120223\n",
      "Iter:  3599  Loss on train set:  -52.40345469142153\n",
      "Iter:  3600  Loss on train set:  -52.38987763378765\n",
      "Iter:  3601  Loss on train set:  -52.35922339826054\n",
      "Iter:  3602  Loss on train set:  -52.398019183896125\n",
      "Iter:  3603  Loss on train set:  -52.40344281433035\n",
      "Iter:  3604  Loss on train set:  -52.40287480594944\n",
      "Iter:  3605  Loss on train set:  -52.406330895504894\n",
      "Iter:  3606  Loss on train set:  -52.393563017554946\n",
      "Iter:  3607  Loss on train set:  -52.395543957855416\n",
      "Iter:  3608  Loss on train set:  -52.402902334116675\n",
      "Iter:  3609  Loss on train set:  -52.406334137929164\n",
      "Iter:  3610  Loss on train set:  -52.40503637668191\n",
      "Iter:  3611  Loss on train set:  -52.402031246280615\n",
      "Iter:  3612  Loss on train set:  -52.40905254410802\n",
      "Iter:  3613  Loss on train set:  -52.409065401845496\n",
      "Iter:  3614  Loss on train set:  -52.4120014630719\n",
      "Iter:  3615  Loss on train set:  -52.41181285998435\n",
      "Iter:  3616  Loss on train set:  -52.41200136913014\n",
      "Iter:  3617  Loss on train set:  -52.41113992482658\n",
      "Iter:  3618  Loss on train set:  -52.411600643775145\n",
      "Iter:  3619  Loss on train set:  -52.411548399873325\n",
      "Iter:  3620  Loss on train set:  -52.411024454634884\n",
      "Iter:  3621  Loss on train set:  -52.4120014630719\n",
      "     fun: -52.4120014630719\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 1.21840106e-01, -4.10984267e-01,  5.06684732e-01, -2.91400080e-01,\n",
      "       -7.74160493e-04, -1.55653530e+00,  2.27888959e+00,  4.47604772e+00,\n",
      "        8.01862206e+00])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3622  Loss on train set:  -47.66752366256842\n",
      "Iter:  3623  Loss on train set:  -46.50200608199668\n",
      "Iter:  3624  Loss on train set:  -37.8018990895612\n",
      "Iter:  3625  Loss on train set:  -47.51929471230967\n",
      "Iter:  3626  Loss on train set:  -47.68483587430065\n",
      "Iter:  3627  Loss on train set:  -50.09530013532589\n",
      "Iter:  3628  Loss on train set:  -45.8358246462121\n",
      "Iter:  3629  Loss on train set:  -52.4119985048801\n",
      "Iter:  3630  Loss on train set:  -52.4119930073285\n",
      "Iter:  3631  Loss on train set:  -41.49522185380484\n",
      "Iter:  3632  Loss on train set:  -49.505891949098654\n",
      "Iter:  3633  Loss on train set:  -52.020670289021886\n",
      "Iter:  3634  Loss on train set:  -52.311526439060344\n",
      "Iter:  3635  Loss on train set:  -51.65441154827773\n",
      "Iter:  3636  Loss on train set:  -52.40635358668356\n",
      "Iter:  3637  Loss on train set:  -51.836598783289276\n",
      "Iter:  3638  Loss on train set:  -52.38072251479657\n",
      "Iter:  3639  Loss on train set:  -51.922579263729865\n",
      "Iter:  3640  Loss on train set:  -52.411998238353675\n",
      "Iter:  3641  Loss on train set:  -51.67956521772936\n",
      "Iter:  3642  Loss on train set:  -52.41198954011673\n",
      "Iter:  3643  Loss on train set:  -51.67956769556096\n",
      "Iter:  3644  Loss on train set:  -52.22205736103304\n",
      "Iter:  3645  Loss on train set:  -52.391259793915744\n",
      "Iter:  3646  Loss on train set:  -52.3974455377743\n",
      "Iter:  3647  Loss on train set:  -52.350688499305505\n",
      "Iter:  3648  Loss on train set:  -52.385823044462825\n",
      "Iter:  3649  Loss on train set:  -52.407111702990456\n",
      "Iter:  3650  Loss on train set:  -52.41250959352224\n",
      "Iter:  3651  Loss on train set:  -52.413212902084126\n",
      "Iter:  3652  Loss on train set:  -52.41321648213943\n",
      "Iter:  3653  Loss on train set:  -52.40368803958263\n",
      "Iter:  3654  Loss on train set:  -52.41320965448712\n",
      "Iter:  3655  Loss on train set:  -52.397119162174505\n",
      "Iter:  3656  Loss on train set:  -52.41034727367839\n",
      "Iter:  3657  Loss on train set:  -52.413477825420784\n",
      "Iter:  3658  Loss on train set:  -52.40950147333493\n",
      "Iter:  3659  Loss on train set:  -52.41287972258436\n",
      "Iter:  3660  Loss on train set:  -52.41162795438369\n",
      "Iter:  3661  Loss on train set:  -52.413469170570764\n",
      "Iter:  3662  Loss on train set:  -52.41368950633214\n",
      "Iter:  3663  Loss on train set:  -52.41236883389627\n",
      "Iter:  3664  Loss on train set:  -52.413522613049075\n",
      "Iter:  3665  Loss on train set:  -52.413678458167155\n",
      "Iter:  3666  Loss on train set:  -52.41157701509304\n",
      "Iter:  3667  Loss on train set:  -52.41369146513934\n",
      "Iter:  3668  Loss on train set:  -52.414421066128575\n",
      "Iter:  3669  Loss on train set:  -52.41279495809907\n",
      "Iter:  3670  Loss on train set:  -52.414824457053555\n",
      "Iter:  3671  Loss on train set:  -52.41442173452244\n",
      "Iter:  3672  Loss on train set:  -52.414824457053555\n",
      "     fun: -52.414824457053555\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-3.01823486e-02,  3.62443424e-03,  7.85856468e-03, -9.29568339e-02,\n",
      "       -6.05543795e-03, -8.81973087e-02, -9.46348469e-03,  2.27892439e+00,\n",
      "        8.99469068e+00])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3673  Loss on train set:  -47.561211825056276\n",
      "Iter:  3674  Loss on train set:  -42.714185173101065\n",
      "Iter:  3675  Loss on train set:  -51.87833553436475\n",
      "Iter:  3676  Loss on train set:  -48.50790130098552\n",
      "Iter:  3677  Loss on train set:  -41.086160254812185\n",
      "Iter:  3678  Loss on train set:  -52.41483111800974\n",
      "Iter:  3679  Loss on train set:  -51.988943653750574\n",
      "Iter:  3680  Loss on train set:  -41.085239651633486\n",
      "Iter:  3681  Loss on train set:  -52.4148256095542\n",
      "Iter:  3682  Loss on train set:  -42.49887165765399\n",
      "Iter:  3683  Loss on train set:  -49.68256207688988\n",
      "Iter:  3684  Loss on train set:  -52.37103144456415\n",
      "Iter:  3685  Loss on train set:  -51.16564369217973\n",
      "Iter:  3686  Loss on train set:  -52.09483617071703\n",
      "Iter:  3687  Loss on train set:  -49.257290198960504\n",
      "Iter:  3688  Loss on train set:  -51.625917551901935\n",
      "Iter:  3689  Loss on train set:  -52.414818853984094\n",
      "Iter:  3690  Loss on train set:  -51.8290090468131\n",
      "Iter:  3691  Loss on train set:  -52.402486102380664\n",
      "Iter:  3692  Loss on train set:  -52.10852271139134\n",
      "Iter:  3693  Loss on train set:  -52.41481671926617\n",
      "Iter:  3694  Loss on train set:  -51.65748852499664\n",
      "Iter:  3695  Loss on train set:  -52.2346743630918\n",
      "Iter:  3696  Loss on train set:  -52.37770030056357\n",
      "Iter:  3697  Loss on train set:  -52.41453075329008\n",
      "Iter:  3698  Loss on train set:  -52.384955859214784\n",
      "Iter:  3699  Loss on train set:  -52.409019756895425\n",
      "Iter:  3700  Loss on train set:  -52.39942275029895\n",
      "Iter:  3701  Loss on train set:  -52.36117880792256\n",
      "Iter:  3702  Loss on train set:  -52.39969963131949\n",
      "Iter:  3703  Loss on train set:  -52.41481388100415\n",
      "Iter:  3704  Loss on train set:  -52.39756011899192\n",
      "Iter:  3705  Loss on train set:  -52.415607010738015\n",
      "Iter:  3706  Loss on train set:  -52.411688526629\n",
      "Iter:  3707  Loss on train set:  -52.41559779616871\n",
      "Iter:  3708  Loss on train set:  -52.41056859466544\n",
      "Iter:  3709  Loss on train set:  -52.41592579154217\n",
      "Iter:  3710  Loss on train set:  -52.408081887300305\n",
      "Iter:  3711  Loss on train set:  -52.41303718007226\n",
      "Iter:  3712  Loss on train set:  -52.41596741613051\n",
      "Iter:  3713  Loss on train set:  -52.41252464875012\n",
      "Iter:  3714  Loss on train set:  -52.415251764686516\n",
      "Iter:  3715  Loss on train set:  -52.41471059655184\n",
      "Iter:  3716  Loss on train set:  -52.41633467347564\n",
      "Iter:  3717  Loss on train set:  -52.41555932478975\n",
      "Iter:  3718  Loss on train set:  -52.41645715021056\n",
      "Iter:  3719  Loss on train set:  -52.41557332158817\n",
      "Iter:  3720  Loss on train set:  -52.41645519154533\n",
      "Iter:  3721  Loss on train set:  -52.416268492143274\n",
      "Iter:  3722  Loss on train set:  -52.41620782790657\n",
      "Iter:  3723  Loss on train set:  -52.41645715021056\n",
      "     fun: -52.41645715021056\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-8.89869158e-02,  2.94598340e-03,  2.50322949e-02, -5.07800244e-02,\n",
      "       -7.96700752e-01,  5.48901300e+00, -4.75496803e-02, -2.96393696e-01,\n",
      "        5.76924203e+00])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3724  Loss on train set:  -42.750319503147544\n",
      "Iter:  3725  Loss on train set:  -42.0834495542827\n",
      "Iter:  3726  Loss on train set:  -46.5287664399943\n",
      "Iter:  3727  Loss on train set:  -40.05573044930426\n",
      "Iter:  3728  Loss on train set:  -52.41645742305918\n",
      "Iter:  3729  Loss on train set:  -52.416460442307\n",
      "Iter:  3730  Loss on train set:  -49.78316132022286\n",
      "Iter:  3731  Loss on train set:  -46.211689611642775\n",
      "Iter:  3732  Loss on train set:  -38.776504849546484\n",
      "Iter:  3733  Loss on train set:  -41.84186362622036\n",
      "Iter:  3734  Loss on train set:  -49.47976578761035\n",
      "Iter:  3735  Loss on train set:  -51.94662642846729\n",
      "Iter:  3736  Loss on train set:  -50.2650050437981\n",
      "Iter:  3737  Loss on train set:  -52.41646156435988\n",
      "Iter:  3738  Loss on train set:  -48.72052380713843\n",
      "Iter:  3739  Loss on train set:  -52.2241766730886\n",
      "Iter:  3740  Loss on train set:  -51.4752829450008\n",
      "Iter:  3741  Loss on train set:  -52.03011817835824\n",
      "Iter:  3742  Loss on train set:  -52.41093683539928\n",
      "Iter:  3743  Loss on train set:  -51.50233619954792\n",
      "Iter:  3744  Loss on train set:  -52.28055169672284\n",
      "Iter:  3745  Loss on train set:  -51.80116502157231\n",
      "Iter:  3746  Loss on train set:  -51.58802843529744\n",
      "Iter:  3747  Loss on train set:  -52.360174805328526\n",
      "Iter:  3748  Loss on train set:  -52.39791510517263\n",
      "Iter:  3749  Loss on train set:  -52.119077305025826\n",
      "Iter:  3750  Loss on train set:  -52.37871916855466\n",
      "Iter:  3751  Loss on train set:  -52.41450174840773\n",
      "Iter:  3752  Loss on train set:  -52.38088863304034\n",
      "Iter:  3753  Loss on train set:  -52.40776681765669\n",
      "Iter:  3754  Loss on train set:  -52.3771139471455\n",
      "Iter:  3755  Loss on train set:  -52.39680660315508\n",
      "Iter:  3756  Loss on train set:  -52.414022120741514\n",
      "Iter:  3757  Loss on train set:  -52.40773360926807\n",
      "Iter:  3758  Loss on train set:  -52.41669813864412\n",
      "Iter:  3759  Loss on train set:  -52.407784390273825\n",
      "Iter:  3760  Loss on train set:  -52.41455167122423\n",
      "Iter:  3761  Loss on train set:  -52.407909897366636\n",
      "Iter:  3762  Loss on train set:  -52.41781215616389\n",
      "Iter:  3763  Loss on train set:  -52.417040968838464\n",
      "Iter:  3764  Loss on train set:  -52.413904350454104\n",
      "Iter:  3765  Loss on train set:  -52.41746157990381\n",
      "Iter:  3766  Loss on train set:  -52.41803291734262\n",
      "Iter:  3767  Loss on train set:  -52.418621384837735\n",
      "Iter:  3768  Loss on train set:  -52.416141884907404\n",
      "Iter:  3769  Loss on train set:  -52.417804529237486\n",
      "Iter:  3770  Loss on train set:  -52.418290530629015\n",
      "Iter:  3771  Loss on train set:  -52.41767785742168\n",
      "Iter:  3772  Loss on train set:  -52.41916627834047\n",
      "Iter:  3773  Loss on train set:  -52.41845412722104\n",
      "Iter:  3774  Loss on train set:  -52.41916627834047\n",
      "     fun: -52.41916627834047\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 0.12667047,  0.20276331,  0.26263754, -0.29835394,  4.81211233,\n",
      "        3.27209864, -0.03499479, -0.0087923 , -1.01239173])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3775  Loss on train set:  -52.419167901105425\n",
      "Iter:  3776  Loss on train set:  -40.683638926637094\n",
      "Iter:  3777  Loss on train set:  -42.72779219505874\n",
      "Iter:  3778  Loss on train set:  -50.95538725564607\n",
      "Iter:  3779  Loss on train set:  -47.69602621186541\n",
      "Iter:  3780  Loss on train set:  -40.14708538845993\n",
      "Iter:  3781  Loss on train set:  -52.419173484817\n",
      "Iter:  3782  Loss on train set:  -50.07240658528702\n",
      "Iter:  3783  Loss on train set:  -47.06442724724524\n",
      "Iter:  3784  Loss on train set:  -41.39841315773107\n",
      "Iter:  3785  Loss on train set:  -49.277859244633966\n",
      "Iter:  3786  Loss on train set:  -52.41916778996546\n",
      "Iter:  3787  Loss on train set:  -49.71971953393061\n",
      "Iter:  3788  Loss on train set:  -52.30195720960644\n",
      "Iter:  3789  Loss on train set:  -51.190190483143766\n",
      "Iter:  3790  Loss on train set:  -49.27826332668992\n",
      "Iter:  3791  Loss on train set:  -51.78118691688655\n",
      "Iter:  3792  Loss on train set:  -52.419164071925934\n",
      "Iter:  3793  Loss on train set:  -52.23330979998576\n",
      "Iter:  3794  Loss on train set:  -52.088101169914914\n",
      "Iter:  3795  Loss on train set:  -52.22412193658265\n",
      "Iter:  3796  Loss on train set:  -52.40445274122888\n",
      "Iter:  3797  Loss on train set:  -52.209219007256685\n",
      "Iter:  3798  Loss on train set:  -52.246644482486545\n",
      "Iter:  3799  Loss on train set:  -52.403591190964526\n",
      "Iter:  3800  Loss on train set:  -52.41916983062065\n",
      "Iter:  3801  Loss on train set:  -52.41100282881443\n",
      "Iter:  3802  Loss on train set:  -52.420713240021165\n",
      "Iter:  3803  Loss on train set:  -52.38029459965826\n",
      "Iter:  3804  Loss on train set:  -52.409208119523626\n",
      "Iter:  3805  Loss on train set:  -52.4149157626549\n",
      "Iter:  3806  Loss on train set:  -52.406233777713325\n",
      "Iter:  3807  Loss on train set:  -52.42071568578565\n",
      "Iter:  3808  Loss on train set:  -52.41669229831512\n",
      "Iter:  3809  Loss on train set:  -52.41994036966967\n",
      "Iter:  3810  Loss on train set:  -52.41901938529482\n",
      "Iter:  3811  Loss on train set:  -52.42106423400795\n",
      "Iter:  3812  Loss on train set:  -52.41334859854761\n",
      "Iter:  3813  Loss on train set:  -52.41876546472583\n",
      "Iter:  3814  Loss on train set:  -52.42105963530169\n",
      "Iter:  3815  Loss on train set:  -52.419128598675115\n",
      "Iter:  3816  Loss on train set:  -52.421685321547166\n",
      "Iter:  3817  Loss on train set:  -52.42046077400758\n",
      "Iter:  3818  Loss on train set:  -52.42165385585098\n",
      "Iter:  3819  Loss on train set:  -52.42201895928464\n",
      "Iter:  3820  Loss on train set:  -52.42170465731158\n",
      "Iter:  3821  Loss on train set:  -52.42199262118265\n",
      "Iter:  3822  Loss on train set:  -52.42236020027258\n",
      "Iter:  3823  Loss on train set:  -52.42152583439251\n",
      "Iter:  3824  Loss on train set:  -52.4221624741185\n",
      "Iter:  3825  Loss on train set:  -52.42236020027258\n",
      "     fun: -52.42236020027258\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 6.49843925, -0.81071616, -0.80634418,  0.02306839, -0.02879582,\n",
      "       -0.29795554,  4.26978601, -0.10177472, -0.03269308])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3826  Loss on train set:  -52.42235743499791\n",
      "Iter:  3827  Loss on train set:  -46.21284618992737\n",
      "Iter:  3828  Loss on train set:  -41.262168719442556\n",
      "Iter:  3829  Loss on train set:  -49.68708312715727\n",
      "Iter:  3830  Loss on train set:  -46.293562654181144\n",
      "Iter:  3831  Loss on train set:  -41.21633773740912\n",
      "Iter:  3832  Loss on train set:  -52.42235866129427\n",
      "Iter:  3833  Loss on train set:  -52.42235772780461\n",
      "Iter:  3834  Loss on train set:  -46.16917181001937\n",
      "Iter:  3835  Loss on train set:  -35.97492865376639\n",
      "Iter:  3836  Loss on train set:  -47.962819031405466\n",
      "Iter:  3837  Loss on train set:  -51.95070007605144\n",
      "Iter:  3838  Loss on train set:  -52.422355122183994\n",
      "Iter:  3839  Loss on train set:  -51.95240370449006\n",
      "Iter:  3840  Loss on train set:  -52.3832473922284\n",
      "Iter:  3841  Loss on train set:  -51.965952088650624\n",
      "Iter:  3842  Loss on train set:  -52.422356237280724\n",
      "Iter:  3843  Loss on train set:  -51.769222636394296\n",
      "Iter:  3844  Loss on train set:  -52.42235760579481\n",
      "Iter:  3845  Loss on train set:  -51.960161586781666\n",
      "Iter:  3846  Loss on train set:  -52.15435440915367\n",
      "Iter:  3847  Loss on train set:  -52.40212398546451\n",
      "Iter:  3848  Loss on train set:  -52.409505229803045\n",
      "Iter:  3849  Loss on train set:  -52.403851620728865\n",
      "Iter:  3850  Loss on train set:  -52.39218345589554\n",
      "Iter:  3851  Loss on train set:  -52.39218297537396\n",
      "Iter:  3852  Loss on train set:  -52.42029653049565\n",
      "Iter:  3853  Loss on train set:  -52.422356351588974\n",
      "Iter:  3854  Loss on train set:  -52.42526280613778\n",
      "Iter:  3855  Loss on train set:  -52.42526725822531\n",
      "Iter:  3856  Loss on train set:  -52.42638778878187\n",
      "Iter:  3857  Loss on train set:  -52.42639322569449\n",
      "Iter:  3858  Loss on train set:  -52.41916754182403\n",
      "Iter:  3859  Loss on train set:  -52.425366043634206\n",
      "Iter:  3860  Loss on train set:  -52.40954344640679\n",
      "Iter:  3861  Loss on train set:  -52.42877150675439\n",
      "Iter:  3862  Loss on train set:  -52.42407018108132\n",
      "Iter:  3863  Loss on train set:  -52.427368117583214\n",
      "Iter:  3864  Loss on train set:  -52.430646335156396\n",
      "Iter:  3865  Loss on train set:  -52.430508176886555\n",
      "Iter:  3866  Loss on train set:  -52.43065576971062\n",
      "Iter:  3867  Loss on train set:  -52.4318068678073\n",
      "Iter:  3868  Loss on train set:  -52.43180549085697\n",
      "Iter:  3869  Loss on train set:  -52.430815171518276\n",
      "Iter:  3870  Loss on train set:  -52.42763558859166\n",
      "Iter:  3871  Loss on train set:  -52.43490442611307\n",
      "Iter:  3872  Loss on train set:  -52.43447703399687\n",
      "Iter:  3873  Loss on train set:  -52.43425707084171\n",
      "Iter:  3874  Loss on train set:  -52.43330612914752\n",
      "Iter:  3875  Loss on train set:  -52.43407145437552\n",
      "Iter:  3876  Loss on train set:  -52.43490442611307\n",
      "     fun: -52.43490442611307\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 8.98686364e+00, -5.45828268e-03, -3.05705112e-01,  7.33999234e-03,\n",
      "       -6.07585573e-01,  5.40933391e-01,  6.51408140e+00,  6.00551543e+00,\n",
      "       -1.05664587e-02])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3877  Loss on train set:  -48.45609785955844\n",
      "Iter:  3878  Loss on train set:  -40.12518786379275\n",
      "Iter:  3879  Loss on train set:  -46.50637881962689\n",
      "Iter:  3880  Loss on train set:  -52.434902692397145\n",
      "Iter:  3881  Loss on train set:  -45.38811414520407\n",
      "Iter:  3882  Loss on train set:  -50.692242129959965\n",
      "Iter:  3883  Loss on train set:  -52.434910572343\n",
      "Iter:  3884  Loss on train set:  -43.93898230831335\n",
      "Iter:  3885  Loss on train set:  -47.74302137449711\n",
      "Iter:  3886  Loss on train set:  -44.881080573484844\n",
      "Iter:  3887  Loss on train set:  -49.51098843908119\n",
      "Iter:  3888  Loss on train set:  -52.146765035419655\n",
      "Iter:  3889  Loss on train set:  -50.8234652484629\n",
      "Iter:  3890  Loss on train set:  -52.43491173454207\n",
      "Iter:  3891  Loss on train set:  -50.573992399928265\n",
      "Iter:  3892  Loss on train set:  -52.32576318566198\n",
      "Iter:  3893  Loss on train set:  -51.83567495625061\n",
      "Iter:  3894  Loss on train set:  -51.93156882527911\n",
      "Iter:  3895  Loss on train set:  -52.42844747305481\n",
      "Iter:  3896  Loss on train set:  -51.86440584572317\n",
      "Iter:  3897  Loss on train set:  -52.299594817006316\n",
      "Iter:  3898  Loss on train set:  -52.02987897370947\n",
      "Iter:  3899  Loss on train set:  -51.98876151938716\n",
      "Iter:  3900  Loss on train set:  -52.4056581878033\n",
      "Iter:  3901  Loss on train set:  -52.41467519702903\n",
      "Iter:  3902  Loss on train set:  -52.2887281713989\n",
      "Iter:  3903  Loss on train set:  -52.41459920827184\n",
      "Iter:  3904  Loss on train set:  -52.43393736924874\n",
      "Iter:  3905  Loss on train set:  -52.42232533020889\n",
      "Iter:  3906  Loss on train set:  -52.427852500390564\n",
      "Iter:  3907  Loss on train set:  -52.408780997937185\n",
      "Iter:  3908  Loss on train set:  -52.4285289085743\n",
      "Iter:  3909  Loss on train set:  -52.435167873972766\n",
      "Iter:  3910  Loss on train set:  -52.42202198329697\n",
      "Iter:  3911  Loss on train set:  -52.43513808757177\n",
      "Iter:  3912  Loss on train set:  -52.43750006175612\n",
      "Iter:  3913  Loss on train set:  -52.43703264643468\n",
      "Iter:  3914  Loss on train set:  -52.43848679769214\n",
      "Iter:  3915  Loss on train set:  -52.43786972208954\n",
      "Iter:  3916  Loss on train set:  -52.439505265161806\n",
      "Iter:  3917  Loss on train set:  -52.43946757989993\n",
      "Iter:  3918  Loss on train set:  -52.43799410236999\n",
      "Iter:  3919  Loss on train set:  -52.43896989831228\n",
      "Iter:  3920  Loss on train set:  -52.43971504622572\n",
      "Iter:  3921  Loss on train set:  -52.43991815728966\n",
      "Iter:  3922  Loss on train set:  -52.43959451633445\n",
      "Iter:  3923  Loss on train set:  -52.43926849746152\n",
      "Iter:  3924  Loss on train set:  -52.43980843540315\n",
      "Iter:  3925  Loss on train set:  -52.43832870843164\n",
      "Iter:  3926  Loss on train set:  -52.440504575161846\n",
      "Iter:  3927  Loss on train set:  -52.43991815728966\n",
      "     fun: -52.43991815728966\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-4.67504491e-02,  2.96265751e+00, -6.19048076e-01,  7.74928542e+00,\n",
      "       -2.68693780e-01, -3.15459863e-02,  7.04294529e+00, -4.19845599e-01,\n",
      "        5.60193597e-03])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3928  Loss on train set:  -52.439913947358534\n",
      "Iter:  3929  Loss on train set:  -49.683337601446\n",
      "Iter:  3930  Loss on train set:  -41.31988420806464\n",
      "Iter:  3931  Loss on train set:  -50.6296111198937\n",
      "Iter:  3932  Loss on train set:  -47.66018041838137\n",
      "Iter:  3933  Loss on train set:  -52.439915401497906\n",
      "Iter:  3934  Loss on train set:  -52.43991676202156\n",
      "Iter:  3935  Loss on train set:  -40.04006709878055\n",
      "Iter:  3936  Loss on train set:  -52.43991588551357\n",
      "Iter:  3937  Loss on train set:  -40.388891033911484\n",
      "Iter:  3938  Loss on train set:  -49.20382087702217\n",
      "Iter:  3939  Loss on train set:  -51.83475133780131\n",
      "Iter:  3940  Loss on train set:  -52.439915893616565\n",
      "Iter:  3941  Loss on train set:  -52.24472373576927\n",
      "Iter:  3942  Loss on train set:  -52.405470308531655\n",
      "Iter:  3943  Loss on train set:  -52.074913552694824\n",
      "Iter:  3944  Loss on train set:  -52.43991279358243\n",
      "Iter:  3945  Loss on train set:  -51.55190860309216\n",
      "Iter:  3946  Loss on train set:  -52.439918514710456\n",
      "Iter:  3947  Loss on train set:  -51.66485410927751\n",
      "Iter:  3948  Loss on train set:  -52.43991542698495\n",
      "Iter:  3949  Loss on train set:  -51.88556236360448\n",
      "Iter:  3950  Loss on train set:  -52.41764147974559\n",
      "Iter:  3951  Loss on train set:  -52.30188325790419\n",
      "Iter:  3952  Loss on train set:  -52.210723821901176\n",
      "Iter:  3953  Loss on train set:  -52.356494553002975\n",
      "Iter:  3954  Loss on train set:  -52.39197120146694\n",
      "Iter:  3955  Loss on train set:  -52.436736365233216\n",
      "Iter:  3956  Loss on train set:  -52.415665584504936\n",
      "Iter:  3957  Loss on train set:  -52.439259717165356\n",
      "Iter:  3958  Loss on train set:  -52.37647047590059\n",
      "Iter:  3959  Loss on train set:  -52.439911460001866\n",
      "Iter:  3960  Loss on train set:  -52.4344334623147\n",
      "Iter:  3961  Loss on train set:  -52.435123887053244\n",
      "Iter:  3962  Loss on train set:  -52.43991985141548\n",
      "Iter:  3963  Loss on train set:  -52.43879227132698\n",
      "Iter:  3964  Loss on train set:  -52.439715494166315\n",
      "Iter:  3965  Loss on train set:  -52.439485282184926\n",
      "Iter:  3966  Loss on train set:  -52.43778268309166\n",
      "Iter:  3967  Loss on train set:  -52.44050672640072\n",
      "Iter:  3968  Loss on train set:  -52.43827735365977\n",
      "Iter:  3969  Loss on train set:  -52.44055651220397\n",
      "Iter:  3970  Loss on train set:  -52.43868038503614\n",
      "Iter:  3971  Loss on train set:  -52.44055722272121\n",
      "Iter:  3972  Loss on train set:  -52.439939232968506\n",
      "Iter:  3973  Loss on train set:  -52.44050803100869\n",
      "Iter:  3974  Loss on train set:  -52.43961140927981\n",
      "Iter:  3975  Loss on train set:  -52.44057873118373\n",
      "Iter:  3976  Loss on train set:  -52.438935043189154\n",
      "Iter:  3977  Loss on train set:  -52.44014414257627\n",
      "Iter:  3978  Loss on train set:  -52.44057873118373\n",
      "     fun: -52.44057873118373\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 4.27489713e+00, -3.51866448e-02, -2.92594001e-01,  3.16936825e+00,\n",
      "        5.14429640e-04,  5.76378904e+00,  4.95556657e+00, -3.05132121e-01,\n",
      "        9.00584017e+00])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  3979  Loss on train set:  -52.440580785304206\n",
      "Iter:  3980  Loss on train set:  -42.560649017401055\n",
      "Iter:  3981  Loss on train set:  -47.67027013384785\n",
      "Iter:  3982  Loss on train set:  -39.66088203020692\n",
      "Iter:  3983  Loss on train set:  -43.55753740013025\n",
      "Iter:  3984  Loss on train set:  -48.43232535938608\n",
      "Iter:  3985  Loss on train set:  -40.166991932884635\n",
      "Iter:  3986  Loss on train set:  -40.578096656537014\n",
      "Iter:  3987  Loss on train set:  -41.271009467310435\n",
      "Iter:  3988  Loss on train set:  -40.62577678806234\n",
      "Iter:  3989  Loss on train set:  -49.00068514023417\n",
      "Iter:  3990  Loss on train set:  -51.75086638737793\n",
      "Iter:  3991  Loss on train set:  -52.440572920032295\n",
      "Iter:  3992  Loss on train set:  -52.08519292369413\n",
      "Iter:  3993  Loss on train set:  -52.235589874899965\n",
      "Iter:  3994  Loss on train set:  -51.60602360531109\n",
      "Iter:  3995  Loss on train set:  -52.36378058772272\n",
      "Iter:  3996  Loss on train set:  -51.688995902163235\n",
      "Iter:  3997  Loss on train set:  -52.33503536479266\n",
      "Iter:  3998  Loss on train set:  -51.30748792564137\n",
      "Iter:  3999  Loss on train set:  -52.2744069924091\n",
      "Iter:  4000  Loss on train set:  -52.37671216531844\n",
      "Iter:  4001  Loss on train set:  -52.43504459311802\n",
      "Iter:  4002  Loss on train set:  -52.40561547757472\n",
      "Iter:  4003  Loss on train set:  -52.43443991047723\n",
      "Iter:  4004  Loss on train set:  -52.39699857766982\n",
      "Iter:  4005  Loss on train set:  -52.42009115118719\n",
      "Iter:  4006  Loss on train set:  -52.44058046104901\n",
      "Iter:  4007  Loss on train set:  -52.43638230548666\n",
      "Iter:  4008  Loss on train set:  -52.43556410442946\n",
      "Iter:  4009  Loss on train set:  -52.433599919240876\n",
      "Iter:  4010  Loss on train set:  -52.43840655994835\n",
      "Iter:  4011  Loss on train set:  -52.43976787972012\n",
      "Iter:  4012  Loss on train set:  -52.440206222638096\n",
      "Iter:  4013  Loss on train set:  -52.441590128709024\n",
      "Iter:  4014  Loss on train set:  -52.44047747329592\n",
      "Iter:  4015  Loss on train set:  -52.4413688848678\n",
      "Iter:  4016  Loss on train set:  -52.44155896395489\n",
      "Iter:  4017  Loss on train set:  -52.44240143348342\n",
      "Iter:  4018  Loss on train set:  -52.44148572976223\n",
      "Iter:  4019  Loss on train set:  -52.44352712302769\n",
      "Iter:  4020  Loss on train set:  -52.44301168401698\n",
      "Iter:  4021  Loss on train set:  -52.44315292380332\n",
      "Iter:  4022  Loss on train set:  -52.44417188600997\n",
      "Iter:  4023  Loss on train set:  -52.444079059051056\n",
      "Iter:  4024  Loss on train set:  -52.44402830756016\n",
      "Iter:  4025  Loss on train set:  -52.44378434897947\n",
      "Iter:  4026  Loss on train set:  -52.443939201090316\n",
      "Iter:  4027  Loss on train set:  -52.44418905177081\n",
      "Iter:  4028  Loss on train set:  -52.44415688722632\n",
      "Iter:  4029  Loss on train set:  -52.44418905177081\n",
      "     fun: -52.44418905177081\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 8.75304729,  0.11612144, -0.02564622,  3.82817765, -0.01476039,\n",
      "       -0.04031888, -0.30531526,  0.54099904, -0.291346  ])\n",
      "Unfrozen params:  9  Frozen params:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trothe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:550: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  4030  Loss on train set:  -44.319672902971924\n",
      "Iter:  4031  Loss on train set:  -46.22638852273992\n",
      "Iter:  4032  Loss on train set:  -46.26598037350667\n",
      "Iter:  4033  Loss on train set:  -41.043115878636044\n",
      "Iter:  4034  Loss on train set:  -48.21817494861368\n",
      "Iter:  4035  Loss on train set:  -51.926474918605315\n",
      "Iter:  4036  Loss on train set:  -39.90411612464985\n",
      "Iter:  4037  Loss on train set:  -52.44419093646042\n",
      "Iter:  4038  Loss on train set:  -46.804216344967024\n",
      "Iter:  4039  Loss on train set:  -49.783989149482046\n",
      "Iter:  4040  Loss on train set:  -50.34402079375843\n",
      "Iter:  4041  Loss on train set:  -52.03335882340769\n",
      "Iter:  4042  Loss on train set:  -51.03822817885213\n",
      "Iter:  4043  Loss on train set:  -52.1493940261313\n",
      "Iter:  4044  Loss on train set:  -47.729837436007095\n",
      "Iter:  4045  Loss on train set:  -52.40249859744721\n",
      "Iter:  4046  Loss on train set:  -47.51092936514785\n",
      "Iter:  4047  Loss on train set:  -52.3058764948053\n",
      "Iter:  4048  Loss on train set:  -52.44418488221676\n",
      "Iter:  4049  Loss on train set:  -52.116006060302006\n",
      "Iter:  4050  Loss on train set:  -52.07358993902716\n",
      "Iter:  4051  Loss on train set:  -52.41878162655252\n",
      "Iter:  4052  Loss on train set:  -52.27166771825066\n",
      "Iter:  4053  Loss on train set:  -52.25275124342479\n",
      "Iter:  4054  Loss on train set:  -52.434138838392634\n",
      "Iter:  4055  Loss on train set:  -52.434506080654295\n",
      "Iter:  4056  Loss on train set:  -52.419229147287005\n",
      "Iter:  4057  Loss on train set:  -52.44476427663615\n",
      "Iter:  4058  Loss on train set:  -52.43934921069067\n",
      "Iter:  4059  Loss on train set:  -52.43124459088417\n",
      "Iter:  4060  Loss on train set:  -52.41126424870785\n",
      "Iter:  4061  Loss on train set:  -52.42096535347504\n",
      "Iter:  4062  Loss on train set:  -52.44473197829487\n",
      "Iter:  4063  Loss on train set:  -52.44240745305243\n",
      "Iter:  4064  Loss on train set:  -52.440025181013496\n",
      "Iter:  4065  Loss on train set:  -52.442491444055285\n",
      "Iter:  4066  Loss on train set:  -52.443371542424444\n",
      "Iter:  4067  Loss on train set:  -52.44548040927124\n",
      "Iter:  4068  Loss on train set:  -52.44510327292866\n",
      "Iter:  4069  Loss on train set:  -52.44459314396746\n",
      "Iter:  4070  Loss on train set:  -52.44565533943206\n",
      "Iter:  4071  Loss on train set:  -52.44335555685021\n",
      "Iter:  4072  Loss on train set:  -52.44484368580923\n",
      "Iter:  4073  Loss on train set:  -52.448163427679596\n",
      "Iter:  4074  Loss on train set:  -52.449953974313274\n",
      "Iter:  4075  Loss on train set:  -52.449968304271906\n",
      "Iter:  4076  Loss on train set:  -52.44644083038181\n",
      "Iter:  4077  Loss on train set:  -52.44936801573406\n",
      "Iter:  4078  Loss on train set:  -52.44991566084596\n",
      "Iter:  4079  Loss on train set:  -52.450600407383675\n",
      "Iter:  4080  Loss on train set:  -52.449968304271906\n",
      "     fun: -52.449968304271906\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 50\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 2.48244662e-01, -4.29069434e-03, -5.02526167e-03, -8.18266591e-01,\n",
      "        6.59699976e-02,  5.02559483e-02, -1.93346996e-01,  5.80452091e+00,\n",
      "        2.70118834e-01])\n"
     ]
    }
   ],
   "source": [
    "def dropout_meta_vqe_energy_loss(circuit_params, qubits, frozen_circuit_params,insertion_pointers, inverted_insertion_pointers, hamilt_params_training_set, n_meas_reps=1000):\n",
    "    \"\"\"\n",
    "    Computes the energy loss of a meta-vqe with parameter dropout, i.e. a meta-vqe where the parameters of the circuit are randomly dropped out in each training step.\n",
    "    \n",
    "    Args:\n",
    "    ----------------\n",
    "        circuit_params [np.array]: array of the circuit parameters\n",
    "        qubits [list]: list of qubits\n",
    "        frozen_circuit_params [np.array]: array of the frozen circuit parameters\n",
    "        insertion_pointers [list]: list of the insertion pointers\n",
    "        inverted_insertion_pointers [list]: list of the inverted insertion pointers\n",
    "        hamilt_params_training_set [list]: list of the parameters of the hamiltonians\n",
    "        n_meas_reps [int]: number of measurement repetitions\n",
    "        \n",
    "    Returns:\n",
    "    ----------------\n",
    "        energy_loss [float]: energy loss of the meta-vqe\n",
    "    \"\"\"\n",
    "    \n",
    "    energy_loss = 0\n",
    "    full_circuit_params = np.zeros(len(circuit_params) + len(frozen_circuit_params))\n",
    "    full_circuit_params[insertion_pointers] = circuit_params\n",
    "    full_circuit_params[inverted_insertion_pointers] = frozen_circuit_params\n",
    "    \n",
    "    #start_time = time.time()\n",
    "    for hamilt_params_sample in hamilt_params_training_set:\n",
    "        #energy_loss += meas_hamilt_expectation(qubits, circuit_params, hamilt_params_sample, n_meas_reps)\n",
    "        energy_loss += direct_hamilt_expectation(qubits, full_circuit_params, hamilt_params_sample)\n",
    "    \n",
    "    #print(\"Time taken for the meta_vqe loss evaluation: \", time.time() - start_time)\n",
    "    \n",
    "    return energy_loss\n",
    "\n",
    "def train_dropout_meta_vqe(qubits, hamilt_params_training_set, init_circuit_params=None, params_init_mode=\"normal_random\", n_meas_reps=1000):\n",
    "    \"\"\"\n",
    "    Trains a meta-vqe with parameter dropout, i.e. a meta-vqe where the parameters of the circuit are randomly dropped out in each training step.\n",
    "    \n",
    "    Args:\n",
    "    ----------------\n",
    "        qubits [list]: list of qubits\n",
    "        hamilt_params_training_set [list]: list of the parameters of the hamiltonians\n",
    "        init_circuit_params [np.array]: array of the initial circuit parameters\n",
    "        params_init_mode [str]: mode for initializing the circuit parameters\n",
    "        n_meas_reps [int]: number of measurement repetitions\n",
    "    \n",
    "    Returns:\n",
    "    ----------------\n",
    "        optim_circuit_params [np.array]: array of the optimized circuit parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    if init_circuit_params is None:\n",
    "        init_circuit_params = initialize_circuit_params(qubits, params_init_mode)\n",
    "\n",
    "\n",
    "    print(\"Train loss should converge to \", sum(exact_hamilt_GS_energy(qubits, hamilt_params_training_set[i]) for i in range(len(hamilt_params_training_set))))\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "\n",
    "    meta_vqe_loss_grad_fun = prepare_loss_gradient_approx(dropout_meta_vqe_energy_loss, stochastic_approx=True, stoch_shift_size_c=0.01)\n",
    "\n",
    "    optim_circuit_params = init_circuit_params\n",
    "    dropout_rate = 0.8\n",
    "    \n",
    "    for repetit in range(80):\n",
    "        shuffled_inds = np.random.permutation(len(optim_circuit_params)) #np.random.randint(0, len(optim_circuit_params), size=int((1-dropout_rate)*len(optim_circuit_params)))\n",
    "        insertion_pointers, inverted_insertion_pointers = shuffled_inds[:int((1-dropout_rate)*len(optim_circuit_params))], shuffled_inds[int((1-dropout_rate)*len(optim_circuit_params)):]\n",
    "        trainable_circuit_params = optim_circuit_params[insertion_pointers]\n",
    "        frozen_circuit_params = optim_circuit_params[inverted_insertion_pointers]\n",
    "        \n",
    "        print(\"Unfrozen params: \", len(trainable_circuit_params), \" Frozen params: \", len(frozen_circuit_params))\n",
    "        \n",
    "        def callbackF(Xi):\n",
    "            global Nfeval\n",
    "            print(\"Iter: \", Nfeval, \" Loss on train set: \", dropout_meta_vqe_energy_loss(Xi, qubits, frozen_circuit_params, insertion_pointers, inverted_insertion_pointers, hamilt_params_training_set, n_meas_reps=1000))\n",
    "            Nfeval += 1\n",
    "        \n",
    "        loss_args = (qubits, frozen_circuit_params, insertion_pointers, inverted_insertion_pointers, hamilt_params_training_set, n_meas_reps)\n",
    "        \n",
    "        #meta_vqe_loss_grad_fun = prepare_loss_gradient_approx(lambda x: dropout_meta_vqe_energy_loss(x, *loss_args), stochastic_approx=True, stoch_shift_size_c=0.001)\n",
    "    \n",
    "        optim_result = minimize(dropout_meta_vqe_energy_loss,\n",
    "                                args=loss_args, \n",
    "                                x0=trainable_circuit_params, \n",
    "                                method=\"COBYLA\", #adam, #'L-BFGS-B', #'COBYLA', #adam_with_lr_decay,,#'COBYLA', \n",
    "                                callback=callbackF,\n",
    "                                jac=meta_vqe_loss_grad_fun, #None,\n",
    "                                options={'maxiter': 50},\n",
    "                                )\n",
    "        print(optim_result)\n",
    "        optim_circuit_params[insertion_pointers] = optim_result.x     \n",
    "        \n",
    "    #print(optim_result.fun)\n",
    "    return optim_circuit_params\n",
    "\n",
    "def dropout_meta_vqe(qubits, hamilt_params_training_set, hamilt_params_test_set):\n",
    "    \"\"\"\n",
    "    Convenience function for training and evaluating a dropout meta-vqe, i.e. a meta-vqe where the parameters of the circuit are randomly dropped out in each training step.\n",
    "    \n",
    "    Args:\n",
    "    ----------------\n",
    "        qubits [list]: list of qubits\n",
    "        hamilt_params_training_set [list]: list of the parameters of the hamiltonians in the training set\n",
    "        hamilt_params_test_set [list]: list of the parameters of the hamiltonians in the test set\n",
    "    \n",
    "    Returns:\n",
    "    ----------------\n",
    "        energy_expectations [list]: list of the energy expectations of the hamiltonians in the test set\n",
    "        abs_energy_errors [list]: list of the absolute energy errors of the hamiltonians in the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    #Train meta-VQE:\n",
    "    opt_circuit_params = train_dropout_meta_vqe(qubits, hamilt_params_training_set, params_init_mode=\"normal_random\", n_meas_reps=1000)\n",
    "    \n",
    "    #Evaluate meta-VQE on test set:\n",
    "    #energy_expectations, abs_energy_errors = evaluate_meta_vqe(qubits, opt_circuit_params, hamilt_params_test_set, n_meas_reps=1000)\n",
    "    \n",
    "    ##return energy_expectations, abs_energy_errors\n",
    "\n",
    "\n",
    "qubits = cirq.LineQubit.range(8)\n",
    "\n",
    "Nfeval = 1  \n",
    "\n",
    "train_set_size = 10\n",
    "min_hamilt_params = {\"lambda\": 0.75, \"Delta\": -1.1}\n",
    "max_hamilt_params = {\"lambda\": 0.75, \"Delta\": 1.1}\n",
    "\n",
    "train_set = construct_training_set(train_set_size=train_set_size, mode=\"equidistant\")\n",
    "test_set = construct_test_set(test_set_size=100, mode=\"uniform_random\", min_hamilt_params=min_hamilt_params, max_hamilt_params=max_hamilt_params)\n",
    "\n",
    "dropout_meta_vqe(qubits, train_set, test_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"classical\" VQE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqe_energy_loss(circuit_params,qubits, c_hamilt_params, n_meas_reps=1000):\n",
    "    \"\"\"\n",
    "    Compute the loss function of a VQE\n",
    "    \n",
    "    Args:\n",
    "    ----------------\n",
    "        circuit_params [np.array]: parameters of the circuit\n",
    "        qubits [list]: list of qubits\n",
    "        c_hamilt_params [dict]: parameters of the hamiltonian\n",
    "        n_meas_reps [int]: number of measurement repetitions\n",
    "    \n",
    "    Returns:\n",
    "    ----------------\n",
    "        loss [float]: loss function value\n",
    "    \"\"\"\n",
    "    \n",
    "    return direct_hamilt_expectation(qubits, circuit_params, c_hamilt_params)\n",
    "    #return meas_hamilt_expectation(qubits, circuit_params, c_hamilt_params, n_meas_reps)\n",
    "\n",
    "def optimize_vqe(qubits,hamilt_params, init_circuit_params, n_meas_reps=1000):\n",
    "    \"\"\"\n",
    "    Optimize a VQE\n",
    "    \n",
    "    Args:\n",
    "    ----------------\n",
    "        qubits [list]: list of qubits\n",
    "        hamilt_params [dict]: parameters of the hamiltonian\n",
    "        init_circuit_params [np.array]: initial parameters of the circuit\n",
    "        n_meas_reps [int]: number of measurement repetitions\n",
    "    \n",
    "    Returns:\n",
    "    ----------------\n",
    "        opt_result [OptimizeResult]: result of the optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    def callbackF(Xi):\n",
    "        global Nfeval\n",
    "        print(\"Iter: \", Nfeval, \" Loss on train set: \", vqe_energy_loss(Xi, qubits, hamilt_params, n_meas_reps))\n",
    "        Nfeval += 1\n",
    "    \n",
    "    print(\"Energy should converge to \", exact_hamilt_GS_energy(qubits, hamilt_params))\n",
    "    print(\"Starting VQE minimization...\")\n",
    "    \n",
    "    opt_result = minimize(vqe_energy_loss,\n",
    "                            args=(qubits, hamilt_params, n_meas_reps),\n",
    "                            x0=init_circuit_params,\n",
    "                            method='COBYLA',\n",
    "                            callback=callbackF,\n",
    "                            )\n",
    "    \n",
    "    print(opt_result)\n",
    "    return opt_result.x\n",
    "\n",
    "def evaluate_vqe(qubits, hamilt_params_set, init_circuit_params=None, params_init_mode=\"normal_random\", n_meas_reps=1000):\n",
    "    \"\"\"\n",
    "    Evaluate a VQE on a set of hamiltonians\n",
    "    \n",
    "    Args:\n",
    "    ----------------\n",
    "        qubits [list]: list of qubits\n",
    "        hamilt_params_set [list]: list of the parameters of the hamiltonians\n",
    "        init_circuit_params [np.array]: initial parameters of the circuit\n",
    "        params_init_mode [str]: mode for initializing the circuit parameters\n",
    "        n_meas_reps [int]: number of measurement repetitions\n",
    "    \n",
    "    Returns:\n",
    "    ----------------\n",
    "        energy_expectations [np.array]: energy expectations of the hamiltonians\n",
    "        exact_energy_expectations [np.array]: exact energy expectations of the hamiltonians\n",
    "        abs_energy_errors [np.array]: absolute energy errors of the hamiltonians\n",
    "    \"\"\"\n",
    "    \n",
    "    energy_expectations = []\n",
    "    exact_energy_expectations = []\n",
    "    abs_energy_errors = []\n",
    "    \n",
    "    for hamilt_params_sample in hamilt_params_set:\n",
    "        if init_circuit_params is None:\n",
    "            init_circuit_params = initialize_circuit_params(qubits, params_init_mode)\n",
    "    \n",
    "        opt_circuit_params = optimize_vqe(qubits, hamilt_params_sample, params_init_mode, n_meas_reps)\n",
    "        \n",
    "        energy_expectation = direct_hamilt_expectation(qubits, opt_circuit_params, hamilt_params_sample, n_meas_reps)\n",
    "        exact_energy_expectation = exact_hamilt_GS_energy(qubits, hamilt_params_sample)\n",
    "        abs_energy_error = np.abs(energy_expectation - exact_energy_expectation)\n",
    "        energy_expectations.append(energy_expectation)\n",
    "        exact_energy_expectations.append(exact_energy_expectation)\n",
    "        abs_energy_errors.append(abs_energy_error)\n",
    "    \n",
    "    return np.array(energy_expectations), np.array(abs_energy_errors)\n",
    "    \n",
    "def vqe(qubits, hamilt_params_set):\n",
    "    \"\"\"\n",
    "    Convenience function for wrapping VQE routines\n",
    "    \n",
    "    Args:\n",
    "    ----------------\n",
    "        qubits [list]: list of qubits\n",
    "        hamilt_params_set [list]: list of the parameters of the hamiltonians\n",
    "        \n",
    "    Returns:\n",
    "    ----------------\n",
    "        energy_expectations [list]: list of the energy expectations of the hamiltonians in the test set\n",
    "        abs_energy_errors [list]: list of the absolute energy errors of the hamiltonians in the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    energy_expectations, abs_energy_errors = evaluate_vqe(qubits, hamilt_params_set, params_init_mode=\"normal_random\", n_meas_reps=1000)\n",
    "    return energy_expectations, abs_energy_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy should converge to  -11.403722989559629\n",
      "Starting VQE minimization...\n",
      "Iter:  1  Loss on train set:  3.3393581853085332\n",
      "Iter:  2  Loss on train set:  3.363206600542764\n",
      "Iter:  3  Loss on train set:  3.3393580730270656\n",
      "Iter:  4  Loss on train set:  3.3129355127064017\n",
      "Iter:  5  Loss on train set:  3.312935295917041\n",
      "Iter:  6  Loss on train set:  3.3336963990519557\n",
      "Iter:  7  Loss on train set:  3.3129350972611995\n",
      "Iter:  8  Loss on train set:  3.355789620554316\n",
      "Iter:  9  Loss on train set:  3.3129353011139546\n",
      "Iter:  10  Loss on train set:  3.2672526606345387\n",
      "Iter:  11  Loss on train set:  3.267253519759116\n",
      "Iter:  12  Loss on train set:  3.31256952664296\n",
      "Iter:  13  Loss on train set:  3.2672526724716793\n",
      "Iter:  14  Loss on train set:  3.2702085574577957\n",
      "Iter:  15  Loss on train set:  3.267252822905118\n",
      "Iter:  16  Loss on train set:  3.2707130465546452\n",
      "Iter:  17  Loss on train set:  3.3448961532893433\n",
      "Iter:  18  Loss on train set:  3.3482541574712372\n",
      "Iter:  19  Loss on train set:  3.2403399282979133\n",
      "Iter:  20  Loss on train set:  3.384644649416417\n",
      "Iter:  21  Loss on train set:  3.2638646818959653\n",
      "Iter:  22  Loss on train set:  3.308905613067238\n",
      "Iter:  23  Loss on train set:  3.1838108995557493\n",
      "Iter:  24  Loss on train set:  3.2244516651561494\n",
      "Iter:  25  Loss on train set:  3.1866507015066214\n",
      "Iter:  26  Loss on train set:  3.227850886140835\n",
      "Iter:  27  Loss on train set:  3.237233966017815\n",
      "Iter:  28  Loss on train set:  3.2169184189913054\n",
      "Iter:  29  Loss on train set:  3.210914803201134\n",
      "Iter:  30  Loss on train set:  3.2427179407775606\n",
      "Iter:  31  Loss on train set:  3.1240355593207365\n",
      "Iter:  32  Loss on train set:  3.1095465453529485\n",
      "Iter:  33  Loss on train set:  3.1095470497925803\n",
      "Iter:  34  Loss on train set:  3.2951773562580398\n",
      "Iter:  35  Loss on train set:  3.109547234927281\n",
      "Iter:  36  Loss on train set:  2.926578794438341\n",
      "Iter:  37  Loss on train set:  2.926577992657372\n",
      "Iter:  38  Loss on train set:  3.1951269303209293\n",
      "Iter:  39  Loss on train set:  2.926578240509831\n",
      "Iter:  40  Loss on train set:  3.038397818642737\n",
      "Iter:  41  Loss on train set:  2.926578440898089\n",
      "Iter:  42  Loss on train set:  2.211526847716563\n",
      "Iter:  43  Loss on train set:  2.2115263258622435\n",
      "Iter:  44  Loss on train set:  2.8772076172713015\n",
      "Iter:  45  Loss on train set:  2.211525817733596\n",
      "Iter:  46  Loss on train set:  2.2143982639108595\n",
      "Iter:  47  Loss on train set:  2.211526068272226\n",
      "Iter:  48  Loss on train set:  1.8825570877235536\n",
      "Iter:  49  Loss on train set:  2.777885422623185\n",
      "Iter:  50  Loss on train set:  2.1744699350514947\n",
      "Iter:  51  Loss on train set:  1.5917582195488884\n",
      "Iter:  52  Loss on train set:  1.5871483870252299\n",
      "Iter:  53  Loss on train set:  1.6663840162529664\n",
      "Iter:  54  Loss on train set:  2.3896132272681943\n",
      "Iter:  55  Loss on train set:  1.385752579806157\n",
      "Iter:  56  Loss on train set:  1.7094011418634991\n",
      "Iter:  57  Loss on train set:  1.253250905732944\n",
      "Iter:  58  Loss on train set:  2.126494413845724\n",
      "Iter:  59  Loss on train set:  0.473333553666788\n",
      "Iter:  60  Loss on train set:  0.6963265352461576\n",
      "Iter:  61  Loss on train set:  0.4318804265579989\n",
      "Iter:  62  Loss on train set:  -0.2777626187043299\n",
      "Iter:  63  Loss on train set:  -0.201488572146852\n",
      "Iter:  64  Loss on train set:  -0.3818523972059948\n",
      "Iter:  65  Loss on train set:  -0.10057868413692803\n",
      "Iter:  66  Loss on train set:  0.018184967923695172\n",
      "Iter:  67  Loss on train set:  -0.5615260454309229\n",
      "Iter:  68  Loss on train set:  0.14851729554288318\n",
      "Iter:  69  Loss on train set:  -0.9914346973519557\n",
      "Iter:  70  Loss on train set:  -1.0028345833897405\n",
      "Iter:  71  Loss on train set:  -0.7152500540425973\n",
      "Iter:  72  Loss on train set:  -1.1136254550386122\n",
      "Iter:  73  Loss on train set:  -0.879190116361911\n",
      "Iter:  74  Loss on train set:  -0.9851890781197727\n",
      "Iter:  75  Loss on train set:  -0.46835620375059095\n",
      "Iter:  76  Loss on train set:  -1.2401672502357173\n",
      "Iter:  77  Loss on train set:  -1.1819530629307058\n",
      "Iter:  78  Loss on train set:  -1.3179692742072644\n",
      "Iter:  79  Loss on train set:  -1.6899422914593827\n",
      "Iter:  80  Loss on train set:  -2.0934619541235038\n",
      "Iter:  81  Loss on train set:  -1.6810550152907107\n",
      "Iter:  82  Loss on train set:  -1.5470671068448558\n",
      "Iter:  83  Loss on train set:  -2.209517625917934\n",
      "Iter:  84  Loss on train set:  -2.139988950058686\n",
      "Iter:  85  Loss on train set:  -1.5646843134587816\n",
      "Iter:  86  Loss on train set:  -1.7243972716569709\n",
      "Iter:  87  Loss on train set:  -2.1979851005419637\n",
      "Iter:  88  Loss on train set:  -2.256234302773972\n",
      "Iter:  89  Loss on train set:  -2.5045929417187276\n",
      "Iter:  90  Loss on train set:  -2.0680755594765654\n",
      "Iter:  91  Loss on train set:  -1.6364761132742371\n",
      "Iter:  92  Loss on train set:  -3.1492854408890465\n",
      "Iter:  93  Loss on train set:  -3.022530386859368\n",
      "Iter:  94  Loss on train set:  -3.1121927057237175\n",
      "Iter:  95  Loss on train set:  -2.582640390548642\n",
      "Iter:  96  Loss on train set:  -3.1213108792322926\n",
      "Iter:  97  Loss on train set:  -3.657777533438574\n",
      "Iter:  98  Loss on train set:  -2.32779295521134\n",
      "Iter:  99  Loss on train set:  -3.5920705839732627\n",
      "Iter:  100  Loss on train set:  -3.4406495194928324\n",
      "Iter:  101  Loss on train set:  -3.6577779128862087\n",
      "Iter:  102  Loss on train set:  -3.579203072544599\n",
      "Iter:  103  Loss on train set:  -3.6577763323697683\n",
      "Iter:  104  Loss on train set:  -3.651170601469455\n",
      "Iter:  105  Loss on train set:  -3.701389578909736\n",
      "Iter:  106  Loss on train set:  -3.771387785013432\n",
      "Iter:  107  Loss on train set:  -3.771387662549383\n",
      "Iter:  108  Loss on train set:  -3.7003618589970975\n",
      "Iter:  109  Loss on train set:  -3.7713857493285694\n",
      "Iter:  110  Loss on train set:  -3.7713882714983376\n",
      "Iter:  111  Loss on train set:  -3.778067256283956\n",
      "Iter:  112  Loss on train set:  -3.757402257594277\n",
      "Iter:  113  Loss on train set:  -3.7458174593420073\n",
      "Iter:  114  Loss on train set:  -3.8239004655172613\n",
      "Iter:  115  Loss on train set:  -3.8229717532520846\n",
      "Iter:  116  Loss on train set:  -3.8634086260022156\n",
      "Iter:  117  Loss on train set:  -3.8634088262220345\n",
      "Iter:  118  Loss on train set:  -3.8411934252190334\n",
      "Iter:  119  Loss on train set:  -3.863407856184367\n",
      "Iter:  120  Loss on train set:  -3.8296819262714665\n",
      "Iter:  121  Loss on train set:  -3.8577071258576883\n",
      "Iter:  122  Loss on train set:  -3.7915201167719266\n",
      "Iter:  123  Loss on train set:  -3.878806010805333\n",
      "Iter:  124  Loss on train set:  -3.89677031155664\n",
      "Iter:  125  Loss on train set:  -3.886867495639497\n",
      "Iter:  126  Loss on train set:  -3.980590513232289\n",
      "Iter:  127  Loss on train set:  -4.028164649834533\n",
      "Iter:  128  Loss on train set:  -3.955560515307207\n",
      "Iter:  129  Loss on train set:  -4.060928253088995\n",
      "Iter:  130  Loss on train set:  -4.080502314310589\n",
      "Iter:  131  Loss on train set:  -4.119648943752129\n",
      "Iter:  132  Loss on train set:  -4.440432065283405\n",
      "Iter:  133  Loss on train set:  -4.440434059864874\n",
      "Iter:  134  Loss on train set:  -4.483850935052866\n",
      "Iter:  135  Loss on train set:  -4.297120448683869\n",
      "Iter:  136  Loss on train set:  -3.902192233509619\n",
      "Iter:  137  Loss on train set:  -4.483851290139587\n",
      "Iter:  138  Loss on train set:  -4.455013952874958\n",
      "Iter:  139  Loss on train set:  -4.48385122098718\n",
      "Iter:  140  Loss on train set:  -4.435390348369771\n",
      "Iter:  141  Loss on train set:  -4.483852827963034\n",
      "Iter:  142  Loss on train set:  -3.855290291376598\n",
      "Iter:  143  Loss on train set:  -4.451204320069134\n",
      "Iter:  144  Loss on train set:  -4.377497737331652\n",
      "Iter:  145  Loss on train set:  -4.4838525110727545\n",
      "Iter:  146  Loss on train set:  -3.8287976192607887\n",
      "Iter:  147  Loss on train set:  -4.264695531923289\n",
      "Iter:  148  Loss on train set:  -3.556356046453634\n",
      "Iter:  149  Loss on train set:  -4.459151300907582\n",
      "Iter:  150  Loss on train set:  -4.722390244474102\n",
      "Iter:  151  Loss on train set:  -4.592708243511947\n",
      "Iter:  152  Loss on train set:  -4.900247990058864\n",
      "Iter:  153  Loss on train set:  -4.62512473368573\n",
      "Iter:  154  Loss on train set:  -4.655275420390238\n",
      "Iter:  155  Loss on train set:  -4.460647384255532\n",
      "Iter:  156  Loss on train set:  -4.823042840168224\n",
      "Iter:  157  Loss on train set:  -4.46300104436574\n",
      "Iter:  158  Loss on train set:  -5.069505259267354\n",
      "Iter:  159  Loss on train set:  -4.965959699754453\n",
      "Iter:  160  Loss on train set:  -5.314644608209689\n",
      "Iter:  161  Loss on train set:  -5.074134927766678\n",
      "Iter:  162  Loss on train set:  -5.378646625533266\n",
      "Iter:  163  Loss on train set:  -5.291378291475817\n",
      "Iter:  164  Loss on train set:  -5.0280035007352994\n",
      "Iter:  165  Loss on train set:  -5.413802905692998\n",
      "Iter:  166  Loss on train set:  -5.207719064655458\n",
      "Iter:  167  Loss on train set:  -5.483688724398634\n",
      "Iter:  168  Loss on train set:  -4.219651219581665\n",
      "Iter:  169  Loss on train set:  -5.412605579128575\n",
      "Iter:  170  Loss on train set:  -5.3549239965598865\n",
      "Iter:  171  Loss on train set:  -5.398697558843672\n",
      "Iter:  172  Loss on train set:  -5.333558700619417\n",
      "Iter:  173  Loss on train set:  -5.342482264900183\n",
      "Iter:  174  Loss on train set:  -5.597530371541165\n",
      "Iter:  175  Loss on train set:  -5.39844111923863\n",
      "Iter:  176  Loss on train set:  -5.573060091142644\n",
      "Iter:  177  Loss on train set:  -5.4898447824910654\n",
      "Iter:  178  Loss on train set:  -4.640980177113844\n",
      "Iter:  179  Loss on train set:  -5.361015145624355\n",
      "Iter:  180  Loss on train set:  -5.148955457927912\n",
      "Iter:  181  Loss on train set:  -5.420879538764076\n",
      "Iter:  182  Loss on train set:  -5.336081922724048\n",
      "Iter:  183  Loss on train set:  -5.43875648442123\n",
      "Iter:  184  Loss on train set:  -5.920685052873463\n",
      "Iter:  185  Loss on train set:  -5.969418262923737\n",
      "Iter:  186  Loss on train set:  -5.198120241705185\n",
      "Iter:  187  Loss on train set:  -5.406797758742845\n",
      "Iter:  188  Loss on train set:  -5.633355804366149\n",
      "Iter:  189  Loss on train set:  -5.6785639198177424\n",
      "Iter:  190  Loss on train set:  -5.726479378478064\n",
      "Iter:  191  Loss on train set:  -5.795291970987112\n",
      "Iter:  192  Loss on train set:  -4.743714128721737\n",
      "Iter:  193  Loss on train set:  -5.763029583162522\n",
      "Iter:  194  Loss on train set:  -5.903843056641563\n",
      "Iter:  195  Loss on train set:  -5.970217684439465\n",
      "Iter:  196  Loss on train set:  -5.654166614057953\n",
      "Iter:  197  Loss on train set:  -5.9560959789640915\n",
      "Iter:  198  Loss on train set:  -5.932721744427306\n",
      "Iter:  199  Loss on train set:  -5.975075018323639\n",
      "Iter:  200  Loss on train set:  -6.014478964393458\n",
      "Iter:  201  Loss on train set:  -6.014479764537681\n",
      "Iter:  202  Loss on train set:  -6.015423603407339\n",
      "Iter:  203  Loss on train set:  -6.019210010973831\n",
      "Iter:  204  Loss on train set:  -6.003774286935268\n",
      "Iter:  205  Loss on train set:  -6.0192112438316325\n",
      "Iter:  206  Loss on train set:  -6.010255200138558\n",
      "Iter:  207  Loss on train set:  -6.019208267727225\n",
      "Iter:  208  Loss on train set:  -6.010226628986777\n",
      "Iter:  209  Loss on train set:  -6.015867553220229\n",
      "Iter:  210  Loss on train set:  -6.001295834536544\n",
      "Iter:  211  Loss on train set:  -6.021751115340761\n",
      "Iter:  212  Loss on train set:  -6.062665672719525\n",
      "Iter:  213  Loss on train set:  -6.051191177115371\n",
      "Iter:  214  Loss on train set:  -6.1259337519193675\n",
      "Iter:  215  Loss on train set:  -6.128740931349984\n",
      "Iter:  216  Loss on train set:  -6.107554794425624\n",
      "Iter:  217  Loss on train set:  -6.124560794960218\n",
      "Iter:  218  Loss on train set:  -6.050689217611593\n",
      "Iter:  219  Loss on train set:  -6.1287400518598965\n",
      "Iter:  220  Loss on train set:  -6.124516867713148\n",
      "Iter:  221  Loss on train set:  -6.145048347193343\n",
      "Iter:  222  Loss on train set:  -6.112222452803348\n",
      "Iter:  223  Loss on train set:  -6.170397740343452\n",
      "Iter:  224  Loss on train set:  -6.142169554273927\n",
      "Iter:  225  Loss on train set:  -6.170633977478298\n",
      "Iter:  226  Loss on train set:  -6.088709352574311\n",
      "Iter:  227  Loss on train set:  -6.147725394075324\n",
      "Iter:  228  Loss on train set:  -6.136825626749156\n",
      "Iter:  229  Loss on train set:  -6.162021512932236\n",
      "Iter:  230  Loss on train set:  -6.203799917040371\n",
      "Iter:  231  Loss on train set:  -6.203798170489545\n",
      "Iter:  232  Loss on train set:  -5.607081626616468\n",
      "Iter:  233  Loss on train set:  -6.131955660103273\n",
      "Iter:  234  Loss on train set:  -5.890893426135787\n",
      "Iter:  235  Loss on train set:  -6.115839121968523\n",
      "Iter:  236  Loss on train set:  -6.18778007839308\n",
      "Iter:  237  Loss on train set:  -6.11471897820427\n",
      "Iter:  238  Loss on train set:  -5.497646517917325\n",
      "Iter:  239  Loss on train set:  -6.205193972332614\n",
      "Iter:  240  Loss on train set:  -5.831013678650552\n",
      "Iter:  241  Loss on train set:  -6.205191403039207\n",
      "Iter:  242  Loss on train set:  -5.39447386878139\n",
      "Iter:  243  Loss on train set:  -6.205195072358097\n",
      "Iter:  244  Loss on train set:  -6.1428754103252245\n",
      "Iter:  245  Loss on train set:  -6.0555470492625805\n",
      "Iter:  246  Loss on train set:  -5.559612561000705\n",
      "Iter:  247  Loss on train set:  -5.78337701269046\n",
      "Iter:  248  Loss on train set:  -5.3622486211561835\n",
      "Iter:  249  Loss on train set:  -6.205192911707334\n",
      "Iter:  250  Loss on train set:  -4.927802029733355\n",
      "Iter:  251  Loss on train set:  -6.205196491993473\n",
      "Iter:  252  Loss on train set:  -5.962250699393423\n",
      "Iter:  253  Loss on train set:  -6.0620414192644105\n",
      "Iter:  254  Loss on train set:  -5.888213592140845\n",
      "Iter:  255  Loss on train set:  -6.116734011577328\n",
      "Iter:  256  Loss on train set:  -5.52273305132551\n",
      "Iter:  257  Loss on train set:  -6.12000100729984\n",
      "Iter:  258  Loss on train set:  -6.350453103633459\n",
      "Iter:  259  Loss on train set:  -6.398770622355096\n",
      "Iter:  260  Loss on train set:  -6.032458954592512\n",
      "Iter:  261  Loss on train set:  -6.14068646840958\n",
      "Iter:  262  Loss on train set:  -6.513325271007759\n",
      "Iter:  263  Loss on train set:  -6.51163914753266\n",
      "Iter:  264  Loss on train set:  -5.213100542716453\n",
      "Iter:  265  Loss on train set:  -6.207390347098892\n",
      "Iter:  266  Loss on train set:  -5.6590914091148825\n",
      "Iter:  267  Loss on train set:  -6.530091064088213\n",
      "Iter:  268  Loss on train set:  -5.557531434693331\n",
      "Iter:  269  Loss on train set:  -6.606106928530947\n",
      "Iter:  270  Loss on train set:  -6.543231343614876\n",
      "Iter:  271  Loss on train set:  -6.533809323900037\n",
      "Iter:  272  Loss on train set:  -6.537488131652439\n",
      "Iter:  273  Loss on train set:  -6.712656052950783\n",
      "Iter:  274  Loss on train set:  -6.167294180764496\n",
      "Iter:  275  Loss on train set:  -6.705162629940501\n",
      "Iter:  276  Loss on train set:  -6.595702335949037\n",
      "Iter:  277  Loss on train set:  -6.347719811250132\n",
      "Iter:  278  Loss on train set:  -6.069587663806735\n",
      "Iter:  279  Loss on train set:  -6.36735236152014\n",
      "Iter:  280  Loss on train set:  -6.075368689161628\n",
      "Iter:  281  Loss on train set:  -6.604553958399373\n",
      "Iter:  282  Loss on train set:  -6.660336576928081\n",
      "Iter:  283  Loss on train set:  -6.666589177278993\n",
      "Iter:  284  Loss on train set:  -5.880800786091941\n",
      "Iter:  285  Loss on train set:  -6.842635886520753\n",
      "Iter:  286  Loss on train set:  -6.218629335446136\n",
      "Iter:  287  Loss on train set:  -6.82772199398035\n",
      "Iter:  288  Loss on train set:  -5.927974245885672\n",
      "Iter:  289  Loss on train set:  -6.6114927605276845\n",
      "Iter:  290  Loss on train set:  -6.651821354869309\n",
      "Iter:  291  Loss on train set:  -6.47680697630328\n",
      "Iter:  292  Loss on train set:  -6.578057091616502\n",
      "Iter:  293  Loss on train set:  -6.893545655419775\n",
      "Iter:  294  Loss on train set:  -6.714356113626307\n",
      "Iter:  295  Loss on train set:  -6.940459318129766\n",
      "Iter:  296  Loss on train set:  -6.535184067230466\n",
      "Iter:  297  Loss on train set:  -6.541384525034646\n",
      "Iter:  298  Loss on train set:  -6.737499943955874\n",
      "Iter:  299  Loss on train set:  -6.9440500937672285\n",
      "Iter:  300  Loss on train set:  -6.9082836826462195\n",
      "Iter:  301  Loss on train set:  -6.944048275792124\n",
      "Iter:  302  Loss on train set:  -6.945000935647428\n",
      "Iter:  303  Loss on train set:  -6.948583041058969\n",
      "Iter:  304  Loss on train set:  -6.98605467888027\n",
      "Iter:  305  Loss on train set:  -6.987056560834608\n",
      "Iter:  306  Loss on train set:  -6.980799460589898\n",
      "Iter:  307  Loss on train set:  -6.977450085079557\n",
      "Iter:  308  Loss on train set:  -7.006799089445376\n",
      "Iter:  309  Loss on train set:  -7.006798511768778\n",
      "Iter:  310  Loss on train set:  -6.998153478503198\n",
      "Iter:  311  Loss on train set:  -7.006798634362226\n",
      "Iter:  312  Loss on train set:  -7.022656986176334\n",
      "Iter:  313  Loss on train set:  -7.02712804811036\n",
      "Iter:  314  Loss on train set:  -7.0559083567743075\n",
      "Iter:  315  Loss on train set:  -7.077234554850804\n",
      "Iter:  316  Loss on train set:  -7.043469459252674\n",
      "Iter:  317  Loss on train set:  -7.094421286504854\n",
      "Iter:  318  Loss on train set:  -7.152538792932428\n",
      "Iter:  319  Loss on train set:  -7.147089665522513\n",
      "Iter:  320  Loss on train set:  -7.222942711918836\n",
      "Iter:  321  Loss on train set:  -7.193949856284513\n",
      "Iter:  322  Loss on train set:  -7.193178316292325\n",
      "Iter:  323  Loss on train set:  -7.222945026846283\n",
      "Iter:  324  Loss on train set:  -7.278155526067527\n",
      "Iter:  325  Loss on train set:  -7.303835793765771\n",
      "Iter:  326  Loss on train set:  -7.297456259128211\n",
      "Iter:  327  Loss on train set:  -7.261195823385044\n",
      "Iter:  328  Loss on train set:  -7.304765846014824\n",
      "Iter:  329  Loss on train set:  -7.300158655712703\n",
      "Iter:  330  Loss on train set:  -6.481302354225585\n",
      "Iter:  331  Loss on train set:  -7.22406088883791\n",
      "Iter:  332  Loss on train set:  -5.758551731786925\n",
      "Iter:  333  Loss on train set:  -7.10496286636331\n",
      "Iter:  334  Loss on train set:  -6.651197249236165\n",
      "Iter:  335  Loss on train set:  -7.116344161669686\n",
      "Iter:  336  Loss on train set:  -6.910387074321466\n",
      "Iter:  337  Loss on train set:  -7.063820325417178\n",
      "Iter:  338  Loss on train set:  -6.590179202560162\n",
      "Iter:  339  Loss on train set:  -7.404559186537375\n",
      "Iter:  340  Loss on train set:  -6.675264907246132\n",
      "Iter:  341  Loss on train set:  -7.232759359315969\n",
      "Iter:  342  Loss on train set:  -7.317921456040123\n",
      "Iter:  343  Loss on train set:  -7.40455769275728\n",
      "Iter:  344  Loss on train set:  -6.654583240983409\n",
      "Iter:  345  Loss on train set:  -7.403990756977542\n",
      "Iter:  346  Loss on train set:  -6.117625872611198\n",
      "Iter:  347  Loss on train set:  -7.4045567030173345\n",
      "Iter:  348  Loss on train set:  -7.486265342940788\n",
      "Iter:  349  Loss on train set:  -7.486259019732556\n",
      "Iter:  350  Loss on train set:  -6.843701462245722\n",
      "Iter:  351  Loss on train set:  -7.486266109397078\n",
      "Iter:  352  Loss on train set:  -7.093102277096014\n",
      "Iter:  353  Loss on train set:  -7.486265699309518\n",
      "Iter:  354  Loss on train set:  -6.41515692723578\n",
      "Iter:  355  Loss on train set:  -7.288552807788949\n",
      "Iter:  356  Loss on train set:  -7.3115226275047\n",
      "Iter:  357  Loss on train set:  -7.523531176405022\n",
      "Iter:  358  Loss on train set:  -7.255413685450701\n",
      "Iter:  359  Loss on train set:  -7.4474953248715385\n",
      "Iter:  360  Loss on train set:  -6.606923715246676\n",
      "Iter:  361  Loss on train set:  -7.6937698500748635\n",
      "Iter:  362  Loss on train set:  -6.696887359498437\n",
      "Iter:  363  Loss on train set:  -7.30082642679889\n",
      "Iter:  364  Loss on train set:  -6.013528038076512\n",
      "Iter:  365  Loss on train set:  -7.631417781853612\n",
      "Iter:  366  Loss on train set:  -7.110273886874883\n",
      "Iter:  367  Loss on train set:  -7.752123744072011\n",
      "Iter:  368  Loss on train set:  -7.006635283050116\n",
      "Iter:  369  Loss on train set:  -7.7180701028663625\n",
      "Iter:  370  Loss on train set:  -7.167668977667799\n",
      "Iter:  371  Loss on train set:  -7.694909336459972\n",
      "Iter:  372  Loss on train set:  -7.37937533438775\n",
      "Iter:  373  Loss on train set:  -7.392690513356054\n",
      "Iter:  374  Loss on train set:  -6.947596327751994\n",
      "Iter:  375  Loss on train set:  -7.562025428358431\n",
      "Iter:  376  Loss on train set:  -7.487370678258069\n",
      "Iter:  377  Loss on train set:  -7.607909758639485\n",
      "Iter:  378  Loss on train set:  -7.227240955722301\n",
      "Iter:  379  Loss on train set:  -7.718282369666575\n",
      "Iter:  380  Loss on train set:  -7.5112106464831925\n",
      "Iter:  381  Loss on train set:  -7.618253311891139\n",
      "Iter:  382  Loss on train set:  -6.715951703612516\n",
      "Iter:  383  Loss on train set:  -7.4580986012939\n",
      "Iter:  384  Loss on train set:  -7.482970784941166\n",
      "Iter:  385  Loss on train set:  -7.4782998617256204\n",
      "Iter:  386  Loss on train set:  -7.0967652698825265\n",
      "Iter:  387  Loss on train set:  -7.627458451728927\n",
      "Iter:  388  Loss on train set:  -7.285896724530761\n",
      "Iter:  389  Loss on train set:  -7.550675173251301\n",
      "Iter:  390  Loss on train set:  -6.866119415439831\n",
      "Iter:  391  Loss on train set:  -7.686897685860362\n",
      "Iter:  392  Loss on train set:  -7.314624333340841\n",
      "Iter:  393  Loss on train set:  -7.794693326760353\n",
      "Iter:  394  Loss on train set:  -6.6302785059374525\n",
      "Iter:  395  Loss on train set:  -7.784211643771538\n",
      "Iter:  396  Loss on train set:  -6.981860993767821\n",
      "Iter:  397  Loss on train set:  -7.77843419787162\n",
      "Iter:  398  Loss on train set:  -6.8548313509830345\n",
      "Iter:  399  Loss on train set:  -7.794693827746263\n",
      "Iter:  400  Loss on train set:  -7.235170943232254\n",
      "Iter:  401  Loss on train set:  -7.796652012900314\n",
      "Iter:  402  Loss on train set:  -7.008366304017295\n",
      "Iter:  403  Loss on train set:  -7.797983145119962\n",
      "Iter:  404  Loss on train set:  -6.956470609123962\n",
      "Iter:  405  Loss on train set:  -7.821051211792344\n",
      "Iter:  406  Loss on train set:  -6.935484311214838\n",
      "Iter:  407  Loss on train set:  -7.821446451008766\n",
      "Iter:  408  Loss on train set:  -6.450336272159436\n",
      "Iter:  409  Loss on train set:  -7.824809526587397\n",
      "Iter:  410  Loss on train set:  -7.007385692138318\n",
      "Iter:  411  Loss on train set:  -7.818198539681739\n",
      "Iter:  412  Loss on train set:  -7.094530973280447\n",
      "Iter:  413  Loss on train set:  -7.837347558157729\n",
      "Iter:  414  Loss on train set:  -7.387822478654024\n",
      "Iter:  415  Loss on train set:  -7.842643785819021\n",
      "Iter:  416  Loss on train set:  -7.759769800226508\n",
      "Iter:  417  Loss on train set:  -7.842651021346967\n",
      "Iter:  418  Loss on train set:  -7.519727463876816\n",
      "Iter:  419  Loss on train set:  -7.842646152310717\n",
      "Iter:  420  Loss on train set:  -7.634237952961229\n",
      "Iter:  421  Loss on train set:  -7.837718629655381\n",
      "Iter:  422  Loss on train set:  -7.747186264357925\n",
      "Iter:  423  Loss on train set:  -7.847573889757459\n",
      "Iter:  424  Loss on train set:  -7.8224493663440136\n",
      "Iter:  425  Loss on train set:  -7.853181095193564\n",
      "Iter:  426  Loss on train set:  -7.7901582148259525\n",
      "Iter:  427  Loss on train set:  -7.83745699007416\n",
      "Iter:  428  Loss on train set:  -7.825221901974918\n",
      "Iter:  429  Loss on train set:  -7.857392398461432\n",
      "Iter:  430  Loss on train set:  -7.779226648448546\n",
      "Iter:  431  Loss on train set:  -7.857387324221822\n",
      "Iter:  432  Loss on train set:  -7.833895923782723\n",
      "Iter:  433  Loss on train set:  -7.858517908372089\n",
      "Iter:  434  Loss on train set:  -7.801951288161875\n",
      "Iter:  435  Loss on train set:  -7.841380933983944\n",
      "Iter:  436  Loss on train set:  -7.806543682905845\n",
      "Iter:  437  Loss on train set:  -7.853393646123346\n",
      "Iter:  438  Loss on train set:  -7.531270223358033\n",
      "Iter:  439  Loss on train set:  -7.854104327309077\n",
      "Iter:  440  Loss on train set:  -7.574334223356622\n",
      "Iter:  441  Loss on train set:  -7.818307031951349\n",
      "Iter:  442  Loss on train set:  -7.042006679193545\n",
      "Iter:  443  Loss on train set:  -7.858513374234506\n",
      "Iter:  444  Loss on train set:  -6.981281707663821\n",
      "Iter:  445  Loss on train set:  -7.85851943495231\n",
      "Iter:  446  Loss on train set:  -7.588977649828437\n",
      "Iter:  447  Loss on train set:  -7.824302383571887\n",
      "Iter:  448  Loss on train set:  -7.598015029126688\n",
      "Iter:  449  Loss on train set:  -7.645406671977524\n",
      "Iter:  450  Loss on train set:  -6.66029558989151\n",
      "Iter:  451  Loss on train set:  -7.661999857424809\n",
      "Iter:  452  Loss on train set:  -7.030090421329222\n",
      "Iter:  453  Loss on train set:  -7.858524706660441\n",
      "Iter:  454  Loss on train set:  -7.279816672484298\n",
      "Iter:  455  Loss on train set:  -7.416308168907717\n",
      "Iter:  456  Loss on train set:  -6.677612911424596\n",
      "Iter:  457  Loss on train set:  -7.858529640601555\n",
      "Iter:  458  Loss on train set:  -6.924289666138405\n",
      "Iter:  459  Loss on train set:  -7.329511754611891\n",
      "Iter:  460  Loss on train set:  -7.425971113100421\n",
      "Iter:  461  Loss on train set:  -7.618499966197907\n",
      "Iter:  462  Loss on train set:  -7.058738483596718\n",
      "Iter:  463  Loss on train set:  -7.858537473704361\n",
      "Iter:  464  Loss on train set:  -7.265464090322104\n",
      "Iter:  465  Loss on train set:  -7.506538111910688\n",
      "Iter:  466  Loss on train set:  -7.18899755892324\n",
      "Iter:  467  Loss on train set:  -7.6048085219902255\n",
      "Iter:  468  Loss on train set:  -7.7022164089319745\n",
      "Iter:  469  Loss on train set:  -7.722283720951468\n",
      "Iter:  470  Loss on train set:  -7.44244582680591\n",
      "Iter:  471  Loss on train set:  -7.856272238267595\n",
      "Iter:  472  Loss on train set:  -7.19658028031699\n",
      "Iter:  473  Loss on train set:  -7.814096202526798\n",
      "Iter:  474  Loss on train set:  -6.9609493529644695\n",
      "Iter:  475  Loss on train set:  -7.582509751025722\n",
      "Iter:  476  Loss on train set:  -6.801284922880994\n",
      "Iter:  477  Loss on train set:  -7.603121758257004\n",
      "Iter:  478  Loss on train set:  -7.121757910942377\n",
      "Iter:  479  Loss on train set:  -7.8053727245371185\n",
      "Iter:  480  Loss on train set:  -7.063199887522179\n",
      "Iter:  481  Loss on train set:  -7.631315285312832\n",
      "Iter:  482  Loss on train set:  -6.9718930244826485\n",
      "Iter:  483  Loss on train set:  -7.8494528931152505\n",
      "Iter:  484  Loss on train set:  -7.298769648564265\n",
      "Iter:  485  Loss on train set:  -7.715140319431424\n",
      "Iter:  486  Loss on train set:  -7.121419967389381\n",
      "Iter:  487  Loss on train set:  -7.736741926857218\n",
      "Iter:  488  Loss on train set:  -7.421709757382675\n",
      "Iter:  489  Loss on train set:  -7.7295861093457905\n",
      "Iter:  490  Loss on train set:  -6.903040110018793\n",
      "Iter:  491  Loss on train set:  -7.752172439844981\n",
      "Iter:  492  Loss on train set:  -7.276320746223767\n",
      "Iter:  493  Loss on train set:  -7.712307654332058\n",
      "Iter:  494  Loss on train set:  -7.15626584495156\n",
      "Iter:  495  Loss on train set:  -7.7285764079083785\n",
      "Iter:  496  Loss on train set:  -7.195857020037691\n",
      "Iter:  497  Loss on train set:  -7.841379195455434\n",
      "Iter:  498  Loss on train set:  -6.686010881470063\n",
      "Iter:  499  Loss on train set:  -7.794205634660919\n",
      "Iter:  500  Loss on train set:  -7.1173385916492435\n",
      "Iter:  501  Loss on train set:  -7.870174446153865\n",
      "Iter:  502  Loss on train set:  -7.164496815524313\n",
      "Iter:  503  Loss on train set:  -7.870180502162261\n",
      "Iter:  504  Loss on train set:  -7.517327425817442\n",
      "Iter:  505  Loss on train set:  -7.870085894790336\n",
      "Iter:  506  Loss on train set:  -7.6899524302795825\n",
      "Iter:  507  Loss on train set:  -7.906025175627965\n",
      "Iter:  508  Loss on train set:  -7.330862832738543\n",
      "Iter:  509  Loss on train set:  -7.892005171366872\n",
      "Iter:  510  Loss on train set:  -7.117591697193625\n",
      "Iter:  511  Loss on train set:  -7.802692642798231\n",
      "Iter:  512  Loss on train set:  -7.700353557165848\n",
      "Iter:  513  Loss on train set:  -7.957248894294132\n",
      "Iter:  514  Loss on train set:  -7.640938943803918\n",
      "Iter:  515  Loss on train set:  -7.960258810351169\n",
      "Iter:  516  Loss on train set:  -7.008823053445286\n",
      "Iter:  517  Loss on train set:  -7.9565328986345705\n",
      "Iter:  518  Loss on train set:  -7.560750247460437\n",
      "Iter:  519  Loss on train set:  -7.912552380845192\n",
      "Iter:  520  Loss on train set:  -7.659179371958611\n",
      "Iter:  521  Loss on train set:  -7.917657187519021\n",
      "Iter:  522  Loss on train set:  -6.737189241886024\n",
      "Iter:  523  Loss on train set:  -7.808431239424621\n",
      "Iter:  524  Loss on train set:  -7.5660453378870445\n",
      "Iter:  525  Loss on train set:  -7.955230128809115\n",
      "Iter:  526  Loss on train set:  -6.938737322426776\n",
      "Iter:  527  Loss on train set:  -7.8674323390799\n",
      "Iter:  528  Loss on train set:  -7.94982356287311\n",
      "Iter:  529  Loss on train set:  -7.933622201806376\n",
      "Iter:  530  Loss on train set:  -7.963357361651372\n",
      "Iter:  531  Loss on train set:  -7.9561200946484565\n",
      "Iter:  532  Loss on train set:  -7.966021541718224\n",
      "Iter:  533  Loss on train set:  -7.940504048385678\n",
      "Iter:  534  Loss on train set:  -7.966344129359035\n",
      "Iter:  535  Loss on train set:  -7.959752011600428\n",
      "Iter:  536  Loss on train set:  -7.966231745649813\n",
      "Iter:  537  Loss on train set:  -7.938781268801781\n",
      "Iter:  538  Loss on train set:  -7.966728788452951\n",
      "Iter:  539  Loss on train set:  -7.94627108097301\n",
      "Iter:  540  Loss on train set:  -7.966005679989195\n",
      "Iter:  541  Loss on train set:  -7.945295833482747\n",
      "Iter:  542  Loss on train set:  -7.939799051103514\n",
      "Iter:  543  Loss on train set:  -8.001435944138466\n",
      "Iter:  544  Loss on train set:  -8.009857531166933\n",
      "Iter:  545  Loss on train set:  -7.750571379048305\n",
      "Iter:  546  Loss on train set:  -8.019379100941524\n",
      "Iter:  547  Loss on train set:  -7.8508758527877\n",
      "Iter:  548  Loss on train set:  -8.029371221581311\n",
      "Iter:  549  Loss on train set:  -8.01327944829179\n",
      "Iter:  550  Loss on train set:  -8.029372877366562\n",
      "Iter:  551  Loss on train set:  -8.020706070320312\n",
      "Iter:  552  Loss on train set:  -8.019847800106078\n",
      "Iter:  553  Loss on train set:  -8.021734113209433\n",
      "Iter:  554  Loss on train set:  -8.033929526412173\n",
      "Iter:  555  Loss on train set:  -8.029165731175604\n",
      "Iter:  556  Loss on train set:  -8.062392229502425\n",
      "Iter:  557  Loss on train set:  -7.659988179175796\n",
      "Iter:  558  Loss on train set:  -8.069705742941455\n",
      "Iter:  559  Loss on train set:  -7.801680778376763\n",
      "Iter:  560  Loss on train set:  -8.00436870625344\n",
      "Iter:  561  Loss on train set:  -7.839899579049291\n",
      "Iter:  562  Loss on train set:  -7.989845034661641\n",
      "Iter:  563  Loss on train set:  -7.948633523917312\n",
      "Iter:  564  Loss on train set:  -7.922909084450859\n",
      "Iter:  565  Loss on train set:  -8.06030836087545\n",
      "Iter:  566  Loss on train set:  -7.9437530109371925\n",
      "Iter:  567  Loss on train set:  -7.90601935488043\n",
      "Iter:  568  Loss on train set:  -7.980436166342233\n",
      "Iter:  569  Loss on train set:  -8.10939755138202\n",
      "Iter:  570  Loss on train set:  -8.126095490762118\n",
      "Iter:  571  Loss on train set:  -7.963325130542062\n",
      "Iter:  572  Loss on train set:  -8.160190551183653\n",
      "Iter:  573  Loss on train set:  -8.18147098269683\n",
      "Iter:  574  Loss on train set:  -8.247410321237004\n",
      "Iter:  575  Loss on train set:  -8.177364757287489\n",
      "Iter:  576  Loss on train set:  -8.131929200873369\n",
      "Iter:  577  Loss on train set:  -8.157666905371883\n",
      "Iter:  578  Loss on train set:  -8.141618745899407\n",
      "Iter:  579  Loss on train set:  -7.959188119826967\n",
      "Iter:  580  Loss on train set:  -8.13044647530131\n",
      "Iter:  581  Loss on train set:  -8.022673969047698\n",
      "Iter:  582  Loss on train set:  -8.23690170218916\n",
      "Iter:  583  Loss on train set:  -8.13684288349047\n",
      "Iter:  584  Loss on train set:  -8.311247366594772\n",
      "Iter:  585  Loss on train set:  -8.066654212417355\n",
      "Iter:  586  Loss on train set:  -8.25079689052285\n",
      "Iter:  587  Loss on train set:  -7.899838107287895\n",
      "Iter:  588  Loss on train set:  -8.329288620138563\n",
      "Iter:  589  Loss on train set:  -7.919222237220624\n",
      "Iter:  590  Loss on train set:  -8.344725982093875\n",
      "Iter:  591  Loss on train set:  -8.243960353147301\n",
      "Iter:  592  Loss on train set:  -8.404309583767438\n",
      "Iter:  593  Loss on train set:  -8.19728602402311\n",
      "Iter:  594  Loss on train set:  -8.410669798999376\n",
      "Iter:  595  Loss on train set:  -8.278601822990092\n",
      "Iter:  596  Loss on train set:  -8.332853583591941\n",
      "Iter:  597  Loss on train set:  -8.303813221436648\n",
      "Iter:  598  Loss on train set:  -8.473320672002659\n",
      "Iter:  599  Loss on train set:  -8.357035981400434\n",
      "Iter:  600  Loss on train set:  -8.439490627780646\n",
      "Iter:  601  Loss on train set:  -8.329357852643996\n",
      "Iter:  602  Loss on train set:  -8.48549914998485\n",
      "Iter:  603  Loss on train set:  -8.366649692101198\n",
      "Iter:  604  Loss on train set:  -8.457605653222688\n",
      "Iter:  605  Loss on train set:  -8.401066580866784\n",
      "Iter:  606  Loss on train set:  -8.469163902945752\n",
      "Iter:  607  Loss on train set:  -8.400402821823812\n",
      "Iter:  608  Loss on train set:  -8.581923120842927\n",
      "Iter:  609  Loss on train set:  -8.378259982996223\n",
      "Iter:  610  Loss on train set:  -8.632851256972844\n",
      "Iter:  611  Loss on train set:  -8.695578096733305\n",
      "Iter:  612  Loss on train set:  -8.694760807130105\n",
      "Iter:  613  Loss on train set:  -8.648028432242473\n",
      "Iter:  614  Loss on train set:  -8.695581417638607\n",
      "Iter:  615  Loss on train set:  -8.51157133460356\n",
      "Iter:  616  Loss on train set:  -8.634455890044752\n",
      "Iter:  617  Loss on train set:  -8.533610084887199\n",
      "Iter:  618  Loss on train set:  -8.658310969155888\n",
      "Iter:  619  Loss on train set:  -8.54807081901226\n",
      "Iter:  620  Loss on train set:  -8.675758215938004\n",
      "Iter:  621  Loss on train set:  -8.694266170420265\n",
      "Iter:  622  Loss on train set:  -8.690047584724095\n",
      "Iter:  623  Loss on train set:  -8.697449395366405\n",
      "Iter:  624  Loss on train set:  -8.69667817834716\n",
      "Iter:  625  Loss on train set:  -8.695482705885308\n",
      "Iter:  626  Loss on train set:  -8.703485662545512\n",
      "Iter:  627  Loss on train set:  -8.704128329467203\n",
      "Iter:  628  Loss on train set:  -8.699112496099357\n",
      "Iter:  629  Loss on train set:  -8.701552288012406\n",
      "Iter:  630  Loss on train set:  -8.703074589139549\n",
      "Iter:  631  Loss on train set:  -8.713869772060988\n",
      "Iter:  632  Loss on train set:  -8.710943545061134\n",
      "Iter:  633  Loss on train set:  -8.71422021126848\n",
      "Iter:  634  Loss on train set:  -8.715881639494166\n",
      "Iter:  635  Loss on train set:  -8.593118588518784\n",
      "Iter:  636  Loss on train set:  -8.713623585631492\n",
      "Iter:  637  Loss on train set:  -8.405061790155248\n",
      "Iter:  638  Loss on train set:  -8.711639279671822\n",
      "Iter:  639  Loss on train set:  -8.441156549328422\n",
      "Iter:  640  Loss on train set:  -8.713558739623073\n",
      "Iter:  641  Loss on train set:  -8.46412030619459\n",
      "Iter:  642  Loss on train set:  -8.714255028869143\n",
      "Iter:  643  Loss on train set:  -8.492673775887976\n",
      "Iter:  644  Loss on train set:  -8.707245510722656\n",
      "Iter:  645  Loss on train set:  -8.570222154821959\n",
      "Iter:  646  Loss on train set:  -8.675820197598426\n",
      "Iter:  647  Loss on train set:  -8.53644922654818\n",
      "Iter:  648  Loss on train set:  -8.72077862538979\n",
      "Iter:  649  Loss on train set:  -8.566020236542615\n",
      "Iter:  650  Loss on train set:  -8.720663765715656\n",
      "Iter:  651  Loss on train set:  -8.47916305326424\n",
      "Iter:  652  Loss on train set:  -8.713896254809756\n",
      "Iter:  653  Loss on train set:  -8.529082020517388\n",
      "Iter:  654  Loss on train set:  -8.726357280116599\n",
      "Iter:  655  Loss on train set:  -8.446805758867988\n",
      "Iter:  656  Loss on train set:  -8.716677385146573\n",
      "Iter:  657  Loss on train set:  -8.413831741638653\n",
      "Iter:  658  Loss on train set:  -8.73740988527404\n",
      "Iter:  659  Loss on train set:  -8.602578138824946\n",
      "Iter:  660  Loss on train set:  -8.727465718160305\n",
      "Iter:  661  Loss on train set:  -8.542692706344843\n",
      "Iter:  662  Loss on train set:  -8.737408824392155\n",
      "Iter:  663  Loss on train set:  -8.710522271667495\n",
      "Iter:  664  Loss on train set:  -8.695867061114432\n",
      "Iter:  665  Loss on train set:  -8.671660076734227\n",
      "Iter:  666  Loss on train set:  -8.678766513998953\n",
      "Iter:  667  Loss on train set:  -8.691000534528763\n",
      "Iter:  668  Loss on train set:  -8.694474439592774\n",
      "Iter:  669  Loss on train set:  -8.504548437774627\n",
      "Iter:  670  Loss on train set:  -8.671446088852068\n",
      "Iter:  671  Loss on train set:  -8.443467755734968\n",
      "Iter:  672  Loss on train set:  -8.695281039096212\n",
      "Iter:  673  Loss on train set:  -8.593226313137468\n",
      "Iter:  674  Loss on train set:  -8.69143240588101\n",
      "Iter:  675  Loss on train set:  -8.68105558453942\n",
      "Iter:  676  Loss on train set:  -8.765067874832168\n",
      "Iter:  677  Loss on train set:  -8.554997916136788\n",
      "Iter:  678  Loss on train set:  -8.687016038453923\n",
      "Iter:  679  Loss on train set:  -8.493811131835756\n",
      "Iter:  680  Loss on train set:  -8.763384587053642\n",
      "Iter:  681  Loss on train set:  -8.535097500900012\n",
      "Iter:  682  Loss on train set:  -8.723634062957665\n",
      "Iter:  683  Loss on train set:  -8.764361853307564\n",
      "Iter:  684  Loss on train set:  -8.781181464461035\n",
      "Iter:  685  Loss on train set:  -8.6015026666691\n",
      "Iter:  686  Loss on train set:  -8.745161335697313\n",
      "Iter:  687  Loss on train set:  -8.696301176041528\n",
      "Iter:  688  Loss on train set:  -8.777142871633481\n",
      "Iter:  689  Loss on train set:  -8.64354989812406\n",
      "Iter:  690  Loss on train set:  -8.816853284401015\n",
      "Iter:  691  Loss on train set:  -8.739843414170915\n",
      "Iter:  692  Loss on train set:  -8.731206178496736\n",
      "Iter:  693  Loss on train set:  -8.73312973120217\n",
      "Iter:  694  Loss on train set:  -8.761514032518292\n",
      "Iter:  695  Loss on train set:  -8.634336101546369\n",
      "Iter:  696  Loss on train set:  -8.777399085021159\n",
      "Iter:  697  Loss on train set:  -8.755561387732426\n",
      "Iter:  698  Loss on train set:  -8.72877503332067\n",
      "Iter:  699  Loss on train set:  -8.571605095240908\n",
      "Iter:  700  Loss on train set:  -8.690214344396074\n",
      "Iter:  701  Loss on train set:  -8.71616020101318\n",
      "Iter:  702  Loss on train set:  -8.782739493007846\n",
      "Iter:  703  Loss on train set:  -8.814000574981828\n",
      "Iter:  704  Loss on train set:  -8.808064063046011\n",
      "Iter:  705  Loss on train set:  -8.704962349668962\n",
      "Iter:  706  Loss on train set:  -8.85922123512229\n",
      "Iter:  707  Loss on train set:  -8.390375445261556\n",
      "Iter:  708  Loss on train set:  -8.802318116200732\n",
      "Iter:  709  Loss on train set:  -8.776295096223297\n",
      "Iter:  710  Loss on train set:  -8.890165586295685\n",
      "Iter:  711  Loss on train set:  -8.685297318636913\n",
      "Iter:  712  Loss on train set:  -8.879406940683591\n",
      "Iter:  713  Loss on train set:  -8.779009431047749\n",
      "Iter:  714  Loss on train set:  -8.89342585256367\n",
      "Iter:  715  Loss on train set:  -8.80744406552083\n",
      "Iter:  716  Loss on train set:  -8.879352051883766\n",
      "Iter:  717  Loss on train set:  -8.737754551064114\n",
      "Iter:  718  Loss on train set:  -8.902618514588326\n",
      "Iter:  719  Loss on train set:  -8.711872718075758\n",
      "Iter:  720  Loss on train set:  -8.902608501721737\n",
      "Iter:  721  Loss on train set:  -8.754608813989535\n",
      "Iter:  722  Loss on train set:  -8.894965087889066\n",
      "Iter:  723  Loss on train set:  -8.594748034323402\n",
      "Iter:  724  Loss on train set:  -8.907258110678221\n",
      "Iter:  725  Loss on train set:  -8.551693112370543\n",
      "Iter:  726  Loss on train set:  -8.908904990244789\n",
      "Iter:  727  Loss on train set:  -8.733428674354501\n",
      "Iter:  728  Loss on train set:  -8.912442849427833\n",
      "Iter:  729  Loss on train set:  -8.816728884361634\n",
      "Iter:  730  Loss on train set:  -8.914675999786528\n",
      "Iter:  731  Loss on train set:  -8.758718493955346\n",
      "Iter:  732  Loss on train set:  -8.9065672798523\n",
      "Iter:  733  Loss on train set:  -8.80155256765243\n",
      "Iter:  734  Loss on train set:  -8.920932064033986\n",
      "Iter:  735  Loss on train set:  -8.812436747158667\n",
      "Iter:  736  Loss on train set:  -8.92593342035244\n",
      "Iter:  737  Loss on train set:  -8.837819854425039\n",
      "Iter:  738  Loss on train set:  -8.926084608464127\n",
      "Iter:  739  Loss on train set:  -8.630233588106991\n",
      "Iter:  740  Loss on train set:  -8.927592707578807\n",
      "Iter:  741  Loss on train set:  -8.729097050701323\n",
      "Iter:  742  Loss on train set:  -8.928539817196684\n",
      "Iter:  743  Loss on train set:  -8.825052603074262\n",
      "Iter:  744  Loss on train set:  -8.913377755183468\n",
      "Iter:  745  Loss on train set:  -8.924024020582785\n",
      "Iter:  746  Loss on train set:  -8.83023618818914\n",
      "Iter:  747  Loss on train set:  -8.66523048236878\n",
      "Iter:  748  Loss on train set:  -8.928661633922314\n",
      "Iter:  749  Loss on train set:  -8.611240878859254\n",
      "Iter:  750  Loss on train set:  -8.932432017885162\n",
      "Iter:  751  Loss on train set:  -8.79471977047095\n",
      "Iter:  752  Loss on train set:  -8.936529455645246\n",
      "Iter:  753  Loss on train set:  -8.910721685945177\n",
      "Iter:  754  Loss on train set:  -8.944738044521797\n",
      "Iter:  755  Loss on train set:  -8.80626639480651\n",
      "Iter:  756  Loss on train set:  -8.944289175750905\n",
      "Iter:  757  Loss on train set:  -8.941616159804813\n",
      "Iter:  758  Loss on train set:  -8.958197374134594\n",
      "Iter:  759  Loss on train set:  -8.799671177840203\n",
      "Iter:  760  Loss on train set:  -8.957194661910705\n",
      "Iter:  761  Loss on train set:  -8.763305225584979\n",
      "Iter:  762  Loss on train set:  -8.957779805941046\n",
      "Iter:  763  Loss on train set:  -8.71100577447492\n",
      "Iter:  764  Loss on train set:  -8.950566064540316\n",
      "Iter:  765  Loss on train set:  -8.856195429282364\n",
      "Iter:  766  Loss on train set:  -8.896286666869468\n",
      "Iter:  767  Loss on train set:  -8.843516357890618\n",
      "Iter:  768  Loss on train set:  -8.958338770943927\n",
      "Iter:  769  Loss on train set:  -8.86713570894008\n",
      "Iter:  770  Loss on train set:  -8.948404791247318\n",
      "Iter:  771  Loss on train set:  -8.745039543813435\n",
      "Iter:  772  Loss on train set:  -8.95828978173949\n",
      "Iter:  773  Loss on train set:  -8.762347557223155\n",
      "Iter:  774  Loss on train set:  -8.944119602771998\n",
      "Iter:  775  Loss on train set:  -8.891758785485916\n",
      "Iter:  776  Loss on train set:  -8.968547766367044\n",
      "Iter:  777  Loss on train set:  -8.83754619157993\n",
      "Iter:  778  Loss on train set:  -8.903162539890449\n",
      "Iter:  779  Loss on train set:  -8.76919426040805\n",
      "Iter:  780  Loss on train set:  -8.885240189590915\n",
      "Iter:  781  Loss on train set:  -8.871281104302955\n",
      "Iter:  782  Loss on train set:  -8.954750809126331\n",
      "Iter:  783  Loss on train set:  -8.703898533039112\n",
      "Iter:  784  Loss on train set:  -8.96855712191516\n",
      "Iter:  785  Loss on train set:  -8.807457513810375\n",
      "Iter:  786  Loss on train set:  -8.948500153137765\n",
      "Iter:  787  Loss on train set:  -8.89923645089504\n",
      "Iter:  788  Loss on train set:  -8.966167918852006\n",
      "Iter:  789  Loss on train set:  -8.752124703010967\n",
      "Iter:  790  Loss on train set:  -8.916999972103248\n",
      "Iter:  791  Loss on train set:  -8.612651209608998\n",
      "Iter:  792  Loss on train set:  -8.931363867146095\n",
      "Iter:  793  Loss on train set:  -8.902658445351815\n",
      "Iter:  794  Loss on train set:  -8.927310017255417\n",
      "Iter:  795  Loss on train set:  -8.875932681294092\n",
      "Iter:  796  Loss on train set:  -8.948561642115541\n",
      "Iter:  797  Loss on train set:  -8.846926808077463\n",
      "Iter:  798  Loss on train set:  -8.948925760354431\n",
      "Iter:  799  Loss on train set:  -8.83179362643812\n",
      "Iter:  800  Loss on train set:  -8.975139821179724\n",
      "Iter:  801  Loss on train set:  -8.744537868669157\n",
      "Iter:  802  Loss on train set:  -8.956486288600926\n",
      "Iter:  803  Loss on train set:  -8.603810942364984\n",
      "Iter:  804  Loss on train set:  -8.935849286412086\n",
      "Iter:  805  Loss on train set:  -8.770970429023956\n",
      "Iter:  806  Loss on train set:  -8.872414383089675\n",
      "Iter:  807  Loss on train set:  -8.786395905893906\n",
      "Iter:  808  Loss on train set:  -8.964900538579808\n",
      "Iter:  809  Loss on train set:  -8.709152274605872\n",
      "Iter:  810  Loss on train set:  -8.959469108630685\n",
      "Iter:  811  Loss on train set:  -8.958113734006773\n",
      "Iter:  812  Loss on train set:  -8.93955001501919\n",
      "Iter:  813  Loss on train set:  -8.696737170355439\n",
      "Iter:  814  Loss on train set:  -8.940241454373284\n",
      "Iter:  815  Loss on train set:  -8.653741389224702\n",
      "Iter:  816  Loss on train set:  -8.98478463910703\n",
      "Iter:  817  Loss on train set:  -8.735558075877083\n",
      "Iter:  818  Loss on train set:  -8.988542477357479\n",
      "Iter:  819  Loss on train set:  -8.898360583912217\n",
      "Iter:  820  Loss on train set:  -8.988526180770513\n",
      "Iter:  821  Loss on train set:  -8.900562985152684\n",
      "Iter:  822  Loss on train set:  -8.984411505780244\n",
      "Iter:  823  Loss on train set:  -8.975201421329812\n",
      "Iter:  824  Loss on train set:  -8.96273900741256\n",
      "Iter:  825  Loss on train set:  -8.965023229182671\n",
      "Iter:  826  Loss on train set:  -8.997876564534765\n",
      "Iter:  827  Loss on train set:  -8.905378501637395\n",
      "Iter:  828  Loss on train set:  -8.999199818657011\n",
      "Iter:  829  Loss on train set:  -8.85536697624926\n",
      "Iter:  830  Loss on train set:  -9.000122193431656\n",
      "Iter:  831  Loss on train set:  -8.752987628312038\n",
      "Iter:  832  Loss on train set:  -9.010122855954979\n",
      "Iter:  833  Loss on train set:  -8.770373976247361\n",
      "Iter:  834  Loss on train set:  -9.017122671160573\n",
      "Iter:  835  Loss on train set:  -8.881920930078737\n",
      "Iter:  836  Loss on train set:  -9.017814982897068\n",
      "Iter:  837  Loss on train set:  -8.756620152077499\n",
      "Iter:  838  Loss on train set:  -9.017678938694623\n",
      "Iter:  839  Loss on train set:  -8.796272154227102\n",
      "Iter:  840  Loss on train set:  -8.990829416518169\n",
      "Iter:  841  Loss on train set:  -8.97229423837308\n",
      "Iter:  842  Loss on train set:  -9.016762282783624\n",
      "Iter:  843  Loss on train set:  -8.760839730950444\n",
      "Iter:  844  Loss on train set:  -9.019211517281146\n",
      "Iter:  845  Loss on train set:  -8.953816256638238\n",
      "Iter:  846  Loss on train set:  -9.020810658975584\n",
      "Iter:  847  Loss on train set:  -8.934315613452316\n",
      "Iter:  848  Loss on train set:  -9.02606334621626\n",
      "Iter:  849  Loss on train set:  -8.975360859637576\n",
      "Iter:  850  Loss on train set:  -9.02689984863326\n",
      "Iter:  851  Loss on train set:  -8.899836230351896\n",
      "Iter:  852  Loss on train set:  -8.92993480778796\n",
      "Iter:  853  Loss on train set:  -8.839233202360136\n",
      "Iter:  854  Loss on train set:  -9.030993134935105\n",
      "Iter:  855  Loss on train set:  -8.870721733920908\n",
      "Iter:  856  Loss on train set:  -9.02667792079087\n",
      "Iter:  857  Loss on train set:  -8.849226433614941\n",
      "Iter:  858  Loss on train set:  -8.990900796316891\n",
      "Iter:  859  Loss on train set:  -8.842538542431429\n",
      "Iter:  860  Loss on train set:  -8.96579600680736\n",
      "Iter:  861  Loss on train set:  -8.841643204133321\n",
      "Iter:  862  Loss on train set:  -9.03183450696775\n",
      "Iter:  863  Loss on train set:  -8.826633035429278\n",
      "Iter:  864  Loss on train set:  -9.046242379049493\n",
      "Iter:  865  Loss on train set:  -8.93762931874268\n",
      "Iter:  866  Loss on train set:  -8.969581001668042\n",
      "Iter:  867  Loss on train set:  -8.92415219777857\n",
      "Iter:  868  Loss on train set:  -9.052197699876952\n",
      "Iter:  869  Loss on train set:  -8.942751059847156\n",
      "Iter:  870  Loss on train set:  -9.089970869850351\n",
      "Iter:  871  Loss on train set:  -9.017810020709735\n",
      "Iter:  872  Loss on train set:  -9.089975507086034\n",
      "Iter:  873  Loss on train set:  -8.973266889410658\n",
      "Iter:  874  Loss on train set:  -9.08975647949254\n",
      "Iter:  875  Loss on train set:  -8.992803558619318\n",
      "Iter:  876  Loss on train set:  -9.090859415581992\n",
      "Iter:  877  Loss on train set:  -8.660970734344268\n",
      "Iter:  878  Loss on train set:  -8.988785319442314\n",
      "Iter:  879  Loss on train set:  -8.897645598618505\n",
      "Iter:  880  Loss on train set:  -9.06680045577285\n",
      "Iter:  881  Loss on train set:  -8.979142930103624\n",
      "Iter:  882  Loss on train set:  -9.1006138395611\n",
      "Iter:  883  Loss on train set:  -8.856632976743608\n",
      "Iter:  884  Loss on train set:  -9.055005846602961\n",
      "Iter:  885  Loss on train set:  -8.84230951683946\n",
      "Iter:  886  Loss on train set:  -9.084708178870901\n",
      "Iter:  887  Loss on train set:  -8.804082484044338\n",
      "Iter:  888  Loss on train set:  -9.09918512432036\n",
      "Iter:  889  Loss on train set:  -8.896572018667964\n",
      "Iter:  890  Loss on train set:  -8.946021475936968\n",
      "Iter:  891  Loss on train set:  -8.960029903520088\n",
      "Iter:  892  Loss on train set:  -9.10089907557926\n",
      "Iter:  893  Loss on train set:  -8.969038512658003\n",
      "Iter:  894  Loss on train set:  -9.118279343640403\n",
      "Iter:  895  Loss on train set:  -8.904832006474816\n",
      "Iter:  896  Loss on train set:  -9.111947886117692\n",
      "Iter:  897  Loss on train set:  -8.968193546117377\n",
      "Iter:  898  Loss on train set:  -9.087688643871225\n",
      "Iter:  899  Loss on train set:  -9.089815041909498\n",
      "Iter:  900  Loss on train set:  -9.115323203372244\n",
      "Iter:  901  Loss on train set:  -8.869886979191497\n",
      "Iter:  902  Loss on train set:  -9.036951421733944\n",
      "Iter:  903  Loss on train set:  -9.003717652947284\n",
      "Iter:  904  Loss on train set:  -9.1182691345537\n",
      "Iter:  905  Loss on train set:  -8.967386457810607\n",
      "Iter:  906  Loss on train set:  -9.125927624109803\n",
      "Iter:  907  Loss on train set:  -8.967658234182535\n",
      "Iter:  908  Loss on train set:  -9.120052783113827\n",
      "Iter:  909  Loss on train set:  -9.113672774948341\n",
      "Iter:  910  Loss on train set:  -9.102495130807892\n",
      "Iter:  911  Loss on train set:  -8.944451355064885\n",
      "Iter:  912  Loss on train set:  -9.119413202143242\n",
      "Iter:  913  Loss on train set:  -9.02522586891355\n",
      "Iter:  914  Loss on train set:  -9.099044775760788\n",
      "Iter:  915  Loss on train set:  -8.89514142667665\n",
      "Iter:  916  Loss on train set:  -9.099508471260133\n",
      "Iter:  917  Loss on train set:  -9.146567404852142\n",
      "Iter:  918  Loss on train set:  -9.176443240317509\n",
      "Iter:  919  Loss on train set:  -9.043794224055555\n",
      "Iter:  920  Loss on train set:  -9.168793642044161\n",
      "Iter:  921  Loss on train set:  -9.06133603101446\n",
      "Iter:  922  Loss on train set:  -9.171305471632055\n",
      "Iter:  923  Loss on train set:  -8.970435837281524\n",
      "Iter:  924  Loss on train set:  -9.176789519436202\n",
      "Iter:  925  Loss on train set:  -9.200190143516668\n",
      "Iter:  926  Loss on train set:  -9.20783814295244\n",
      "Iter:  927  Loss on train set:  -9.212676887449955\n",
      "Iter:  928  Loss on train set:  -9.212686387787642\n",
      "Iter:  929  Loss on train set:  -8.974569761221193\n",
      "Iter:  930  Loss on train set:  -9.214315238978811\n",
      "Iter:  931  Loss on train set:  -8.997385086885396\n",
      "Iter:  932  Loss on train set:  -9.214482072707652\n",
      "Iter:  933  Loss on train set:  -9.034595670117636\n",
      "Iter:  934  Loss on train set:  -9.215708493029615\n",
      "Iter:  935  Loss on train set:  -9.213915924714797\n",
      "Iter:  936  Loss on train set:  -9.193706685580029\n",
      "Iter:  937  Loss on train set:  -9.206413978043562\n",
      "Iter:  938  Loss on train set:  -9.229789928153924\n",
      "Iter:  939  Loss on train set:  -9.260412240837095\n",
      "Iter:  940  Loss on train set:  -9.262148832220873\n",
      "Iter:  941  Loss on train set:  -9.192094551079272\n",
      "Iter:  942  Loss on train set:  -9.263429427852298\n",
      "Iter:  943  Loss on train set:  -9.134338136326\n",
      "Iter:  944  Loss on train set:  -9.261928459007452\n",
      "Iter:  945  Loss on train set:  -9.196080086630726\n",
      "Iter:  946  Loss on train set:  -9.268150033710565\n",
      "Iter:  947  Loss on train set:  -9.028929275628663\n",
      "Iter:  948  Loss on train set:  -9.168751970485648\n",
      "Iter:  949  Loss on train set:  -8.998773802415501\n",
      "Iter:  950  Loss on train set:  -9.264377222791248\n",
      "Iter:  951  Loss on train set:  -9.113147918937196\n",
      "Iter:  952  Loss on train set:  -9.260335764822953\n",
      "Iter:  953  Loss on train set:  -8.958633206904851\n",
      "Iter:  954  Loss on train set:  -9.239493958463958\n",
      "Iter:  955  Loss on train set:  -9.203422702529133\n",
      "Iter:  956  Loss on train set:  -9.271074891803426\n",
      "Iter:  957  Loss on train set:  -9.024936882136068\n",
      "Iter:  958  Loss on train set:  -9.212687755233125\n",
      "Iter:  959  Loss on train set:  -9.201928543240594\n",
      "Iter:  960  Loss on train set:  -9.269811340979302\n",
      "Iter:  961  Loss on train set:  -9.095439122362427\n",
      "Iter:  962  Loss on train set:  -9.27270913020916\n",
      "Iter:  963  Loss on train set:  -9.086389500637996\n",
      "Iter:  964  Loss on train set:  -9.274487970069842\n",
      "Iter:  965  Loss on train set:  -8.988415032564642\n",
      "Iter:  966  Loss on train set:  -9.21259118124679\n",
      "Iter:  967  Loss on train set:  -9.178476794720396\n",
      "Iter:  968  Loss on train set:  -9.270333391175868\n",
      "Iter:  969  Loss on train set:  -9.100447918934558\n",
      "Iter:  970  Loss on train set:  -9.193097211410322\n",
      "Iter:  971  Loss on train set:  -9.073173107425532\n",
      "Iter:  972  Loss on train set:  -9.286544909164586\n",
      "Iter:  973  Loss on train set:  -8.903736753085475\n",
      "Iter:  974  Loss on train set:  -9.233680473145299\n",
      "Iter:  975  Loss on train set:  -9.047504368387031\n",
      "Iter:  976  Loss on train set:  -9.274242538012436\n",
      "Iter:  977  Loss on train set:  -9.0562599165189\n",
      "Iter:  978  Loss on train set:  -9.2417822129418\n",
      "Iter:  979  Loss on train set:  -9.15141081560217\n",
      "Iter:  980  Loss on train set:  -9.287772349078704\n",
      "Iter:  981  Loss on train set:  -9.003352805423331\n",
      "Iter:  982  Loss on train set:  -9.276878325285404\n",
      "Iter:  983  Loss on train set:  -9.114846733561972\n",
      "Iter:  984  Loss on train set:  -9.251024120008049\n",
      "Iter:  985  Loss on train set:  -9.180449654403107\n",
      "Iter:  986  Loss on train set:  -9.273441687898515\n",
      "Iter:  987  Loss on train set:  -9.068146399532715\n",
      "Iter:  988  Loss on train set:  -9.285144204814072\n",
      "Iter:  989  Loss on train set:  -9.189500762404375\n",
      "Iter:  990  Loss on train set:  -9.159914696143986\n",
      "Iter:  991  Loss on train set:  -9.161313329153538\n",
      "Iter:  992  Loss on train set:  -9.284463371361115\n",
      "Iter:  993  Loss on train set:  -9.021190094236005\n",
      "Iter:  994  Loss on train set:  -9.269949002424276\n",
      "Iter:  995  Loss on train set:  -9.145729986185957\n",
      "Iter:  996  Loss on train set:  -9.211603770295351\n",
      "Iter:  997  Loss on train set:  -9.151189088547774\n",
      "Iter:  998  Loss on train set:  -9.279332073726133\n",
      "Iter:  999  Loss on train set:  -9.058896339551708\n",
      "Iter:  1000  Loss on train set:  -9.287333067105488\n",
      "Iter:  1001  Loss on train set:  -9.287772349078704\n",
      "     fun: -9.287772349078704\n",
      "   maxcv: 0.0\n",
      " message: 'Maximum number of function evaluations has been exceeded.'\n",
      "    nfev: 1000\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([-3.40615141e+00, -5.75763495e+00, -3.58247081e-01,  1.58103833e+00,\n",
      "       -4.34470205e+00, -1.29090600e+00, -1.54258520e-01, -1.60486064e+00,\n",
      "       -2.39582979e+00, -6.33258957e-01, -1.58237320e+00, -5.21096240e+00,\n",
      "       -2.28358839e+00, -4.39769263e+00, -1.20142789e+00, -1.63704795e+00,\n",
      "       -1.25695181e-02, -1.46126384e+00,  1.04195161e+00, -8.48302394e-01,\n",
      "       -1.83382851e+00, -7.00085392e-01,  2.34622541e+00, -2.20266945e+00,\n",
      "       -1.02857440e+00,  5.30632900e-01, -7.60667792e-01, -3.75529664e-03,\n",
      "       -1.21267679e+00, -8.94638967e-01, -1.90545962e+00, -2.06503516e+00,\n",
      "       -2.16643349e+00, -2.36447328e+00, -4.36660665e+00,  3.25064928e-02,\n",
      "       -1.94674348e+00, -3.06293361e+00,  5.23453678e-01, -7.06343568e-01,\n",
      "       -2.29598230e+00,  8.69279493e-01, -2.43189860e+00,  1.73964873e+00,\n",
      "       -2.10520300e+00,  2.63708078e+00, -4.06958243e+00,  2.64929422e-01,\n",
      "       -2.85506972e+00, -3.13756808e+00, -9.23064199e-01, -9.24200749e-01,\n",
      "       -3.67153448e+00, -1.71902969e+00,  9.17797898e-01, -1.68327908e+00,\n",
      "       -2.80353791e-01, -9.10872494e-01, -1.05392995e+00, -1.49637217e+00,\n",
      "       -2.99587500e+00, -1.28278855e+00, -3.64560148e+00, -2.90390140e-01,\n",
      "       -1.67733977e+00,  6.33940343e-01, -9.28511362e-01, -3.27080535e+00,\n",
      "       -2.62241002e+00,  1.74900821e+00, -2.03441698e+00, -3.40108819e+00,\n",
      "       -8.17146869e-01, -9.49555839e-01, -1.00212855e+00, -6.01065542e+00,\n",
      "       -3.27962787e+00, -6.38853532e-01,  1.37342825e+00, -6.87823512e-02,\n",
      "       -3.44001772e+00, -1.10391082e+00, -5.46121921e-01, -1.49601060e+00,\n",
      "       -1.95212106e+00, -1.19997285e+00, -3.82459922e-01,  8.24455438e-01,\n",
      "        2.71279403e+00, -2.55510647e+00,  2.03026992e-01, -5.73056809e-01,\n",
      "       -2.03198104e+00, -1.31739735e+00, -2.55080694e+00, -1.43336611e+00])\n"
     ]
    }
   ],
   "source": [
    "qubits = cirq.LineQubit.range(8)\n",
    "\n",
    "Nfeval = 1  \n",
    "\n",
    "vqe_hamilt_params = {\"lambda\": 0.75, \"Delta\": 0.1}\n",
    "\n",
    "vqe(qubits,hamilt_params=vqe_hamilt_params, params_init_mode=\"normal_random\", n_meas_reps=100000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Opt-Meta-VQE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_meta_vqe(qubits, hamilt_params_training_set, hamilt_params_test_set, n_meas_reps=1000):\n",
    "    \"\"\"\n",
    "    Full routine to train and evaluate a opt-meta-VQE\n",
    "    \n",
    "    Args:\n",
    "    ----------------\n",
    "        qubits [list]: list of qubits\n",
    "        hamilt_params_training_set [list]: list of the parameters of the hamiltonians in the training set\n",
    "        hamilt_params_test_set [list]: list of the parameters of the hamiltonians in the test set\n",
    "        n_meas_reps [int]: number of measurement repetitions\n",
    "    \n",
    "    Returns:\n",
    "    ----------------\n",
    "        energy_expectations [list]: list of the energy expectations of the hamiltonians in the test set\n",
    "        abs_energy_errors [list]: list of the absolute energy errors of the hamiltonians in the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    #Train meta-VQE:\n",
    "    meta_vqe_opt_circuit_params = train_meta_vqe(qubits, hamilt_params_training_set, params_init_mode=\"normal_random\")\n",
    "    \n",
    "    #Optimize a VQE on each test set sample:\n",
    "    energy_expectations, abs_energy_errors = evaluate_vqe(qubits, hamilt_params_test_set, init_circuit_params=meta_vqe_opt_circuit_params)\n",
    "    \n",
    "    return energy_expectations, abs_energy_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qubits = cirq.LineQubit.range(5)\n",
    "\n",
    "Nfeval = 1\n",
    "\n",
    "train_set_size = 10\n",
    "min_hamilt_params = {\"lambda\": 0.75, \"Delta\": -1.1}\n",
    "max_hamilt_params = {\"lambda\": 0.75, \"Delta\": 1.1}\n",
    "\n",
    "train_set = construct_training_set(train_set_size=train_set_size, mode=\"equidistant\")\n",
    "test_set = construct_test_set(test_set_size=10, mode=\"uniform_random\", min_hamilt_params=min_hamilt_params, max_hamilt_params=max_hamilt_params)\n",
    "\n",
    "opt_meta_vqe(qubits, train_set, test_set, n_meas_reps=1000)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reproducing figure 2 of the meta-vqe paper: Absolute energy and ground state energy as a function of Delta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subfigure(2)\n",
    "#axes[0].plot() #Exact GS energy\n",
    "axes[0].plot() #VQE GS energy\n",
    "axes[0].plot() #Meta-VQE GS energy\n",
    "axes[0].plot() #Opt-Meta-VQE GS energy\n",
    "axes[0].set_title(\"Ground State Energy\")\n",
    "axes[0].set_xlabel(\"Delta\")\n",
    "axes[0].set_ylabel(\"GS Energy\")\n",
    "\n",
    "axes[1].plot() #VQE Absolute Energy Error\n",
    "axes[1].plot() #Meta-VQE Absolute Energy Error\n",
    "axes[1].plot() #Opt-Meta-VQE Absolute Energy Error\n",
    "axes[1].set_title(\"Absolute Energy Error\")\n",
    "axes[1].set_xlabel(\"Delta\")\n",
    "axes[1].set_ylabel(\"Absolute Energy Error\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
